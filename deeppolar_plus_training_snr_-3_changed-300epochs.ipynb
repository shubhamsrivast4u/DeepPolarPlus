{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = True  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 300  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = -3.  # SNR at which encoder is trained\n",
    "dec_train_snr = -5.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.001  # Decoder Learning rate\n",
    "enc_lr = 0.001  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}__{enc_train_snr}_Encoder_{decoder_type}_{dec_train_snr}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/300] At -5.0 dB, Train Loss: 8.80435848236084 Train BER 0.4822648763656616,                  \n",
      " [1/300] At -3.0 dB, Train Loss: 8.418436050415039 Train BER 0.47708648443222046\n",
      "Time for one full iteration is 9.0837 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[2/300] At -5.0 dB, Train Loss: 6.57529354095459 Train BER 0.481183797121048,                  \n",
      " [2/300] At -3.0 dB, Train Loss: 6.489841461181641 Train BER 0.47565406560897827\n",
      "Time for one full iteration is 8.9634 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[3/300] At -5.0 dB, Train Loss: 6.0507001876831055 Train BER 0.4799405336380005,                  \n",
      " [3/300] At -3.0 dB, Train Loss: 6.022477626800537 Train BER 0.47215133905410767\n",
      "Time for one full iteration is 9.0510 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[4/300] At -5.0 dB, Train Loss: 5.914200305938721 Train BER 0.47605404257774353,                  \n",
      " [4/300] At -3.0 dB, Train Loss: 5.887794017791748 Train BER 0.4673945903778076\n",
      "Time for one full iteration is 8.9904 minutes\n",
      "encoder learning rate: 8.00e-05, decoder learning rate: 8.00e-05\n",
      "[5/300] At -5.0 dB, Train Loss: 5.821595668792725 Train BER 0.4554864764213562,                  \n",
      " [5/300] At -3.0 dB, Train Loss: 5.740475654602051 Train BER 0.4380648732185364\n",
      "Time for one full iteration is 8.9220 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[6/300] At -5.0 dB, Train Loss: 5.733768939971924 Train BER 0.44010812044143677,                  \n",
      " [6/300] At -3.0 dB, Train Loss: 5.57907247543335 Train BER 0.414000004529953\n",
      "Time for one full iteration is 8.9959 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[7/300] At -5.0 dB, Train Loss: 5.229001522064209 Train BER 0.3622864782810211,                  \n",
      " [7/300] At -3.0 dB, Train Loss: 4.444194316864014 Train BER 0.28169187903404236\n",
      "Time for one full iteration is 8.9972 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[8/300] At -5.0 dB, Train Loss: 4.178098678588867 Train BER 0.2549351453781128,                  \n",
      " [8/300] At -3.0 dB, Train Loss: 2.376322031021118 Train BER 0.11900540441274643\n",
      "Time for one full iteration is 8.8904 minutes\n",
      "encoder learning rate: 1.60e-04, decoder learning rate: 1.60e-04\n",
      "[9/300] At -5.0 dB, Train Loss: 3.453920841217041 Train BER 0.20202162861824036,                  \n",
      " [9/300] At -3.0 dB, Train Loss: 1.5776076316833496 Train BER 0.07055135071277618\n",
      "Time for one full iteration is 8.7982 minutes\n",
      "encoder learning rate: 1.80e-04, decoder learning rate: 1.80e-04\n",
      "[10/300] At -5.0 dB, Train Loss: 2.955631732940674 Train BER 0.16506487131118774,                  \n",
      " [10/300] At -3.0 dB, Train Loss: 1.1619068384170532 Train BER 0.05070810765028\n",
      "Time for one full iteration is 8.9846 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[11/300] At -5.0 dB, Train Loss: 2.664931058883667 Train BER 0.1457081139087677,                  \n",
      " [11/300] At -3.0 dB, Train Loss: 0.9917059540748596 Train BER 0.04289189353585243\n",
      "Time for one full iteration is 8.6425 minutes\n",
      "encoder learning rate: 2.20e-04, decoder learning rate: 2.20e-04\n",
      "[12/300] At -5.0 dB, Train Loss: 2.4774208068847656 Train BER 0.13504324853420258,                  \n",
      " [12/300] At -3.0 dB, Train Loss: 0.8665747046470642 Train BER 0.03595675528049469\n",
      "Time for one full iteration is 8.6699 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[13/300] At -5.0 dB, Train Loss: 2.350656032562256 Train BER 0.1268540471792221,                  \n",
      " [13/300] At -3.0 dB, Train Loss: 0.8501840233802795 Train BER 0.037075676023960114\n",
      "Time for one full iteration is 8.5904 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[14/300] At -5.0 dB, Train Loss: 2.254422664642334 Train BER 0.11954054236412048,                  \n",
      " [14/300] At -3.0 dB, Train Loss: 0.8226842880249023 Train BER 0.034989189356565475\n",
      "Time for one full iteration is 8.8470 minutes\n",
      "encoder learning rate: 2.80e-04, decoder learning rate: 2.80e-04\n",
      "[15/300] At -5.0 dB, Train Loss: 2.214273691177368 Train BER 0.1161351352930069,                  \n",
      " [15/300] At -3.0 dB, Train Loss: 0.7751461267471313 Train BER 0.02916216291487217\n",
      "Time for one full iteration is 8.6192 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[16/300] At -5.0 dB, Train Loss: 2.1721384525299072 Train BER 0.1148756742477417,                  \n",
      " [16/300] At -3.0 dB, Train Loss: 0.6694941520690918 Train BER 0.02383783832192421\n",
      "Time for one full iteration is 8.7405 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[17/300] At -5.0 dB, Train Loss: 2.0821259021759033 Train BER 0.10894054174423218,                  \n",
      " [17/300] At -3.0 dB, Train Loss: 0.6231800317764282 Train BER 0.022648649290204048\n",
      "Time for one full iteration is 8.7850 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[18/300] At -5.0 dB, Train Loss: 2.0448615550994873 Train BER 0.1076432466506958,                  \n",
      " [18/300] At -3.0 dB, Train Loss: 0.6190506815910339 Train BER 0.02221621572971344\n",
      "Time for one full iteration is 8.7827 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[19/300] At -5.0 dB, Train Loss: 2.022386074066162 Train BER 0.10576216131448746,                  \n",
      " [19/300] At -3.0 dB, Train Loss: 0.6025972962379456 Train BER 0.022594595327973366\n",
      "Time for one full iteration is 8.5647 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[20/300] At -5.0 dB, Train Loss: 2.0210320949554443 Train BER 0.10543783754110336,                  \n",
      " [20/300] At -3.0 dB, Train Loss: 0.6199832558631897 Train BER 0.02356216125190258\n",
      "Time for one full iteration is 8.8236 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[21/300] At -5.0 dB, Train Loss: 2.0413010120391846 Train BER 0.10719999670982361,                  \n",
      " [21/300] At -3.0 dB, Train Loss: 0.5938917994499207 Train BER 0.022162161767482758\n",
      "Time for one full iteration is 8.9013 minutes\n",
      "encoder learning rate: 4.20e-04, decoder learning rate: 4.20e-04\n",
      "[22/300] At -5.0 dB, Train Loss: 1.9626476764678955 Train BER 0.10304324328899384,                  \n",
      " [22/300] At -3.0 dB, Train Loss: 0.5675808191299438 Train BER 0.02077837847173214\n",
      "Time for one full iteration is 8.9056 minutes\n",
      "encoder learning rate: 4.40e-04, decoder learning rate: 4.40e-04\n",
      "[23/300] At -5.0 dB, Train Loss: 2.0240767002105713 Train BER 0.10532432794570923,                  \n",
      " [23/300] At -3.0 dB, Train Loss: 0.5640012621879578 Train BER 0.021129729226231575\n",
      "Time for one full iteration is 8.5929 minutes\n",
      "encoder learning rate: 4.60e-04, decoder learning rate: 4.60e-04\n",
      "[24/300] At -5.0 dB, Train Loss: 2.0039873123168945 Train BER 0.10423243045806885,                  \n",
      " [24/300] At -3.0 dB, Train Loss: 0.5591460466384888 Train BER 0.021216215565800667\n",
      "Time for one full iteration is 8.7674 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[25/300] At -5.0 dB, Train Loss: 1.993842601776123 Train BER 0.10449188947677612,                  \n",
      " [25/300] At -3.0 dB, Train Loss: 0.5916184782981873 Train BER 0.02312972955405712\n",
      "Time for one full iteration is 8.6644 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[26/300] At -5.0 dB, Train Loss: 2.004389762878418 Train BER 0.1043243259191513,                  \n",
      " [26/300] At -3.0 dB, Train Loss: 0.615705668926239 Train BER 0.024005405604839325\n",
      "Time for one full iteration is 8.6639 minutes\n",
      "encoder learning rate: 5.20e-04, decoder learning rate: 5.20e-04\n",
      "[27/300] At -5.0 dB, Train Loss: 1.9894479513168335 Train BER 0.104854054749012,                  \n",
      " [27/300] At -3.0 dB, Train Loss: 0.5637513995170593 Train BER 0.022394593805074692\n",
      "Time for one full iteration is 8.8187 minutes\n",
      "encoder learning rate: 5.40e-04, decoder learning rate: 5.40e-04\n",
      "[28/300] At -5.0 dB, Train Loss: 1.947115421295166 Train BER 0.10192432254552841,                  \n",
      " [28/300] At -3.0 dB, Train Loss: 0.5190434455871582 Train BER 0.019756756722927094\n",
      "Time for one full iteration is 8.9162 minutes\n",
      "encoder learning rate: 5.60e-04, decoder learning rate: 5.60e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/300] At -5.0 dB, Train Loss: 1.9052581787109375 Train BER 0.10067567229270935,                  \n",
      " [29/300] At -3.0 dB, Train Loss: 0.5796464085578918 Train BER 0.02337837778031826\n",
      "Time for one full iteration is 8.8079 minutes\n",
      "encoder learning rate: 5.80e-04, decoder learning rate: 5.80e-04\n",
      "[30/300] At -5.0 dB, Train Loss: 1.9628485441207886 Train BER 0.10337837785482407,                  \n",
      " [30/300] At -3.0 dB, Train Loss: 0.6529395580291748 Train BER 0.026735134422779083\n",
      "Time for one full iteration is 8.9268 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[31/300] At -5.0 dB, Train Loss: 2.002805709838867 Train BER 0.10537838190793991,                  \n",
      " [31/300] At -3.0 dB, Train Loss: 0.6879757046699524 Train BER 0.02805405482649803\n",
      "Time for one full iteration is 8.8577 minutes\n",
      "encoder learning rate: 6.20e-04, decoder learning rate: 6.20e-04\n",
      "[32/300] At -5.0 dB, Train Loss: 2.0348165035247803 Train BER 0.10852973163127899,                  \n",
      " [32/300] At -3.0 dB, Train Loss: 0.676809549331665 Train BER 0.0286378376185894\n",
      "Time for one full iteration is 8.6522 minutes\n",
      "encoder learning rate: 6.40e-04, decoder learning rate: 6.40e-04\n",
      "[33/300] At -5.0 dB, Train Loss: 2.054384469985962 Train BER 0.10984324663877487,                  \n",
      " [33/300] At -3.0 dB, Train Loss: 0.7592505216598511 Train BER 0.032632432878017426\n",
      "Time for one full iteration is 8.7901 minutes\n",
      "encoder learning rate: 6.60e-04, decoder learning rate: 6.60e-04\n",
      "[34/300] At -5.0 dB, Train Loss: 2.115551710128784 Train BER 0.1139405369758606,                  \n",
      " [34/300] At -3.0 dB, Train Loss: 0.8973018527030945 Train BER 0.04235675558447838\n",
      "Time for one full iteration is 8.6929 minutes\n",
      "encoder learning rate: 6.80e-04, decoder learning rate: 6.80e-04\n",
      "[35/300] At -5.0 dB, Train Loss: 2.2290616035461426 Train BER 0.12193513661623001,                  \n",
      " [35/300] At -3.0 dB, Train Loss: 0.8905713558197021 Train BER 0.03931351378560066\n",
      "Time for one full iteration is 8.9004 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n",
      "[36/300] At -5.0 dB, Train Loss: 2.233086347579956 Train BER 0.12029729783535004,                  \n",
      " [36/300] At -3.0 dB, Train Loss: 0.8382681608200073 Train BER 0.03870270401239395\n",
      "Time for one full iteration is 8.8881 minutes\n",
      "encoder learning rate: 7.20e-04, decoder learning rate: 7.20e-04\n",
      "[37/300] At -5.0 dB, Train Loss: 2.265416145324707 Train BER 0.12585945427417755,                  \n",
      " [37/300] At -3.0 dB, Train Loss: 0.8630991578102112 Train BER 0.03721081092953682\n",
      "Time for one full iteration is 8.9425 minutes\n",
      "encoder learning rate: 7.40e-04, decoder learning rate: 7.40e-04\n",
      "[38/300] At -5.0 dB, Train Loss: 2.1937813758850098 Train BER 0.11690811067819595,                  \n",
      " [38/300] At -3.0 dB, Train Loss: 0.843058705329895 Train BER 0.03709189221262932\n",
      "Time for one full iteration is 8.6288 minutes\n",
      "encoder learning rate: 7.60e-04, decoder learning rate: 7.60e-04\n",
      "[39/300] At -5.0 dB, Train Loss: 2.24216890335083 Train BER 0.12107567489147186,                  \n",
      " [39/300] At -3.0 dB, Train Loss: 0.7661924958229065 Train BER 0.03132972866296768\n",
      "Time for one full iteration is 8.5782 minutes\n",
      "encoder learning rate: 7.80e-04, decoder learning rate: 7.80e-04\n",
      "[40/300] At -5.0 dB, Train Loss: 2.0018246173858643 Train BER 0.1044270247220993,                  \n",
      " [40/300] At -3.0 dB, Train Loss: 0.7752788066864014 Train BER 0.033248648047447205\n",
      "Time for one full iteration is 8.7414 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[41/300] At -5.0 dB, Train Loss: 2.15114688873291 Train BER 0.11481621861457825,                  \n",
      " [41/300] At -3.0 dB, Train Loss: 0.7556837797164917 Train BER 0.031145945191383362\n",
      "Time for one full iteration is 8.7723 minutes\n",
      "encoder learning rate: 8.20e-04, decoder learning rate: 8.20e-04\n",
      "[42/300] At -5.0 dB, Train Loss: 2.109757900238037 Train BER 0.11205945909023285,                  \n",
      " [42/300] At -3.0 dB, Train Loss: 0.7229135036468506 Train BER 0.029729729518294334\n",
      "Time for one full iteration is 8.6849 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n",
      "[43/300] At -5.0 dB, Train Loss: 2.0201704502105713 Train BER 0.10683242976665497,                  \n",
      " [43/300] At -3.0 dB, Train Loss: 0.7867701649665833 Train BER 0.0335729718208313\n",
      "Time for one full iteration is 8.8040 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[44/300] At -5.0 dB, Train Loss: 2.142666816711426 Train BER 0.1142973005771637,                  \n",
      " [44/300] At -3.0 dB, Train Loss: 0.7963642477989197 Train BER 0.03344324231147766\n",
      "Time for one full iteration is 8.8223 minutes\n",
      "encoder learning rate: 8.80e-04, decoder learning rate: 8.80e-04\n",
      "[45/300] At -5.0 dB, Train Loss: 1.9998998641967773 Train BER 0.1048702672123909,                  \n",
      " [45/300] At -3.0 dB, Train Loss: 0.8416302800178528 Train BER 0.0357675664126873\n",
      "Time for one full iteration is 8.7615 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[46/300] At -5.0 dB, Train Loss: 2.1366899013519287 Train BER 0.1140756756067276,                  \n",
      " [46/300] At -3.0 dB, Train Loss: 0.8338738679885864 Train BER 0.035427026450634\n",
      "Time for one full iteration is 8.8008 minutes\n",
      "encoder learning rate: 9.20e-04, decoder learning rate: 9.20e-04\n",
      "[47/300] At -5.0 dB, Train Loss: 2.1013216972351074 Train BER 0.11248107999563217,                  \n",
      " [47/300] At -3.0 dB, Train Loss: 1.0011807680130005 Train BER 0.042427025735378265\n",
      "Time for one full iteration is 8.8785 minutes\n",
      "encoder learning rate: 9.40e-04, decoder learning rate: 9.40e-04\n",
      "[48/300] At -5.0 dB, Train Loss: 2.0395421981811523 Train BER 0.10965405404567719,                  \n",
      " [48/300] At -3.0 dB, Train Loss: 0.8579351305961609 Train BER 0.03672432526946068\n",
      "Time for one full iteration is 8.6365 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[49/300] At -5.0 dB, Train Loss: 2.0966625213623047 Train BER 0.1136162132024765,                  \n",
      " [49/300] At -3.0 dB, Train Loss: 0.7750095129013062 Train BER 0.03216216340661049\n",
      "Time for one full iteration is 8.5911 minutes\n",
      "encoder learning rate: 9.80e-04, decoder learning rate: 9.80e-04\n",
      "[50/300] At -5.0 dB, Train Loss: 2.065570592880249 Train BER 0.10933513194322586,                  \n",
      " [50/300] At -3.0 dB, Train Loss: 0.8655328154563904 Train BER 0.03741621598601341\n",
      "Time for one full iteration is 8.9971 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[51/300] At -5.0 dB, Train Loss: 2.134018898010254 Train BER 0.11463243514299393,                  \n",
      " [51/300] At -3.0 dB, Train Loss: 0.9610421061515808 Train BER 0.04244324192404747\n",
      "Time for one full iteration is 8.8470 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[52/300] At -5.0 dB, Train Loss: 2.13349986076355 Train BER 0.1133783757686615,                  \n",
      " [52/300] At -3.0 dB, Train Loss: 0.9111403226852417 Train BER 0.039583783596754074\n",
      "Time for one full iteration is 8.7426 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[53/300] At -5.0 dB, Train Loss: 2.180925130844116 Train BER 0.11564324051141739,                  \n",
      " [53/300] At -3.0 dB, Train Loss: 0.8548794984817505 Train BER 0.036151349544525146\n",
      "Time for one full iteration is 8.6209 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[54/300] At -5.0 dB, Train Loss: 1.9669746160507202 Train BER 0.10402702540159225,                  \n",
      " [54/300] At -3.0 dB, Train Loss: 0.7920007705688477 Train BER 0.03290270268917084\n",
      "Time for one full iteration is 8.7271 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[55/300] At -5.0 dB, Train Loss: 2.1360955238342285 Train BER 0.1144324317574501,                  \n",
      " [55/300] At -3.0 dB, Train Loss: 0.7883011102676392 Train BER 0.03347567468881607\n",
      "Time for one full iteration is 8.9524 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[56/300] At -5.0 dB, Train Loss: 1.9802374839782715 Train BER 0.10521621257066727,                  \n",
      " [56/300] At -3.0 dB, Train Loss: 1.0622639656066895 Train BER 0.05066486448049545\n",
      "Time for one full iteration is 8.9478 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/300] At -5.0 dB, Train Loss: 2.3819491863250732 Train BER 0.13077837228775024,                  \n",
      " [57/300] At -3.0 dB, Train Loss: 0.9556975364685059 Train BER 0.044605404138565063\n",
      "Time for one full iteration is 8.7667 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[58/300] At -5.0 dB, Train Loss: 2.053142547607422 Train BER 0.11162702739238739,                  \n",
      " [58/300] At -3.0 dB, Train Loss: 0.9324500560760498 Train BER 0.044843241572380066\n",
      "Time for one full iteration is 8.7903 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[59/300] At -5.0 dB, Train Loss: 2.211094379425049 Train BER 0.12341081351041794,                  \n",
      " [59/300] At -3.0 dB, Train Loss: 0.7083290219306946 Train BER 0.030989188700914383\n",
      "Time for one full iteration is 8.8058 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[60/300] At -5.0 dB, Train Loss: 2.0291261672973633 Train BER 0.11301621794700623,                  \n",
      " [60/300] At -3.0 dB, Train Loss: 0.8051210045814514 Train BER 0.03710810840129852\n",
      "Time for one full iteration is 8.7264 minutes\n",
      "encoder learning rate: 9.96e-04, decoder learning rate: 9.96e-04\n",
      "[61/300] At -5.0 dB, Train Loss: 2.2193546295166016 Train BER 0.12502162158489227,                  \n",
      " [61/300] At -3.0 dB, Train Loss: 0.8136995434761047 Train BER 0.037962161004543304\n",
      "Time for one full iteration is 8.9509 minutes\n",
      "encoder learning rate: 9.95e-04, decoder learning rate: 9.95e-04\n",
      "[62/300] At -5.0 dB, Train Loss: 2.069155693054199 Train BER 0.11558378487825394,                  \n",
      " [62/300] At -3.0 dB, Train Loss: 0.7323811650276184 Train BER 0.03351351246237755\n",
      "Time for one full iteration is 8.7114 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[63/300] At -5.0 dB, Train Loss: 2.054682970046997 Train BER 0.11349189281463623,                  \n",
      " [63/300] At -3.0 dB, Train Loss: 0.6775527596473694 Train BER 0.030091891065239906\n",
      "Time for one full iteration is 8.8785 minutes\n",
      "encoder learning rate: 9.93e-04, decoder learning rate: 9.93e-04\n",
      "[64/300] At -5.0 dB, Train Loss: 1.9773939847946167 Train BER 0.10907567292451859,                  \n",
      " [64/300] At -3.0 dB, Train Loss: 0.6330640316009521 Train BER 0.02778918854892254\n",
      "Time for one full iteration is 8.9403 minutes\n",
      "encoder learning rate: 9.92e-04, decoder learning rate: 9.92e-04\n",
      "[65/300] At -5.0 dB, Train Loss: 2.0383543968200684 Train BER 0.1142108142375946,                  \n",
      " [65/300] At -3.0 dB, Train Loss: 0.6676415801048279 Train BER 0.02950270287692547\n",
      "Time for one full iteration is 9.0290 minutes\n",
      "encoder learning rate: 9.91e-04, decoder learning rate: 9.91e-04\n",
      "[66/300] At -5.0 dB, Train Loss: 2.0423028469085693 Train BER 0.11312972754240036,                  \n",
      " [66/300] At -3.0 dB, Train Loss: 0.7812344431877136 Train BER 0.03573513403534889\n",
      "Time for one full iteration is 8.8616 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[67/300] At -5.0 dB, Train Loss: 2.0294947624206543 Train BER 0.11294054239988327,                  \n",
      " [67/300] At -3.0 dB, Train Loss: 0.7843077182769775 Train BER 0.0362270288169384\n",
      "Time for one full iteration is 8.9816 minutes\n",
      "encoder learning rate: 9.89e-04, decoder learning rate: 9.89e-04\n",
      "[68/300] At -5.0 dB, Train Loss: 2.201139211654663 Train BER 0.12088648974895477,                  \n",
      " [68/300] At -3.0 dB, Train Loss: 0.9904095530509949 Train BER 0.04078378528356552\n",
      "Time for one full iteration is 8.9354 minutes\n",
      "encoder learning rate: 9.87e-04, decoder learning rate: 9.87e-04\n",
      "[69/300] At -5.0 dB, Train Loss: 2.0781054496765137 Train BER 0.11337297409772873,                  \n",
      " [69/300] At -3.0 dB, Train Loss: 0.6920896172523499 Train BER 0.031475674360990524\n",
      "Time for one full iteration is 8.9989 minutes\n",
      "encoder learning rate: 9.86e-04, decoder learning rate: 9.86e-04\n",
      "[70/300] At -5.0 dB, Train Loss: 2.000192642211914 Train BER 0.11068648844957352,                  \n",
      " [70/300] At -3.0 dB, Train Loss: 0.6107963919639587 Train BER 0.025037838146090508\n",
      "Time for one full iteration is 8.9584 minutes\n",
      "encoder learning rate: 9.84e-04, decoder learning rate: 9.84e-04\n",
      "[71/300] At -5.0 dB, Train Loss: 1.952063798904419 Train BER 0.10558918863534927,                  \n",
      " [71/300] At -3.0 dB, Train Loss: 0.5938996076583862 Train BER 0.0252432432025671\n",
      "Time for one full iteration is 8.7810 minutes\n",
      "encoder learning rate: 9.83e-04, decoder learning rate: 9.83e-04\n",
      "[72/300] At -5.0 dB, Train Loss: 1.896989345550537 Train BER 0.10210810601711273,                  \n",
      " [72/300] At -3.0 dB, Train Loss: 0.6090983152389526 Train BER 0.02484864927828312\n",
      "Time for one full iteration is 8.7455 minutes\n",
      "encoder learning rate: 9.81e-04, decoder learning rate: 9.81e-04\n",
      "[73/300] At -5.0 dB, Train Loss: 1.9345593452453613 Train BER 0.10369729995727539,                  \n",
      " [73/300] At -3.0 dB, Train Loss: 0.597675621509552 Train BER 0.02441621571779251\n",
      "Time for one full iteration is 8.7698 minutes\n",
      "encoder learning rate: 9.79e-04, decoder learning rate: 9.79e-04\n",
      "[74/300] At -5.0 dB, Train Loss: 1.9226610660552979 Train BER 0.10257837921380997,                  \n",
      " [74/300] At -3.0 dB, Train Loss: 0.6023871302604675 Train BER 0.02469729818403721\n",
      "Time for one full iteration is 8.7568 minutes\n",
      "encoder learning rate: 9.77e-04, decoder learning rate: 9.77e-04\n",
      "[75/300] At -5.0 dB, Train Loss: 1.9374271631240845 Train BER 0.10410811007022858,                  \n",
      " [75/300] At -3.0 dB, Train Loss: 0.5563644766807556 Train BER 0.022567568346858025\n",
      "Time for one full iteration is 8.6832 minutes\n",
      "encoder learning rate: 9.76e-04, decoder learning rate: 9.76e-04\n",
      "[76/300] At -5.0 dB, Train Loss: 1.9304895401000977 Train BER 0.10461080819368362,                  \n",
      " [76/300] At -3.0 dB, Train Loss: 0.5459631085395813 Train BER 0.021794594824314117\n",
      "Time for one full iteration is 8.8463 minutes\n",
      "encoder learning rate: 9.74e-04, decoder learning rate: 9.74e-04\n",
      "[77/300] At -5.0 dB, Train Loss: 1.8424818515777588 Train BER 0.0978432446718216,                  \n",
      " [77/300] At -3.0 dB, Train Loss: 0.4803391098976135 Train BER 0.019637838006019592\n",
      "Time for one full iteration is 8.6602 minutes\n",
      "encoder learning rate: 9.72e-04, decoder learning rate: 9.72e-04\n",
      "[78/300] At -5.0 dB, Train Loss: 1.7850061655044556 Train BER 0.09463784098625183,                  \n",
      " [78/300] At -3.0 dB, Train Loss: 0.468395471572876 Train BER 0.01860000006854534\n",
      "Time for one full iteration is 8.7737 minutes\n",
      "encoder learning rate: 9.69e-04, decoder learning rate: 9.69e-04\n",
      "[79/300] At -5.0 dB, Train Loss: 1.8222342729568481 Train BER 0.09570270031690598,                  \n",
      " [79/300] At -3.0 dB, Train Loss: 0.4772985279560089 Train BER 0.01918919011950493\n",
      "Time for one full iteration is 8.6777 minutes\n",
      "encoder learning rate: 9.67e-04, decoder learning rate: 9.67e-04\n",
      "[80/300] At -5.0 dB, Train Loss: 1.81296968460083 Train BER 0.09574594348669052,                  \n",
      " [80/300] At -3.0 dB, Train Loss: 0.4730820953845978 Train BER 0.01857297308743\n",
      "Time for one full iteration is 8.8633 minutes\n",
      "encoder learning rate: 9.65e-04, decoder learning rate: 9.65e-04\n",
      "[81/300] At -5.0 dB, Train Loss: 1.7964036464691162 Train BER 0.09503784030675888,                  \n",
      " [81/300] At -3.0 dB, Train Loss: 0.5040862560272217 Train BER 0.020372973755002022\n",
      "Time for one full iteration is 8.9696 minutes\n",
      "encoder learning rate: 9.63e-04, decoder learning rate: 9.63e-04\n",
      "[82/300] At -5.0 dB, Train Loss: 1.8975703716278076 Train BER 0.10080000013113022,                  \n",
      " [82/300] At -3.0 dB, Train Loss: 0.5174428820610046 Train BER 0.02099459432065487\n",
      "Time for one full iteration is 8.8101 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[83/300] At -5.0 dB, Train Loss: 1.761462926864624 Train BER 0.0911027044057846,                  \n",
      " [83/300] At -3.0 dB, Train Loss: 0.5568599700927734 Train BER 0.023816216737031937\n",
      "Time for one full iteration is 8.8998 minutes\n",
      "encoder learning rate: 9.58e-04, decoder learning rate: 9.58e-04\n",
      "[84/300] At -5.0 dB, Train Loss: 1.9365200996398926 Train BER 0.10304324328899384,                  \n",
      " [84/300] At -3.0 dB, Train Loss: 0.5086584687232971 Train BER 0.020113512873649597\n",
      "Time for one full iteration is 8.7298 minutes\n",
      "encoder learning rate: 9.55e-04, decoder learning rate: 9.55e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/300] At -5.0 dB, Train Loss: 1.7960513830184937 Train BER 0.09450270235538483,                  \n",
      " [85/300] At -3.0 dB, Train Loss: 0.5096733570098877 Train BER 0.021108107641339302\n",
      "Time for one full iteration is 8.8276 minutes\n",
      "encoder learning rate: 9.52e-04, decoder learning rate: 9.52e-04\n",
      "[86/300] At -5.0 dB, Train Loss: 1.9437133073806763 Train BER 0.10272432118654251,                  \n",
      " [86/300] At -3.0 dB, Train Loss: 0.5538478493690491 Train BER 0.022443242371082306\n",
      "Time for one full iteration is 8.8871 minutes\n",
      "encoder learning rate: 9.50e-04, decoder learning rate: 9.50e-04\n",
      "[87/300] At -5.0 dB, Train Loss: 1.7760443687438965 Train BER 0.09225405752658844,                  \n",
      " [87/300] At -3.0 dB, Train Loss: 0.477224737405777 Train BER 0.019502703100442886\n",
      "Time for one full iteration is 8.9906 minutes\n",
      "encoder learning rate: 9.47e-04, decoder learning rate: 9.47e-04\n",
      "[88/300] At -5.0 dB, Train Loss: 1.9518530368804932 Train BER 0.10347027331590652,                  \n",
      " [88/300] At -3.0 dB, Train Loss: 0.46378034353256226 Train BER 0.018772972747683525\n",
      "Time for one full iteration is 8.9232 minutes\n",
      "encoder learning rate: 9.44e-04, decoder learning rate: 9.44e-04\n",
      "[89/300] At -5.0 dB, Train Loss: 1.784840703010559 Train BER 0.09340540319681168,                  \n",
      " [89/300] At -3.0 dB, Train Loss: 0.4775754511356354 Train BER 0.02019459381699562\n",
      "Time for one full iteration is 9.0185 minutes\n",
      "encoder learning rate: 9.41e-04, decoder learning rate: 9.41e-04\n",
      "[90/300] At -5.0 dB, Train Loss: 1.9400955438613892 Train BER 0.10346486419439316,                  \n",
      " [90/300] At -3.0 dB, Train Loss: 0.48155882954597473 Train BER 0.019535135477781296\n",
      "Time for one full iteration is 9.0102 minutes\n",
      "encoder learning rate: 9.38e-04, decoder learning rate: 9.38e-04\n",
      "[91/300] At -5.0 dB, Train Loss: 1.743403673171997 Train BER 0.09304864704608917,                  \n",
      " [91/300] At -3.0 dB, Train Loss: 0.40051543712615967 Train BER 0.016508108004927635\n",
      "Time for one full iteration is 8.9640 minutes\n",
      "encoder learning rate: 9.35e-04, decoder learning rate: 9.35e-04\n",
      "[92/300] At -5.0 dB, Train Loss: 1.882920503616333 Train BER 0.10127027332782745,                  \n",
      " [92/300] At -3.0 dB, Train Loss: 0.39760687947273254 Train BER 0.015394594520330429\n",
      "Time for one full iteration is 8.9422 minutes\n",
      "encoder learning rate: 9.32e-04, decoder learning rate: 9.32e-04\n",
      "[93/300] At -5.0 dB, Train Loss: 1.7974642515182495 Train BER 0.09481621533632278,                  \n",
      " [93/300] At -3.0 dB, Train Loss: 0.42125046253204346 Train BER 0.017524324357509613\n",
      "Time for one full iteration is 8.8626 minutes\n",
      "encoder learning rate: 9.29e-04, decoder learning rate: 9.29e-04\n",
      "[94/300] At -5.0 dB, Train Loss: 1.8742103576660156 Train BER 0.09991351515054703,                  \n",
      " [94/300] At -3.0 dB, Train Loss: 0.40147486329078674 Train BER 0.01576216146349907\n",
      "Time for one full iteration is 8.8136 minutes\n",
      "encoder learning rate: 9.26e-04, decoder learning rate: 9.26e-04\n",
      "[95/300] At -5.0 dB, Train Loss: 1.741396188735962 Train BER 0.09204324334859848,                  \n",
      " [95/300] At -3.0 dB, Train Loss: 0.4403415024280548 Train BER 0.018589189276099205\n",
      "Time for one full iteration is 8.8965 minutes\n",
      "encoder learning rate: 9.22e-04, decoder learning rate: 9.22e-04\n",
      "[96/300] At -5.0 dB, Train Loss: 1.8764922618865967 Train BER 0.10156216472387314,                  \n",
      " [96/300] At -3.0 dB, Train Loss: 0.5555270314216614 Train BER 0.022654054686427116\n",
      "Time for one full iteration is 8.9419 minutes\n",
      "encoder learning rate: 9.19e-04, decoder learning rate: 9.19e-04\n",
      "[97/300] At -5.0 dB, Train Loss: 1.784886121749878 Train BER 0.09467027336359024,                  \n",
      " [97/300] At -3.0 dB, Train Loss: 0.4379388093948364 Train BER 0.017983783036470413\n",
      "Time for one full iteration is 8.9106 minutes\n",
      "encoder learning rate: 9.15e-04, decoder learning rate: 9.15e-04\n",
      "[98/300] At -5.0 dB, Train Loss: 1.7807791233062744 Train BER 0.09473513811826706,                  \n",
      " [98/300] At -3.0 dB, Train Loss: 0.4192987382411957 Train BER 0.016318919137120247\n",
      "Time for one full iteration is 8.9689 minutes\n",
      "encoder learning rate: 9.12e-04, decoder learning rate: 9.12e-04\n",
      "[99/300] At -5.0 dB, Train Loss: 1.770895004272461 Train BER 0.09284324198961258,                  \n",
      " [99/300] At -3.0 dB, Train Loss: 0.43711361289024353 Train BER 0.017637837678194046\n",
      "Time for one full iteration is 8.8748 minutes\n",
      "encoder learning rate: 9.08e-04, decoder learning rate: 9.08e-04\n",
      "[100/300] At -5.0 dB, Train Loss: 1.8176980018615723 Train BER 0.09695135056972504,                  \n",
      " [100/300] At -3.0 dB, Train Loss: 0.43949517607688904 Train BER 0.0180540531873703\n",
      "Time for one full iteration is 8.6159 minutes\n",
      "encoder learning rate: 9.05e-04, decoder learning rate: 9.05e-04\n",
      "[101/300] At -5.0 dB, Train Loss: 1.8211058378219604 Train BER 0.0956810787320137,                  \n",
      " [101/300] At -3.0 dB, Train Loss: 0.4350377023220062 Train BER 0.018254054710268974\n",
      "Time for one full iteration is 8.9964 minutes\n",
      "encoder learning rate: 9.01e-04, decoder learning rate: 9.01e-04\n",
      "[102/300] At -5.0 dB, Train Loss: 1.8203989267349243 Train BER 0.09683243185281754,                  \n",
      " [102/300] At -3.0 dB, Train Loss: 0.39931628108024597 Train BER 0.01618378423154354\n",
      "Time for one full iteration is 8.7329 minutes\n",
      "encoder learning rate: 8.97e-04, decoder learning rate: 8.97e-04\n",
      "[103/300] At -5.0 dB, Train Loss: 1.8193941116333008 Train BER 0.09784864634275436,                  \n",
      " [103/300] At -3.0 dB, Train Loss: 0.39317426085472107 Train BER 0.016178378835320473\n",
      "Time for one full iteration is 8.9162 minutes\n",
      "encoder learning rate: 8.93e-04, decoder learning rate: 8.93e-04\n",
      "[104/300] At -5.0 dB, Train Loss: 1.8137493133544922 Train BER 0.09732432663440704,                  \n",
      " [104/300] At -3.0 dB, Train Loss: 0.4015711545944214 Train BER 0.016751350834965706\n",
      "Time for one full iteration is 8.9523 minutes\n",
      "encoder learning rate: 8.89e-04, decoder learning rate: 8.89e-04\n",
      "[105/300] At -5.0 dB, Train Loss: 1.8048988580703735 Train BER 0.09814594686031342,                  \n",
      " [105/300] At -3.0 dB, Train Loss: 0.38844722509384155 Train BER 0.015210811048746109\n",
      "Time for one full iteration is 8.9753 minutes\n",
      "encoder learning rate: 8.85e-04, decoder learning rate: 8.85e-04\n",
      "[106/300] At -5.0 dB, Train Loss: 1.8504791259765625 Train BER 0.10037297010421753,                  \n",
      " [106/300] At -3.0 dB, Train Loss: 0.508537232875824 Train BER 0.021189188584685326\n",
      "Time for one full iteration is 9.0020 minutes\n",
      "encoder learning rate: 8.81e-04, decoder learning rate: 8.81e-04\n",
      "[107/300] At -5.0 dB, Train Loss: 1.7815117835998535 Train BER 0.09535135328769684,                  \n",
      " [107/300] At -3.0 dB, Train Loss: 0.4137328565120697 Train BER 0.016621621325612068\n",
      "Time for one full iteration is 9.0032 minutes\n",
      "encoder learning rate: 8.77e-04, decoder learning rate: 8.77e-04\n",
      "[108/300] At -5.0 dB, Train Loss: 1.8112167119979858 Train BER 0.0977567583322525,                  \n",
      " [108/300] At -3.0 dB, Train Loss: 0.3975314199924469 Train BER 0.01594054140150547\n",
      "Time for one full iteration is 8.8408 minutes\n",
      "encoder learning rate: 8.73e-04, decoder learning rate: 8.73e-04\n",
      "[109/300] At -5.0 dB, Train Loss: 1.7845121622085571 Train BER 0.0951189175248146,                  \n",
      " [109/300] At -3.0 dB, Train Loss: 0.3955649733543396 Train BER 0.01609729789197445\n",
      "Time for one full iteration is 8.9003 minutes\n",
      "encoder learning rate: 8.69e-04, decoder learning rate: 8.69e-04\n",
      "[110/300] At -5.0 dB, Train Loss: 1.7883338928222656 Train BER 0.0946270301938057,                  \n",
      " [110/300] At -3.0 dB, Train Loss: 0.406128466129303 Train BER 0.01643243245780468\n",
      "Time for one full iteration is 8.8614 minutes\n",
      "encoder learning rate: 8.65e-04, decoder learning rate: 8.65e-04\n",
      "[111/300] At -5.0 dB, Train Loss: 1.7387473583221436 Train BER 0.09230811148881912,                  \n",
      " [111/300] At -3.0 dB, Train Loss: 0.36195069551467896 Train BER 0.014567567966878414\n",
      "Time for one full iteration is 8.9105 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[112/300] At -5.0 dB, Train Loss: 1.8095378875732422 Train BER 0.09651891887187958,                  \n",
      " [112/300] At -3.0 dB, Train Loss: 0.37037256360054016 Train BER 0.015248648822307587\n",
      "Time for one full iteration is 8.8257 minutes\n",
      "encoder learning rate: 8.56e-04, decoder learning rate: 8.56e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/300] At -5.0 dB, Train Loss: 1.7837355136871338 Train BER 0.09490811079740524,                  \n",
      " [113/300] At -3.0 dB, Train Loss: 0.3800351321697235 Train BER 0.01536216214299202\n",
      "Time for one full iteration is 8.9307 minutes\n",
      "encoder learning rate: 8.51e-04, decoder learning rate: 8.51e-04\n",
      "[114/300] At -5.0 dB, Train Loss: 1.8298238515853882 Train BER 0.09824864566326141,                  \n",
      " [114/300] At -3.0 dB, Train Loss: 0.4308367967605591 Train BER 0.018956756219267845\n",
      "Time for one full iteration is 8.8766 minutes\n",
      "encoder learning rate: 8.47e-04, decoder learning rate: 8.47e-04\n",
      "[115/300] At -5.0 dB, Train Loss: 1.8286246061325073 Train BER 0.09865405410528183,                  \n",
      " [115/300] At -3.0 dB, Train Loss: 0.39963626861572266 Train BER 0.01658918894827366\n",
      "Time for one full iteration is 8.8012 minutes\n",
      "encoder learning rate: 8.42e-04, decoder learning rate: 8.42e-04\n",
      "[116/300] At -5.0 dB, Train Loss: 1.8198285102844238 Train BER 0.09801621735095978,                  \n",
      " [116/300] At -3.0 dB, Train Loss: 0.4221993386745453 Train BER 0.017756756395101547\n",
      "Time for one full iteration is 8.8073 minutes\n",
      "encoder learning rate: 8.38e-04, decoder learning rate: 8.38e-04\n",
      "[117/300] At -5.0 dB, Train Loss: 1.7748758792877197 Train BER 0.09462162107229233,                  \n",
      " [117/300] At -3.0 dB, Train Loss: 0.3802527189254761 Train BER 0.015156757086515427\n",
      "Time for one full iteration is 8.9607 minutes\n",
      "encoder learning rate: 8.33e-04, decoder learning rate: 8.33e-04\n",
      "[118/300] At -5.0 dB, Train Loss: 1.7875685691833496 Train BER 0.09597837924957275,                  \n",
      " [118/300] At -3.0 dB, Train Loss: 0.44450312852859497 Train BER 0.018437838181853294\n",
      "Time for one full iteration is 8.9211 minutes\n",
      "encoder learning rate: 8.28e-04, decoder learning rate: 8.28e-04\n",
      "[119/300] At -5.0 dB, Train Loss: 1.8439949750900269 Train BER 0.09951891750097275,                  \n",
      " [119/300] At -3.0 dB, Train Loss: 0.4680618941783905 Train BER 0.01961621642112732\n",
      "Time for one full iteration is 8.6511 minutes\n",
      "encoder learning rate: 8.24e-04, decoder learning rate: 8.24e-04\n",
      "[120/300] At -5.0 dB, Train Loss: 1.7397546768188477 Train BER 0.0923297330737114,                  \n",
      " [120/300] At -3.0 dB, Train Loss: 0.41655468940734863 Train BER 0.017545945942401886\n",
      "Time for one full iteration is 8.8527 minutes\n",
      "encoder learning rate: 8.19e-04, decoder learning rate: 8.19e-04\n",
      "[121/300] At -5.0 dB, Train Loss: 1.7566367387771606 Train BER 0.09404323995113373,                  \n",
      " [121/300] At -3.0 dB, Train Loss: 0.3807162344455719 Train BER 0.015248648822307587\n",
      "Time for one full iteration is 8.9067 minutes\n",
      "encoder learning rate: 8.14e-04, decoder learning rate: 8.14e-04\n",
      "[122/300] At -5.0 dB, Train Loss: 1.7800168991088867 Train BER 0.09426486492156982,                  \n",
      " [122/300] At -3.0 dB, Train Loss: 0.4005827009677887 Train BER 0.016810810193419456\n",
      "Time for one full iteration is 8.8260 minutes\n",
      "encoder learning rate: 8.09e-04, decoder learning rate: 8.09e-04\n",
      "[123/300] At -5.0 dB, Train Loss: 1.7309540510177612 Train BER 0.09220000356435776,                  \n",
      " [123/300] At -3.0 dB, Train Loss: 0.3564485013484955 Train BER 0.01441081054508686\n",
      "Time for one full iteration is 8.7799 minutes\n",
      "encoder learning rate: 8.04e-04, decoder learning rate: 8.04e-04\n",
      "[124/300] At -5.0 dB, Train Loss: 1.770898461341858 Train BER 0.09567567706108093,                  \n",
      " [124/300] At -3.0 dB, Train Loss: 0.34157925844192505 Train BER 0.013583783991634846\n",
      "Time for one full iteration is 8.9522 minutes\n",
      "encoder learning rate: 7.99e-04, decoder learning rate: 7.99e-04\n",
      "[125/300] At -5.0 dB, Train Loss: 1.759512186050415 Train BER 0.09461621940135956,                  \n",
      " [125/300] At -3.0 dB, Train Loss: 0.34557774662971497 Train BER 0.014183783903717995\n",
      "Time for one full iteration is 8.9499 minutes\n",
      "encoder learning rate: 7.94e-04, decoder learning rate: 7.94e-04\n",
      "[126/300] At -5.0 dB, Train Loss: 1.7995175123214722 Train BER 0.09743243455886841,                  \n",
      " [126/300] At -3.0 dB, Train Loss: 0.3594720661640167 Train BER 0.014891891740262508\n",
      "Time for one full iteration is 8.9818 minutes\n",
      "encoder learning rate: 7.89e-04, decoder learning rate: 7.89e-04\n",
      "[127/300] At -5.0 dB, Train Loss: 1.7788410186767578 Train BER 0.09554053843021393,                  \n",
      " [127/300] At -3.0 dB, Train Loss: 0.3725641071796417 Train BER 0.015735134482383728\n",
      "Time for one full iteration is 8.9495 minutes\n",
      "encoder learning rate: 7.84e-04, decoder learning rate: 7.84e-04\n",
      "[128/300] At -5.0 dB, Train Loss: 1.6998226642608643 Train BER 0.09241621941328049,                  \n",
      " [128/300] At -3.0 dB, Train Loss: 0.3372797966003418 Train BER 0.01387567538768053\n",
      "Time for one full iteration is 8.7719 minutes\n",
      "encoder learning rate: 7.79e-04, decoder learning rate: 7.79e-04\n",
      "[129/300] At -5.0 dB, Train Loss: 1.682979941368103 Train BER 0.09037838131189346,                  \n",
      " [129/300] At -3.0 dB, Train Loss: 0.3502424359321594 Train BER 0.015016215853393078\n",
      "Time for one full iteration is 8.8400 minutes\n",
      "encoder learning rate: 7.73e-04, decoder learning rate: 7.73e-04\n",
      "[130/300] At -5.0 dB, Train Loss: 1.7088432312011719 Train BER 0.09154053777456284,                  \n",
      " [130/300] At -3.0 dB, Train Loss: 0.35313525795936584 Train BER 0.014491891488432884\n",
      "Time for one full iteration is 8.7595 minutes\n",
      "encoder learning rate: 7.68e-04, decoder learning rate: 7.68e-04\n",
      "[131/300] At -5.0 dB, Train Loss: 1.7615935802459717 Train BER 0.09409189224243164,                  \n",
      " [131/300] At -3.0 dB, Train Loss: 0.4052375257015228 Train BER 0.01737837865948677\n",
      "Time for one full iteration is 8.8691 minutes\n",
      "encoder learning rate: 7.63e-04, decoder learning rate: 7.63e-04\n",
      "[132/300] At -5.0 dB, Train Loss: 1.7228249311447144 Train BER 0.0924810841679573,                  \n",
      " [132/300] At -3.0 dB, Train Loss: 0.36960360407829285 Train BER 0.014248648658394814\n",
      "Time for one full iteration is 8.9616 minutes\n",
      "encoder learning rate: 7.57e-04, decoder learning rate: 7.57e-04\n",
      "[133/300] At -5.0 dB, Train Loss: 1.740265130996704 Train BER 0.09294053912162781,                  \n",
      " [133/300] At -3.0 dB, Train Loss: 0.3741832971572876 Train BER 0.015643242746591568\n",
      "Time for one full iteration is 9.0139 minutes\n",
      "encoder learning rate: 7.52e-04, decoder learning rate: 7.52e-04\n",
      "[134/300] At -5.0 dB, Train Loss: 1.7724609375 Train BER 0.09384864568710327,                  \n",
      " [134/300] At -3.0 dB, Train Loss: 0.439763605594635 Train BER 0.018778378143906593\n",
      "Time for one full iteration is 8.8394 minutes\n",
      "encoder learning rate: 7.47e-04, decoder learning rate: 7.47e-04\n",
      "[135/300] At -5.0 dB, Train Loss: 1.7127408981323242 Train BER 0.09097297489643097,                  \n",
      " [135/300] At -3.0 dB, Train Loss: 0.4273949861526489 Train BER 0.017459459602832794\n",
      "Time for one full iteration is 8.8406 minutes\n",
      "encoder learning rate: 7.41e-04, decoder learning rate: 7.41e-04\n",
      "[136/300] At -5.0 dB, Train Loss: 1.7637189626693726 Train BER 0.09308107942342758,                  \n",
      " [136/300] At -3.0 dB, Train Loss: 0.4212208688259125 Train BER 0.017389189451932907\n",
      "Time for one full iteration is 8.6444 minutes\n",
      "encoder learning rate: 7.36e-04, decoder learning rate: 7.36e-04\n",
      "[137/300] At -5.0 dB, Train Loss: 1.7992126941680908 Train BER 0.09472972899675369,                  \n",
      " [137/300] At -3.0 dB, Train Loss: 0.3704759180545807 Train BER 0.014686486683785915\n",
      "Time for one full iteration is 8.7044 minutes\n",
      "encoder learning rate: 7.30e-04, decoder learning rate: 7.30e-04\n",
      "[138/300] At -5.0 dB, Train Loss: 1.7867257595062256 Train BER 0.0960054025053978,                  \n",
      " [138/300] At -3.0 dB, Train Loss: 0.3668366074562073 Train BER 0.014843243174254894\n",
      "Time for one full iteration is 8.9603 minutes\n",
      "encoder learning rate: 7.24e-04, decoder learning rate: 7.24e-04\n",
      "[139/300] At -5.0 dB, Train Loss: 1.760597825050354 Train BER 0.09260540455579758,                  \n",
      " [139/300] At -3.0 dB, Train Loss: 0.3320103883743286 Train BER 0.013486486859619617\n",
      "Time for one full iteration is 9.0074 minutes\n",
      "encoder learning rate: 7.19e-04, decoder learning rate: 7.19e-04\n",
      "[140/300] At -5.0 dB, Train Loss: 1.7043582201004028 Train BER 0.09043783694505692,                  \n",
      " [140/300] At -3.0 dB, Train Loss: 0.3307759761810303 Train BER 0.013470270670950413\n",
      "Time for one full iteration is 8.9181 minutes\n",
      "encoder learning rate: 7.13e-04, decoder learning rate: 7.13e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/300] At -5.0 dB, Train Loss: 1.702122688293457 Train BER 0.09097297489643097,                  \n",
      " [141/300] At -3.0 dB, Train Loss: 0.29178619384765625 Train BER 0.01181081123650074\n",
      "Time for one full iteration is 8.8470 minutes\n",
      "encoder learning rate: 7.07e-04, decoder learning rate: 7.07e-04\n",
      "[142/300] At -5.0 dB, Train Loss: 1.7189401388168335 Train BER 0.09112972766160965,                  \n",
      " [142/300] At -3.0 dB, Train Loss: 0.30498695373535156 Train BER 0.012572973035275936\n",
      "Time for one full iteration is 8.9246 minutes\n",
      "encoder learning rate: 7.02e-04, decoder learning rate: 7.02e-04\n",
      "[143/300] At -5.0 dB, Train Loss: 1.6646003723144531 Train BER 0.08851891756057739,                  \n",
      " [143/300] At -3.0 dB, Train Loss: 0.27976834774017334 Train BER 0.0109243243932724\n",
      "Time for one full iteration is 8.9896 minutes\n",
      "encoder learning rate: 6.96e-04, decoder learning rate: 6.96e-04\n",
      "[144/300] At -5.0 dB, Train Loss: 1.7275186777114868 Train BER 0.09293513745069504,                  \n",
      " [144/300] At -3.0 dB, Train Loss: 0.29837554693222046 Train BER 0.012491892091929913\n",
      "Time for one full iteration is 8.8941 minutes\n",
      "encoder learning rate: 6.90e-04, decoder learning rate: 6.90e-04\n",
      "[145/300] At -5.0 dB, Train Loss: 1.6440986394882202 Train BER 0.08711891621351242,                  \n",
      " [145/300] At -3.0 dB, Train Loss: 0.29914411902427673 Train BER 0.012140540406107903\n",
      "Time for one full iteration is 8.6617 minutes\n",
      "encoder learning rate: 6.84e-04, decoder learning rate: 6.84e-04\n",
      "[146/300] At -5.0 dB, Train Loss: 1.7094024419784546 Train BER 0.09235135465860367,                  \n",
      " [146/300] At -3.0 dB, Train Loss: 0.31538060307502747 Train BER 0.013113513588905334\n",
      "Time for one full iteration is 8.8344 minutes\n",
      "encoder learning rate: 6.79e-04, decoder learning rate: 6.79e-04\n",
      "[147/300] At -5.0 dB, Train Loss: 1.7151284217834473 Train BER 0.09228108078241348,                  \n",
      " [147/300] At -3.0 dB, Train Loss: 0.267979234457016 Train BER 0.0102648651227355\n",
      "Time for one full iteration is 8.6764 minutes\n",
      "encoder learning rate: 6.73e-04, decoder learning rate: 6.73e-04\n",
      "[148/300] At -5.0 dB, Train Loss: 1.6349310874938965 Train BER 0.08708108216524124,                  \n",
      " [148/300] At -3.0 dB, Train Loss: 0.3048679232597351 Train BER 0.01193513534963131\n",
      "Time for one full iteration is 8.8110 minutes\n",
      "encoder learning rate: 6.67e-04, decoder learning rate: 6.67e-04\n",
      "[149/300] At -5.0 dB, Train Loss: 1.7384432554244995 Train BER 0.09333513677120209,                  \n",
      " [149/300] At -3.0 dB, Train Loss: 0.3233480453491211 Train BER 0.013416215777397156\n",
      "Time for one full iteration is 8.6915 minutes\n",
      "encoder learning rate: 6.61e-04, decoder learning rate: 6.61e-04\n",
      "[150/300] At -5.0 dB, Train Loss: 1.60749351978302 Train BER 0.08492432534694672,                  \n",
      " [150/300] At -3.0 dB, Train Loss: 0.3148226737976074 Train BER 0.012600000016391277\n",
      "Time for one full iteration is 8.8861 minutes\n",
      "encoder learning rate: 6.55e-04, decoder learning rate: 6.55e-04\n",
      "[151/300] At -5.0 dB, Train Loss: 1.7170255184173584 Train BER 0.09276216477155685,                  \n",
      " [151/300] At -3.0 dB, Train Loss: 0.2695268392562866 Train BER 0.010454053990542889\n",
      "Time for one full iteration is 8.7966 minutes\n",
      "encoder learning rate: 6.49e-04, decoder learning rate: 6.49e-04\n",
      "[152/300] At -5.0 dB, Train Loss: 1.6131986379623413 Train BER 0.0860162153840065,                  \n",
      " [152/300] At -3.0 dB, Train Loss: 0.28952038288116455 Train BER 0.011470270343124866\n",
      "Time for one full iteration is 8.8622 minutes\n",
      "encoder learning rate: 6.43e-04, decoder learning rate: 6.43e-04\n",
      "[153/300] At -5.0 dB, Train Loss: 1.727219820022583 Train BER 0.09265945851802826,                  \n",
      " [153/300] At -3.0 dB, Train Loss: 0.2710490822792053 Train BER 0.010875675827264786\n",
      "Time for one full iteration is 8.7676 minutes\n",
      "encoder learning rate: 6.37e-04, decoder learning rate: 6.37e-04\n",
      "[154/300] At -5.0 dB, Train Loss: 1.6080131530761719 Train BER 0.08554594963788986,                  \n",
      " [154/300] At -3.0 dB, Train Loss: 0.2645312547683716 Train BER 0.010486486367881298\n",
      "Time for one full iteration is 8.9785 minutes\n",
      "encoder learning rate: 6.31e-04, decoder learning rate: 6.31e-04\n",
      "[155/300] At -5.0 dB, Train Loss: 1.7551019191741943 Train BER 0.09356756508350372,                  \n",
      " [155/300] At -3.0 dB, Train Loss: 0.26791438460350037 Train BER 0.01066486444324255\n",
      "Time for one full iteration is 8.9947 minutes\n",
      "encoder learning rate: 6.25e-04, decoder learning rate: 6.25e-04\n",
      "[156/300] At -5.0 dB, Train Loss: 1.638374924659729 Train BER 0.08677297085523605,                  \n",
      " [156/300] At -3.0 dB, Train Loss: 0.2913969159126282 Train BER 0.012010810896754265\n",
      "Time for one full iteration is 8.8778 minutes\n",
      "encoder learning rate: 6.19e-04, decoder learning rate: 6.19e-04\n",
      "[157/300] At -5.0 dB, Train Loss: 1.7306255102157593 Train BER 0.09369189292192459,                  \n",
      " [157/300] At -3.0 dB, Train Loss: 0.2800934314727783 Train BER 0.011264865286648273\n",
      "Time for one full iteration is 8.9257 minutes\n",
      "encoder learning rate: 6.13e-04, decoder learning rate: 6.13e-04\n",
      "[158/300] At -5.0 dB, Train Loss: 1.6467269659042358 Train BER 0.08687567710876465,                  \n",
      " [158/300] At -3.0 dB, Train Loss: 0.2681698501110077 Train BER 0.010718919336795807\n",
      "Time for one full iteration is 8.8589 minutes\n",
      "encoder learning rate: 6.06e-04, decoder learning rate: 6.06e-04\n",
      "[159/300] At -5.0 dB, Train Loss: 1.699252963066101 Train BER 0.09102702885866165,                  \n",
      " [159/300] At -3.0 dB, Train Loss: 0.30551546812057495 Train BER 0.012643243186175823\n",
      "Time for one full iteration is 8.8934 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[160/300] At -5.0 dB, Train Loss: 1.635809063911438 Train BER 0.0877351388335228,                  \n",
      " [160/300] At -3.0 dB, Train Loss: 0.2738887071609497 Train BER 0.011070270091295242\n",
      "Time for one full iteration is 8.5644 minutes\n",
      "encoder learning rate: 5.94e-04, decoder learning rate: 5.94e-04\n",
      "[161/300] At -5.0 dB, Train Loss: 1.7181642055511475 Train BER 0.09337297081947327,                  \n",
      " [161/300] At -3.0 dB, Train Loss: 0.2641085386276245 Train BER 0.010810811072587967\n",
      "Time for one full iteration is 8.5119 minutes\n",
      "encoder learning rate: 5.88e-04, decoder learning rate: 5.88e-04\n",
      "[162/300] At -5.0 dB, Train Loss: 1.6618438959121704 Train BER 0.08964864909648895,                  \n",
      " [162/300] At -3.0 dB, Train Loss: 0.2589336335659027 Train BER 0.01000540517270565\n",
      "Time for one full iteration is 8.6110 minutes\n",
      "encoder learning rate: 5.82e-04, decoder learning rate: 5.82e-04\n",
      "[163/300] At -5.0 dB, Train Loss: 1.7598249912261963 Train BER 0.09520000219345093,                  \n",
      " [163/300] At -3.0 dB, Train Loss: 0.398569792509079 Train BER 0.015918919816613197\n",
      "Time for one full iteration is 8.5508 minutes\n",
      "encoder learning rate: 5.76e-04, decoder learning rate: 5.76e-04\n",
      "[164/300] At -5.0 dB, Train Loss: 1.6004523038864136 Train BER 0.08438918739557266,                  \n",
      " [164/300] At -3.0 dB, Train Loss: 0.30066514015197754 Train BER 0.012637837789952755\n",
      "Time for one full iteration is 8.4641 minutes\n",
      "encoder learning rate: 5.69e-04, decoder learning rate: 5.69e-04\n",
      "[165/300] At -5.0 dB, Train Loss: 1.7431567907333374 Train BER 0.09508108347654343,                  \n",
      " [165/300] At -3.0 dB, Train Loss: 0.2787277400493622 Train BER 0.011286486871540546\n",
      "Time for one full iteration is 8.4733 minutes\n",
      "encoder learning rate: 5.63e-04, decoder learning rate: 5.63e-04\n",
      "[166/300] At -5.0 dB, Train Loss: 1.627988338470459 Train BER 0.08758918941020966,                  \n",
      " [166/300] At -3.0 dB, Train Loss: 0.290406197309494 Train BER 0.011832432821393013\n",
      "Time for one full iteration is 8.4334 minutes\n",
      "encoder learning rate: 5.57e-04, decoder learning rate: 5.57e-04\n",
      "[167/300] At -5.0 dB, Train Loss: 1.698472499847412 Train BER 0.09132973104715347,                  \n",
      " [167/300] At -3.0 dB, Train Loss: 0.26650387048721313 Train BER 0.010713513940572739\n",
      "Time for one full iteration is 8.4305 minutes\n",
      "encoder learning rate: 5.51e-04, decoder learning rate: 5.51e-04\n",
      "[168/300] At -5.0 dB, Train Loss: 1.6280440092086792 Train BER 0.08682702481746674,                  \n",
      " [168/300] At -3.0 dB, Train Loss: 0.264299213886261 Train BER 0.010281081311404705\n",
      "Time for one full iteration is 8.6116 minutes\n",
      "encoder learning rate: 5.44e-04, decoder learning rate: 5.44e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169/300] At -5.0 dB, Train Loss: 1.7084264755249023 Train BER 0.09275675565004349,                  \n",
      " [169/300] At -3.0 dB, Train Loss: 0.2553369402885437 Train BER 0.010064864531159401\n",
      "Time for one full iteration is 8.5436 minutes\n",
      "encoder learning rate: 5.38e-04, decoder learning rate: 5.38e-04\n",
      "[170/300] At -5.0 dB, Train Loss: 1.632710576057434 Train BER 0.08587567508220673,                  \n",
      " [170/300] At -3.0 dB, Train Loss: 0.2502479553222656 Train BER 0.010270270518958569\n",
      "Time for one full iteration is 8.7729 minutes\n",
      "encoder learning rate: 5.32e-04, decoder learning rate: 5.32e-04\n",
      "[171/300] At -5.0 dB, Train Loss: 1.7070226669311523 Train BER 0.09250270575284958,                  \n",
      " [171/300] At -3.0 dB, Train Loss: 0.2979530394077301 Train BER 0.012421621941030025\n",
      "Time for one full iteration is 8.7734 minutes\n",
      "encoder learning rate: 5.26e-04, decoder learning rate: 5.26e-04\n",
      "[172/300] At -5.0 dB, Train Loss: 1.6431427001953125 Train BER 0.08751891553401947,                  \n",
      " [172/300] At -3.0 dB, Train Loss: 0.2581891715526581 Train BER 0.01003783755004406\n",
      "Time for one full iteration is 8.5415 minutes\n",
      "encoder learning rate: 5.19e-04, decoder learning rate: 5.19e-04\n",
      "[173/300] At -5.0 dB, Train Loss: 1.6943491697311401 Train BER 0.09137837588787079,                  \n",
      " [173/300] At -3.0 dB, Train Loss: 0.2582951486110687 Train BER 0.010470270179212093\n",
      "Time for one full iteration is 8.5709 minutes\n",
      "encoder learning rate: 5.13e-04, decoder learning rate: 5.13e-04\n",
      "[174/300] At -5.0 dB, Train Loss: 1.6743794679641724 Train BER 0.08972432464361191,                  \n",
      " [174/300] At -3.0 dB, Train Loss: 0.24449379742145538 Train BER 0.0099459458142519\n",
      "Time for one full iteration is 8.5056 minutes\n",
      "encoder learning rate: 5.07e-04, decoder learning rate: 5.07e-04\n",
      "[175/300] At -5.0 dB, Train Loss: 1.6309922933578491 Train BER 0.08743242919445038,                  \n",
      " [175/300] At -3.0 dB, Train Loss: 0.23318538069725037 Train BER 0.009383783675730228\n",
      "Time for one full iteration is 8.6842 minutes\n",
      "encoder learning rate: 5.01e-04, decoder learning rate: 5.01e-04\n",
      "[176/300] At -5.0 dB, Train Loss: 1.682991623878479 Train BER 0.09103783965110779,                  \n",
      " [176/300] At -3.0 dB, Train Loss: 0.2529734671115875 Train BER 0.010756757110357285\n",
      "Time for one full iteration is 8.6780 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[177/300] At -5.0 dB, Train Loss: 1.6416802406311035 Train BER 0.08848108351230621,                  \n",
      " [177/300] At -3.0 dB, Train Loss: 0.24342437088489532 Train BER 0.00991351343691349\n",
      "Time for one full iteration is 8.5857 minutes\n",
      "encoder learning rate: 4.88e-04, decoder learning rate: 4.88e-04\n",
      "[178/300] At -5.0 dB, Train Loss: 1.639108419418335 Train BER 0.08929729461669922,                  \n",
      " [178/300] At -3.0 dB, Train Loss: 0.25787442922592163 Train BER 0.010502702556550503\n",
      "Time for one full iteration is 8.5788 minutes\n",
      "encoder learning rate: 4.82e-04, decoder learning rate: 4.82e-04\n",
      "[179/300] At -5.0 dB, Train Loss: 1.6551889181137085 Train BER 0.08994054049253464,                  \n",
      " [179/300] At -3.0 dB, Train Loss: 0.25531885027885437 Train BER 0.010529729537665844\n",
      "Time for one full iteration is 8.6229 minutes\n",
      "encoder learning rate: 4.75e-04, decoder learning rate: 4.75e-04\n",
      "[180/300] At -5.0 dB, Train Loss: 1.6350641250610352 Train BER 0.0880756750702858,                  \n",
      " [180/300] At -3.0 dB, Train Loss: 0.2360682636499405 Train BER 0.009405405260622501\n",
      "Time for one full iteration is 8.5837 minutes\n",
      "encoder learning rate: 4.69e-04, decoder learning rate: 4.69e-04\n",
      "[181/300] At -5.0 dB, Train Loss: 1.5689769983291626 Train BER 0.08435135334730148,                  \n",
      " [181/300] At -3.0 dB, Train Loss: 0.2360205352306366 Train BER 0.009518918581306934\n",
      "Time for one full iteration is 8.6560 minutes\n",
      "encoder learning rate: 4.63e-04, decoder learning rate: 4.63e-04\n",
      "[182/300] At -5.0 dB, Train Loss: 1.609898567199707 Train BER 0.08516756445169449,                  \n",
      " [182/300] At -3.0 dB, Train Loss: 0.22954559326171875 Train BER 0.009145946241915226\n",
      "Time for one full iteration is 8.5751 minutes\n",
      "encoder learning rate: 4.57e-04, decoder learning rate: 4.57e-04\n",
      "[183/300] At -5.0 dB, Train Loss: 1.603572964668274 Train BER 0.08631351590156555,                  \n",
      " [183/300] At -3.0 dB, Train Loss: 0.2403596043586731 Train BER 0.009962162002921104\n",
      "Time for one full iteration is 8.6173 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[184/300] At -5.0 dB, Train Loss: 1.6039642095565796 Train BER 0.08525405079126358,                  \n",
      " [184/300] At -3.0 dB, Train Loss: 0.218903049826622 Train BER 0.008654054254293442\n",
      "Time for one full iteration is 8.6236 minutes\n",
      "encoder learning rate: 4.44e-04, decoder learning rate: 4.44e-04\n",
      "[185/300] At -5.0 dB, Train Loss: 1.6362398862838745 Train BER 0.0889945924282074,                  \n",
      " [185/300] At -3.0 dB, Train Loss: 0.22925761342048645 Train BER 0.00931891892105341\n",
      "Time for one full iteration is 8.6162 minutes\n",
      "encoder learning rate: 4.38e-04, decoder learning rate: 4.38e-04\n",
      "[186/300] At -5.0 dB, Train Loss: 1.6369903087615967 Train BER 0.08812972903251648,                  \n",
      " [186/300] At -3.0 dB, Train Loss: 0.23361948132514954 Train BER 0.009572972543537617\n",
      "Time for one full iteration is 8.6002 minutes\n",
      "encoder learning rate: 4.32e-04, decoder learning rate: 4.32e-04\n",
      "[187/300] At -5.0 dB, Train Loss: 1.6363354921340942 Train BER 0.0882973000407219,                  \n",
      " [187/300] At -3.0 dB, Train Loss: 0.24308940768241882 Train BER 0.010178378783166409\n",
      "Time for one full iteration is 8.5791 minutes\n",
      "encoder learning rate: 4.25e-04, decoder learning rate: 4.25e-04\n",
      "[188/300] At -5.0 dB, Train Loss: 1.5783265829086304 Train BER 0.08449189364910126,                  \n",
      " [188/300] At -3.0 dB, Train Loss: 0.22645576298236847 Train BER 0.009059459902346134\n",
      "Time for one full iteration is 8.5456 minutes\n",
      "encoder learning rate: 4.19e-04, decoder learning rate: 4.19e-04\n",
      "[189/300] At -5.0 dB, Train Loss: 1.6141774654388428 Train BER 0.08661621809005737,                  \n",
      " [189/300] At -3.0 dB, Train Loss: 0.24754589796066284 Train BER 0.00976216234266758\n",
      "Time for one full iteration is 8.5362 minutes\n",
      "encoder learning rate: 4.13e-04, decoder learning rate: 4.13e-04\n",
      "[190/300] At -5.0 dB, Train Loss: 1.5396991968154907 Train BER 0.08160000294446945,                  \n",
      " [190/300] At -3.0 dB, Train Loss: 0.23734155297279358 Train BER 0.009394594468176365\n",
      "Time for one full iteration is 8.6733 minutes\n",
      "encoder learning rate: 4.07e-04, decoder learning rate: 4.07e-04\n",
      "[191/300] At -5.0 dB, Train Loss: 1.6266440153121948 Train BER 0.08758918941020966,                  \n",
      " [191/300] At -3.0 dB, Train Loss: 0.2448859065771103 Train BER 0.010232432745397091\n",
      "Time for one full iteration is 8.6264 minutes\n",
      "encoder learning rate: 4.01e-04, decoder learning rate: 4.01e-04\n",
      "[192/300] At -5.0 dB, Train Loss: 1.5776326656341553 Train BER 0.08417297154664993,                  \n",
      " [192/300] At -3.0 dB, Train Loss: 0.2392820566892624 Train BER 0.009583783335983753\n",
      "Time for one full iteration is 8.5423 minutes\n",
      "encoder learning rate: 3.95e-04, decoder learning rate: 3.95e-04\n",
      "[193/300] At -5.0 dB, Train Loss: 1.6317397356033325 Train BER 0.08796756714582443,                  \n",
      " [193/300] At -3.0 dB, Train Loss: 0.24039612710475922 Train BER 0.009848648682236671\n",
      "Time for one full iteration is 8.5348 minutes\n",
      "encoder learning rate: 3.88e-04, decoder learning rate: 3.88e-04\n",
      "[194/300] At -5.0 dB, Train Loss: 1.5765506029129028 Train BER 0.08362703025341034,                  \n",
      " [194/300] At -3.0 dB, Train Loss: 0.22975534200668335 Train BER 0.00941081065684557\n",
      "Time for one full iteration is 8.5704 minutes\n",
      "encoder learning rate: 3.82e-04, decoder learning rate: 3.82e-04\n",
      "[195/300] At -5.0 dB, Train Loss: 1.5924519300460815 Train BER 0.08522702753543854,                  \n",
      " [195/300] At -3.0 dB, Train Loss: 0.24355247616767883 Train BER 0.009956756606698036\n",
      "Time for one full iteration is 8.6237 minutes\n",
      "encoder learning rate: 3.76e-04, decoder learning rate: 3.76e-04\n",
      "[196/300] At -5.0 dB, Train Loss: 1.5808607339859009 Train BER 0.08397837728261948,                  \n",
      " [196/300] At -3.0 dB, Train Loss: 0.22466139495372772 Train BER 0.008816216140985489\n",
      "Time for one full iteration is 8.5597 minutes\n",
      "encoder learning rate: 3.70e-04, decoder learning rate: 3.70e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/300] At -5.0 dB, Train Loss: 1.6552518606185913 Train BER 0.08946486562490463,                  \n",
      " [197/300] At -3.0 dB, Train Loss: 0.22629863023757935 Train BER 0.009270270355045795\n",
      "Time for one full iteration is 8.5880 minutes\n",
      "encoder learning rate: 3.64e-04, decoder learning rate: 3.64e-04\n",
      "[198/300] At -5.0 dB, Train Loss: 1.5848947763442993 Train BER 0.08467567712068558,                  \n",
      " [198/300] At -3.0 dB, Train Loss: 0.22013932466506958 Train BER 0.008929729461669922\n",
      "Time for one full iteration is 8.6775 minutes\n",
      "encoder learning rate: 3.58e-04, decoder learning rate: 3.58e-04\n",
      "[199/300] At -5.0 dB, Train Loss: 1.6063346862792969 Train BER 0.08665405213832855,                  \n",
      " [199/300] At -3.0 dB, Train Loss: 0.23004411160945892 Train BER 0.009264864958822727\n",
      "Time for one full iteration is 8.8534 minutes\n",
      "encoder learning rate: 3.52e-04, decoder learning rate: 3.52e-04\n",
      "[200/300] At -5.0 dB, Train Loss: 1.5635367631912231 Train BER 0.08240000158548355,                  \n",
      " [200/300] At -3.0 dB, Train Loss: 0.2278348207473755 Train BER 0.009259459562599659\n",
      "Time for one full iteration is 8.8069 minutes\n",
      "encoder learning rate: 3.46e-04, decoder learning rate: 3.46e-04\n",
      "[201/300] At -5.0 dB, Train Loss: 1.632133960723877 Train BER 0.08870270103216171,                  \n",
      " [201/300] At -3.0 dB, Train Loss: 0.20773327350616455 Train BER 0.008427026681602001\n",
      "Time for one full iteration is 8.8181 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[202/300] At -5.0 dB, Train Loss: 1.5997110605239868 Train BER 0.08595675975084305,                  \n",
      " [202/300] At -3.0 dB, Train Loss: 0.2025601863861084 Train BER 0.00744324317201972\n",
      "Time for one full iteration is 8.7802 minutes\n",
      "encoder learning rate: 3.34e-04, decoder learning rate: 3.34e-04\n",
      "[203/300] At -5.0 dB, Train Loss: 1.5993013381958008 Train BER 0.08558919280767441,                  \n",
      " [203/300] At -3.0 dB, Train Loss: 0.2082507163286209 Train BER 0.008167567662894726\n",
      "Time for one full iteration is 8.4020 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[204/300] At -5.0 dB, Train Loss: 1.590181589126587 Train BER 0.08546486496925354,                  \n",
      " [204/300] At -3.0 dB, Train Loss: 0.21017833054065704 Train BER 0.008664865046739578\n",
      "Time for one full iteration is 8.3992 minutes\n",
      "encoder learning rate: 3.22e-04, decoder learning rate: 3.22e-04\n",
      "[205/300] At -5.0 dB, Train Loss: 1.6175477504730225 Train BER 0.0877675712108612,                  \n",
      " [205/300] At -3.0 dB, Train Loss: 0.22816941142082214 Train BER 0.00937837827950716\n",
      "Time for one full iteration is 8.3789 minutes\n",
      "encoder learning rate: 3.17e-04, decoder learning rate: 3.17e-04\n",
      "[206/300] At -5.0 dB, Train Loss: 1.5731759071350098 Train BER 0.08311351388692856,                  \n",
      " [206/300] At -3.0 dB, Train Loss: 0.2161635160446167 Train BER 0.008664865046739578\n",
      "Time for one full iteration is 8.2740 minutes\n",
      "encoder learning rate: 3.11e-04, decoder learning rate: 3.11e-04\n",
      "[207/300] At -5.0 dB, Train Loss: 1.6059889793395996 Train BER 0.08654054254293442,                  \n",
      " [207/300] At -3.0 dB, Train Loss: 0.21350052952766418 Train BER 0.0083081079646945\n",
      "Time for one full iteration is 8.2913 minutes\n",
      "encoder learning rate: 3.05e-04, decoder learning rate: 3.05e-04\n",
      "[208/300] At -5.0 dB, Train Loss: 1.5789512395858765 Train BER 0.08542702347040176,                  \n",
      " [208/300] At -3.0 dB, Train Loss: 0.20597527921199799 Train BER 0.007735135033726692\n",
      "Time for one full iteration is 8.2567 minutes\n",
      "encoder learning rate: 2.99e-04, decoder learning rate: 2.99e-04\n",
      "[209/300] At -5.0 dB, Train Loss: 1.6420471668243408 Train BER 0.08752432465553284,                  \n",
      " [209/300] At -3.0 dB, Train Loss: 0.2296285778284073 Train BER 0.009475675411522388\n",
      "Time for one full iteration is 8.2669 minutes\n",
      "encoder learning rate: 2.94e-04, decoder learning rate: 2.94e-04\n",
      "[210/300] At -5.0 dB, Train Loss: 1.6212031841278076 Train BER 0.08695675432682037,                  \n",
      " [210/300] At -3.0 dB, Train Loss: 0.20627877116203308 Train BER 0.008108108304440975\n",
      "Time for one full iteration is 8.2560 minutes\n",
      "encoder learning rate: 2.88e-04, decoder learning rate: 2.88e-04\n",
      "[211/300] At -5.0 dB, Train Loss: 1.5867303609848022 Train BER 0.08467567712068558,                  \n",
      " [211/300] At -3.0 dB, Train Loss: 0.21257586777210236 Train BER 0.0086270272731781\n",
      "Time for one full iteration is 8.2616 minutes\n",
      "encoder learning rate: 2.82e-04, decoder learning rate: 2.82e-04\n",
      "[212/300] At -5.0 dB, Train Loss: 1.5889153480529785 Train BER 0.08457297086715698,                  \n",
      " [212/300] At -3.0 dB, Train Loss: 0.2178315669298172 Train BER 0.008821621537208557\n",
      "Time for one full iteration is 8.2213 minutes\n",
      "encoder learning rate: 2.77e-04, decoder learning rate: 2.77e-04\n",
      "[213/300] At -5.0 dB, Train Loss: 1.6164807081222534 Train BER 0.08717837929725647,                  \n",
      " [213/300] At -3.0 dB, Train Loss: 0.23114633560180664 Train BER 0.009297297336161137\n",
      "Time for one full iteration is 8.2256 minutes\n",
      "encoder learning rate: 2.71e-04, decoder learning rate: 2.71e-04\n",
      "[214/300] At -5.0 dB, Train Loss: 1.5643796920776367 Train BER 0.08421081304550171,                  \n",
      " [214/300] At -3.0 dB, Train Loss: 0.22083084285259247 Train BER 0.00871891900897026\n",
      "Time for one full iteration is 8.2135 minutes\n",
      "encoder learning rate: 2.65e-04, decoder learning rate: 2.65e-04\n",
      "[215/300] At -5.0 dB, Train Loss: 1.5685484409332275 Train BER 0.08391892164945602,                  \n",
      " [215/300] At -3.0 dB, Train Loss: 0.21151220798492432 Train BER 0.008664865046739578\n",
      "Time for one full iteration is 8.2249 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[216/300] At -5.0 dB, Train Loss: 1.5786936283111572 Train BER 0.08405405282974243,                  \n",
      " [216/300] At -3.0 dB, Train Loss: 0.20272263884544373 Train BER 0.008086486719548702\n",
      "Time for one full iteration is 8.2017 minutes\n",
      "encoder learning rate: 2.54e-04, decoder learning rate: 2.54e-04\n",
      "[217/300] At -5.0 dB, Train Loss: 1.5684763193130493 Train BER 0.08294054120779037,                  \n",
      " [217/300] At -3.0 dB, Train Loss: 0.20313149690628052 Train BER 0.00792432390153408\n",
      "Time for one full iteration is 8.2134 minutes\n",
      "encoder learning rate: 2.49e-04, decoder learning rate: 2.49e-04\n",
      "[218/300] At -5.0 dB, Train Loss: 1.585701584815979 Train BER 0.08376757055521011,                  \n",
      " [218/300] At -3.0 dB, Train Loss: 0.2203756719827652 Train BER 0.008637838065624237\n",
      "Time for one full iteration is 8.2175 minutes\n",
      "encoder learning rate: 2.44e-04, decoder learning rate: 2.44e-04\n",
      "[219/300] At -5.0 dB, Train Loss: 1.5600099563598633 Train BER 0.08330269902944565,                  \n",
      " [219/300] At -3.0 dB, Train Loss: 0.22079487144947052 Train BER 0.009043242782354355\n",
      "Time for one full iteration is 8.2461 minutes\n",
      "encoder learning rate: 2.38e-04, decoder learning rate: 2.38e-04\n",
      "[220/300] At -5.0 dB, Train Loss: 1.6105408668518066 Train BER 0.0862378403544426,                  \n",
      " [220/300] At -3.0 dB, Train Loss: 0.2155645191669464 Train BER 0.00869189202785492\n",
      "Time for one full iteration is 8.2108 minutes\n",
      "encoder learning rate: 2.33e-04, decoder learning rate: 2.33e-04\n",
      "[221/300] At -5.0 dB, Train Loss: 1.5821340084075928 Train BER 0.08479459583759308,                  \n",
      " [221/300] At -3.0 dB, Train Loss: 0.21388399600982666 Train BER 0.008518919348716736\n",
      "Time for one full iteration is 8.2219 minutes\n",
      "encoder learning rate: 2.28e-04, decoder learning rate: 2.28e-04\n",
      "[222/300] At -5.0 dB, Train Loss: 1.5927343368530273 Train BER 0.08423243463039398,                  \n",
      " [222/300] At -3.0 dB, Train Loss: 0.21548131108283997 Train BER 0.008924324065446854\n",
      "Time for one full iteration is 8.2109 minutes\n",
      "encoder learning rate: 2.22e-04, decoder learning rate: 2.22e-04\n",
      "[223/300] At -5.0 dB, Train Loss: 1.5984084606170654 Train BER 0.08548107743263245,                  \n",
      " [223/300] At -3.0 dB, Train Loss: 0.19960670173168182 Train BER 0.00792432390153408\n",
      "Time for one full iteration is 8.1915 minutes\n",
      "encoder learning rate: 2.17e-04, decoder learning rate: 2.17e-04\n",
      "[224/300] At -5.0 dB, Train Loss: 1.6102815866470337 Train BER 0.08688107877969742,                  \n",
      " [224/300] At -3.0 dB, Train Loss: 0.20257408916950226 Train BER 0.008081081323325634\n",
      "Time for one full iteration is 8.2173 minutes\n",
      "encoder learning rate: 2.12e-04, decoder learning rate: 2.12e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[225/300] At -5.0 dB, Train Loss: 1.6157116889953613 Train BER 0.08682162314653397,                  \n",
      " [225/300] At -3.0 dB, Train Loss: 0.19349785149097443 Train BER 0.0073837838135659695\n",
      "Time for one full iteration is 8.2205 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[226/300] At -5.0 dB, Train Loss: 1.5916032791137695 Train BER 0.08501081168651581,                  \n",
      " [226/300] At -3.0 dB, Train Loss: 0.21546943485736847 Train BER 0.008972972631454468\n",
      "Time for one full iteration is 8.1804 minutes\n",
      "encoder learning rate: 2.02e-04, decoder learning rate: 2.02e-04\n",
      "[227/300] At -5.0 dB, Train Loss: 1.559462547302246 Train BER 0.08372972905635834,                  \n",
      " [227/300] At -3.0 dB, Train Loss: 0.2122991681098938 Train BER 0.00845945905894041\n",
      "Time for one full iteration is 8.2142 minutes\n",
      "encoder learning rate: 1.97e-04, decoder learning rate: 1.97e-04\n",
      "[228/300] At -5.0 dB, Train Loss: 1.5917472839355469 Train BER 0.0853189155459404,                  \n",
      " [228/300] At -3.0 dB, Train Loss: 0.20580288767814636 Train BER 0.008135135285556316\n",
      "Time for one full iteration is 8.2012 minutes\n",
      "encoder learning rate: 1.92e-04, decoder learning rate: 1.92e-04\n",
      "[229/300] At -5.0 dB, Train Loss: 1.553673267364502 Train BER 0.08271891623735428,                  \n",
      " [229/300] At -3.0 dB, Train Loss: 0.22060002386569977 Train BER 0.008918918669223785\n",
      "Time for one full iteration is 8.2082 minutes\n",
      "encoder learning rate: 1.87e-04, decoder learning rate: 1.87e-04\n",
      "[230/300] At -5.0 dB, Train Loss: 1.6354461908340454 Train BER 0.08828108012676239,                  \n",
      " [230/300] At -3.0 dB, Train Loss: 0.20303186774253845 Train BER 0.008162162266671658\n",
      "Time for one full iteration is 8.2113 minutes\n",
      "encoder learning rate: 1.82e-04, decoder learning rate: 1.82e-04\n",
      "[231/300] At -5.0 dB, Train Loss: 1.556659460067749 Train BER 0.0839405432343483,                  \n",
      " [231/300] At -3.0 dB, Train Loss: 0.21278773248195648 Train BER 0.008821621537208557\n",
      "Time for one full iteration is 8.1949 minutes\n",
      "encoder learning rate: 1.77e-04, decoder learning rate: 1.77e-04\n",
      "[232/300] At -5.0 dB, Train Loss: 1.5461580753326416 Train BER 0.08235134929418564,                  \n",
      " [232/300] At -3.0 dB, Train Loss: 0.2209271639585495 Train BER 0.00916216243058443\n",
      "Time for one full iteration is 8.2129 minutes\n",
      "encoder learning rate: 1.73e-04, decoder learning rate: 1.73e-04\n",
      "[233/300] At -5.0 dB, Train Loss: 1.5424072742462158 Train BER 0.0823405385017395,                  \n",
      " [233/300] At -3.0 dB, Train Loss: 0.21480125188827515 Train BER 0.008708108216524124\n",
      "Time for one full iteration is 8.2058 minutes\n",
      "encoder learning rate: 1.68e-04, decoder learning rate: 1.68e-04\n",
      "[234/300] At -5.0 dB, Train Loss: 1.5635337829589844 Train BER 0.08353513479232788,                  \n",
      " [234/300] At -3.0 dB, Train Loss: 0.1940402388572693 Train BER 0.007281081285327673\n",
      "Time for one full iteration is 8.2111 minutes\n",
      "encoder learning rate: 1.63e-04, decoder learning rate: 1.63e-04\n",
      "[235/300] At -5.0 dB, Train Loss: 1.521925449371338 Train BER 0.08070269972085953,                  \n",
      " [235/300] At -3.0 dB, Train Loss: 0.20380406081676483 Train BER 0.007708108052611351\n",
      "Time for one full iteration is 8.1914 minutes\n",
      "encoder learning rate: 1.59e-04, decoder learning rate: 1.59e-04\n",
      "[236/300] At -5.0 dB, Train Loss: 1.6140549182891846 Train BER 0.08689729869365692,                  \n",
      " [236/300] At -3.0 dB, Train Loss: 0.19043874740600586 Train BER 0.00702162180095911\n",
      "Time for one full iteration is 8.1990 minutes\n",
      "encoder learning rate: 1.54e-04, decoder learning rate: 1.54e-04\n",
      "[237/300] At -5.0 dB, Train Loss: 1.566978931427002 Train BER 0.0820864886045456,                  \n",
      " [237/300] At -3.0 dB, Train Loss: 0.19670523703098297 Train BER 0.007367567624896765\n",
      "Time for one full iteration is 8.2028 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[238/300] At -5.0 dB, Train Loss: 1.5551891326904297 Train BER 0.08338378369808197,                  \n",
      " [238/300] At -3.0 dB, Train Loss: 0.2202959507703781 Train BER 0.008983783423900604\n",
      "Time for one full iteration is 8.1954 minutes\n",
      "encoder learning rate: 1.45e-04, decoder learning rate: 1.45e-04\n",
      "[239/300] At -5.0 dB, Train Loss: 1.5757609605789185 Train BER 0.08488108217716217,                  \n",
      " [239/300] At -3.0 dB, Train Loss: 0.22132574021816254 Train BER 0.008940540254116058\n",
      "Time for one full iteration is 8.2091 minutes\n",
      "encoder learning rate: 1.41e-04, decoder learning rate: 1.41e-04\n",
      "[240/300] At -5.0 dB, Train Loss: 1.5693670511245728 Train BER 0.08388108015060425,                  \n",
      " [240/300] At -3.0 dB, Train Loss: 0.22057974338531494 Train BER 0.009081081487238407\n",
      "Time for one full iteration is 8.1961 minutes\n",
      "encoder learning rate: 1.36e-04, decoder learning rate: 1.36e-04\n",
      "[241/300] At -5.0 dB, Train Loss: 1.5998135805130005 Train BER 0.08578378707170486,                  \n",
      " [241/300] At -3.0 dB, Train Loss: 0.20376236736774445 Train BER 0.008129729889333248\n",
      "Time for one full iteration is 8.1968 minutes\n",
      "encoder learning rate: 1.32e-04, decoder learning rate: 1.32e-04\n",
      "[242/300] At -5.0 dB, Train Loss: 1.5108121633529663 Train BER 0.08003243058919907,                  \n",
      " [242/300] At -3.0 dB, Train Loss: 0.2140835076570511 Train BER 0.008713513612747192\n",
      "Time for one full iteration is 8.2113 minutes\n",
      "encoder learning rate: 1.28e-04, decoder learning rate: 1.28e-04\n",
      "[243/300] At -5.0 dB, Train Loss: 1.5648446083068848 Train BER 0.08254054188728333,                  \n",
      " [243/300] At -3.0 dB, Train Loss: 0.2069953829050064 Train BER 0.008227027021348476\n",
      "Time for one full iteration is 8.2139 minutes\n",
      "encoder learning rate: 1.24e-04, decoder learning rate: 1.24e-04\n",
      "[244/300] At -5.0 dB, Train Loss: 1.584701657295227 Train BER 0.08442161977291107,                  \n",
      " [244/300] At -3.0 dB, Train Loss: 0.1943950206041336 Train BER 0.007540540769696236\n",
      "Time for one full iteration is 8.2052 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[245/300] At -5.0 dB, Train Loss: 1.5840795040130615 Train BER 0.08461621403694153,                  \n",
      " [245/300] At -3.0 dB, Train Loss: 0.19990986585617065 Train BER 0.007729729637503624\n",
      "Time for one full iteration is 8.2120 minutes\n",
      "encoder learning rate: 1.16e-04, decoder learning rate: 1.16e-04\n",
      "[246/300] At -5.0 dB, Train Loss: 1.5394363403320312 Train BER 0.08320540189743042,                  \n",
      " [246/300] At -3.0 dB, Train Loss: 0.2030303031206131 Train BER 0.00792432390153408\n",
      "Time for one full iteration is 8.1987 minutes\n",
      "encoder learning rate: 1.12e-04, decoder learning rate: 1.12e-04\n",
      "[247/300] At -5.0 dB, Train Loss: 1.5277609825134277 Train BER 0.08045405149459839,                  \n",
      " [247/300] At -3.0 dB, Train Loss: 0.20452922582626343 Train BER 0.008540540933609009\n",
      "Time for one full iteration is 8.1936 minutes\n",
      "encoder learning rate: 1.08e-04, decoder learning rate: 1.08e-04\n",
      "[248/300] At -5.0 dB, Train Loss: 1.560957431793213 Train BER 0.0822378396987915,                  \n",
      " [248/300] At -3.0 dB, Train Loss: 0.19987694919109344 Train BER 0.007908107712864876\n",
      "Time for one full iteration is 8.2150 minutes\n",
      "encoder learning rate: 1.04e-04, decoder learning rate: 1.04e-04\n",
      "[249/300] At -5.0 dB, Train Loss: 1.5833518505096436 Train BER 0.08436216413974762,                  \n",
      " [249/300] At -3.0 dB, Train Loss: 0.22344748675823212 Train BER 0.009075676091015339\n",
      "Time for one full iteration is 8.1393 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[250/300] At -5.0 dB, Train Loss: 1.588099718093872 Train BER 0.0843999981880188,                  \n",
      " [250/300] At -3.0 dB, Train Loss: 0.2138734608888626 Train BER 0.008648648858070374\n",
      "Time for one full iteration is 8.1796 minutes\n",
      "encoder learning rate: 9.64e-05, decoder learning rate: 9.64e-05\n",
      "[251/300] At -5.0 dB, Train Loss: 1.588319182395935 Train BER 0.08507026731967926,                  \n",
      " [251/300] At -3.0 dB, Train Loss: 0.2056625932455063 Train BER 0.007859459146857262\n",
      "Time for one full iteration is 8.1259 minutes\n",
      "encoder learning rate: 9.27e-05, decoder learning rate: 9.27e-05\n",
      "[252/300] At -5.0 dB, Train Loss: 1.5504916906356812 Train BER 0.08213513344526291,                  \n",
      " [252/300] At -3.0 dB, Train Loss: 0.19139640033245087 Train BER 0.007318919058889151\n",
      "Time for one full iteration is 8.1780 minutes\n",
      "encoder learning rate: 8.91e-05, decoder learning rate: 8.91e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253/300] At -5.0 dB, Train Loss: 1.492875337600708 Train BER 0.07827027142047882,                  \n",
      " [253/300] At -3.0 dB, Train Loss: 0.20801879465579987 Train BER 0.008140540681779385\n",
      "Time for one full iteration is 8.1570 minutes\n",
      "encoder learning rate: 8.56e-05, decoder learning rate: 8.56e-05\n",
      "[254/300] At -5.0 dB, Train Loss: 1.5821644067764282 Train BER 0.08362703025341034,                  \n",
      " [254/300] At -3.0 dB, Train Loss: 0.21260982751846313 Train BER 0.008562162518501282\n",
      "Time for one full iteration is 8.1750 minutes\n",
      "encoder learning rate: 8.22e-05, decoder learning rate: 8.22e-05\n",
      "[255/300] At -5.0 dB, Train Loss: 1.5340863466262817 Train BER 0.08155135065317154,                  \n",
      " [255/300] At -3.0 dB, Train Loss: 0.20965075492858887 Train BER 0.008583784103393555\n",
      "Time for one full iteration is 8.1612 minutes\n",
      "encoder learning rate: 7.88e-05, decoder learning rate: 7.88e-05\n",
      "[256/300] At -5.0 dB, Train Loss: 1.5730687379837036 Train BER 0.08544324338436127,                  \n",
      " [256/300] At -3.0 dB, Train Loss: 0.20616865158081055 Train BER 0.008129729889333248\n",
      "Time for one full iteration is 8.1615 minutes\n",
      "encoder learning rate: 7.54e-05, decoder learning rate: 7.54e-05\n",
      "[257/300] At -5.0 dB, Train Loss: 1.5636570453643799 Train BER 0.08294054120779037,                  \n",
      " [257/300] At -3.0 dB, Train Loss: 0.19808152318000793 Train BER 0.00758918933570385\n",
      "Time for one full iteration is 8.1387 minutes\n",
      "encoder learning rate: 7.22e-05, decoder learning rate: 7.22e-05\n",
      "[258/300] At -5.0 dB, Train Loss: 1.590205430984497 Train BER 0.085897296667099,                  \n",
      " [258/300] At -3.0 dB, Train Loss: 0.2061624377965927 Train BER 0.00836756732314825\n",
      "Time for one full iteration is 8.1447 minutes\n",
      "encoder learning rate: 6.90e-05, decoder learning rate: 6.90e-05\n",
      "[259/300] At -5.0 dB, Train Loss: 1.5221331119537354 Train BER 0.08074594289064407,                  \n",
      " [259/300] At -3.0 dB, Train Loss: 0.19193926453590393 Train BER 0.007637837901711464\n",
      "Time for one full iteration is 8.1330 minutes\n",
      "encoder learning rate: 6.58e-05, decoder learning rate: 6.58e-05\n",
      "[260/300] At -5.0 dB, Train Loss: 1.5623819828033447 Train BER 0.08283243328332901,                  \n",
      " [260/300] At -3.0 dB, Train Loss: 0.20037133991718292 Train BER 0.007789188995957375\n",
      "Time for one full iteration is 8.1242 minutes\n",
      "encoder learning rate: 6.28e-05, decoder learning rate: 6.28e-05\n",
      "[261/300] At -5.0 dB, Train Loss: 1.584235429763794 Train BER 0.0856756791472435,                  \n",
      " [261/300] At -3.0 dB, Train Loss: 0.20618855953216553 Train BER 0.00836756732314825\n",
      "Time for one full iteration is 8.1405 minutes\n",
      "encoder learning rate: 5.98e-05, decoder learning rate: 5.98e-05\n",
      "[262/300] At -5.0 dB, Train Loss: 1.561508059501648 Train BER 0.08331350982189178,                  \n",
      " [262/300] At -3.0 dB, Train Loss: 0.1949758678674698 Train BER 0.007378378417342901\n",
      "Time for one full iteration is 8.1152 minutes\n",
      "encoder learning rate: 5.69e-05, decoder learning rate: 5.69e-05\n",
      "[263/300] At -5.0 dB, Train Loss: 1.6247190237045288 Train BER 0.08785945922136307,                  \n",
      " [263/300] At -3.0 dB, Train Loss: 0.19524681568145752 Train BER 0.007745945826172829\n",
      "Time for one full iteration is 8.1347 minutes\n",
      "encoder learning rate: 5.40e-05, decoder learning rate: 5.40e-05\n",
      "[264/300] At -5.0 dB, Train Loss: 1.520204782485962 Train BER 0.08138918876647949,                  \n",
      " [264/300] At -3.0 dB, Train Loss: 0.21307042241096497 Train BER 0.008589189499616623\n",
      "Time for one full iteration is 8.1407 minutes\n",
      "encoder learning rate: 5.12e-05, decoder learning rate: 5.12e-05\n",
      "[265/300] At -5.0 dB, Train Loss: 1.5623245239257812 Train BER 0.08354594558477402,                  \n",
      " [265/300] At -3.0 dB, Train Loss: 0.2076408714056015 Train BER 0.008421621285378933\n",
      "Time for one full iteration is 8.1412 minutes\n",
      "encoder learning rate: 4.85e-05, decoder learning rate: 4.85e-05\n",
      "[266/300] At -5.0 dB, Train Loss: 1.5816160440444946 Train BER 0.08411891758441925,                  \n",
      " [266/300] At -3.0 dB, Train Loss: 0.1947082281112671 Train BER 0.007632432505488396\n",
      "Time for one full iteration is 8.0553 minutes\n",
      "encoder learning rate: 4.59e-05, decoder learning rate: 4.59e-05\n",
      "[267/300] At -5.0 dB, Train Loss: 1.5611985921859741 Train BER 0.08429189026355743,                  \n",
      " [267/300] At -3.0 dB, Train Loss: 0.20350627601146698 Train BER 0.007886486127972603\n",
      "Time for one full iteration is 8.1263 minutes\n",
      "encoder learning rate: 4.33e-05, decoder learning rate: 4.33e-05\n",
      "[268/300] At -5.0 dB, Train Loss: 1.5712088346481323 Train BER 0.08443783968687057,                  \n",
      " [268/300] At -3.0 dB, Train Loss: 0.19836550951004028 Train BER 0.0076972972601652145\n",
      "Time for one full iteration is 8.1280 minutes\n",
      "encoder learning rate: 4.08e-05, decoder learning rate: 4.08e-05\n",
      "[269/300] At -5.0 dB, Train Loss: 1.5718615055084229 Train BER 0.08470270037651062,                  \n",
      " [269/300] At -3.0 dB, Train Loss: 0.20093339681625366 Train BER 0.008054054342210293\n",
      "Time for one full iteration is 8.0726 minutes\n",
      "encoder learning rate: 3.84e-05, decoder learning rate: 3.84e-05\n",
      "[270/300] At -5.0 dB, Train Loss: 1.5720678567886353 Train BER 0.08432973176240921,                  \n",
      " [270/300] At -3.0 dB, Train Loss: 0.2023596316576004 Train BER 0.007935134693980217\n",
      "Time for one full iteration is 8.0995 minutes\n",
      "encoder learning rate: 3.61e-05, decoder learning rate: 3.61e-05\n",
      "[271/300] At -5.0 dB, Train Loss: 1.5650957822799683 Train BER 0.08334594964981079,                  \n",
      " [271/300] At -3.0 dB, Train Loss: 0.2164253443479538 Train BER 0.008854053914546967\n",
      "Time for one full iteration is 8.0823 minutes\n",
      "encoder learning rate: 3.38e-05, decoder learning rate: 3.38e-05\n",
      "[272/300] At -5.0 dB, Train Loss: 1.6020852327346802 Train BER 0.0848918929696083,                  \n",
      " [272/300] At -3.0 dB, Train Loss: 0.2096588909626007 Train BER 0.008616216480731964\n",
      "Time for one full iteration is 8.1194 minutes\n",
      "encoder learning rate: 3.16e-05, decoder learning rate: 3.16e-05\n",
      "[273/300] At -5.0 dB, Train Loss: 1.561611533164978 Train BER 0.0829189196228981,                  \n",
      " [273/300] At -3.0 dB, Train Loss: 0.20708397030830383 Train BER 0.007989189587533474\n",
      "Time for one full iteration is 8.1283 minutes\n",
      "encoder learning rate: 2.95e-05, decoder learning rate: 2.95e-05\n",
      "[274/300] At -5.0 dB, Train Loss: 1.4869712591171265 Train BER 0.07964324206113815,                  \n",
      " [274/300] At -3.0 dB, Train Loss: 0.19994211196899414 Train BER 0.008259459398686886\n",
      "Time for one full iteration is 8.1633 minutes\n",
      "encoder learning rate: 2.74e-05, decoder learning rate: 2.74e-05\n",
      "[275/300] At -5.0 dB, Train Loss: 1.5725247859954834 Train BER 0.08275134861469269,                  \n",
      " [275/300] At -3.0 dB, Train Loss: 0.21490558981895447 Train BER 0.00856756791472435\n",
      "Time for one full iteration is 8.1606 minutes\n",
      "encoder learning rate: 2.54e-05, decoder learning rate: 2.54e-05\n",
      "[276/300] At -5.0 dB, Train Loss: 1.594140887260437 Train BER 0.08674594759941101,                  \n",
      " [276/300] At -3.0 dB, Train Loss: 0.2108868956565857 Train BER 0.008416215889155865\n",
      "Time for one full iteration is 8.1264 minutes\n",
      "encoder learning rate: 2.35e-05, decoder learning rate: 2.35e-05\n",
      "[277/300] At -5.0 dB, Train Loss: 1.5946714878082275 Train BER 0.08430811017751694,                  \n",
      " [277/300] At -3.0 dB, Train Loss: 0.19651992619037628 Train BER 0.007729729637503624\n",
      "Time for one full iteration is 8.2074 minutes\n",
      "encoder learning rate: 2.17e-05, decoder learning rate: 2.17e-05\n",
      "[278/300] At -5.0 dB, Train Loss: 1.601970911026001 Train BER 0.08575675636529922,                  \n",
      " [278/300] At -3.0 dB, Train Loss: 0.2210976630449295 Train BER 0.009270270355045795\n",
      "Time for one full iteration is 8.1021 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[279/300] At -5.0 dB, Train Loss: 1.5988683700561523 Train BER 0.08542702347040176,                  \n",
      " [279/300] At -3.0 dB, Train Loss: 0.21176619827747345 Train BER 0.0085081085562706\n",
      "Time for one full iteration is 8.0865 minutes\n",
      "encoder learning rate: 1.83e-05, decoder learning rate: 1.83e-05\n",
      "[280/300] At -5.0 dB, Train Loss: 1.5755881071090698 Train BER 0.08461081236600876,                  \n",
      " [280/300] At -3.0 dB, Train Loss: 0.1997249722480774 Train BER 0.0078108105808496475\n",
      "Time for one full iteration is 8.4238 minutes\n",
      "encoder learning rate: 1.67e-05, decoder learning rate: 1.67e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281/300] At -5.0 dB, Train Loss: 1.5690550804138184 Train BER 0.08343783766031265,                  \n",
      " [281/300] At -3.0 dB, Train Loss: 0.21081720292568207 Train BER 0.008335134945809841\n",
      "Time for one full iteration is 8.3039 minutes\n",
      "encoder learning rate: 1.52e-05, decoder learning rate: 1.52e-05\n",
      "[282/300] At -5.0 dB, Train Loss: 1.5419589281082153 Train BER 0.08195675909519196,                  \n",
      " [282/300] At -3.0 dB, Train Loss: 0.20541416108608246 Train BER 0.008140540681779385\n",
      "Time for one full iteration is 8.3625 minutes\n",
      "encoder learning rate: 1.37e-05, decoder learning rate: 1.37e-05\n",
      "[283/300] At -5.0 dB, Train Loss: 1.536821722984314 Train BER 0.08231351524591446,                  \n",
      " [283/300] At -3.0 dB, Train Loss: 0.20766684412956238 Train BER 0.00818378385156393\n",
      "Time for one full iteration is 8.5664 minutes\n",
      "encoder learning rate: 1.24e-05, decoder learning rate: 1.24e-05\n",
      "[284/300] At -5.0 dB, Train Loss: 1.5207501649856567 Train BER 0.07935135066509247,                  \n",
      " [284/300] At -3.0 dB, Train Loss: 0.19480940699577332 Train BER 0.00758918933570385\n",
      "Time for one full iteration is 8.2931 minutes\n",
      "encoder learning rate: 1.11e-05, decoder learning rate: 1.11e-05\n",
      "[285/300] At -5.0 dB, Train Loss: 1.4951856136322021 Train BER 0.07878378033638,                  \n",
      " [285/300] At -3.0 dB, Train Loss: 0.20262423157691956 Train BER 0.008324324153363705\n",
      "Time for one full iteration is 8.2080 minutes\n",
      "encoder learning rate: 9.85e-06, decoder learning rate: 9.85e-06\n",
      "[286/300] At -5.0 dB, Train Loss: 1.5202217102050781 Train BER 0.08016756922006607,                  \n",
      " [286/300] At -3.0 dB, Train Loss: 0.20567676424980164 Train BER 0.008745945990085602\n",
      "Time for one full iteration is 8.3638 minutes\n",
      "encoder learning rate: 8.71e-06, decoder learning rate: 8.71e-06\n",
      "[287/300] At -5.0 dB, Train Loss: 1.5038808584213257 Train BER 0.07927567511796951,                  \n",
      " [287/300] At -3.0 dB, Train Loss: 0.20675455033779144 Train BER 0.008021621964871883\n",
      "Time for one full iteration is 8.1403 minutes\n",
      "encoder learning rate: 7.65e-06, decoder learning rate: 7.65e-06\n",
      "[288/300] At -5.0 dB, Train Loss: 1.566325068473816 Train BER 0.08310270309448242,                  \n",
      " [288/300] At -3.0 dB, Train Loss: 0.20959588885307312 Train BER 0.008529730141162872\n",
      "Time for one full iteration is 8.3748 minutes\n",
      "encoder learning rate: 6.67e-06, decoder learning rate: 6.67e-06\n",
      "[289/300] At -5.0 dB, Train Loss: 1.582295536994934 Train BER 0.08444865047931671,                  \n",
      " [289/300] At -3.0 dB, Train Loss: 0.21662846207618713 Train BER 0.00865945965051651\n",
      "Time for one full iteration is 8.2468 minutes\n",
      "encoder learning rate: 5.76e-06, decoder learning rate: 5.76e-06\n",
      "[290/300] At -5.0 dB, Train Loss: 1.5249704122543335 Train BER 0.0815243273973465,                  \n",
      " [290/300] At -3.0 dB, Train Loss: 0.19990944862365723 Train BER 0.008021621964871883\n",
      "Time for one full iteration is 8.3183 minutes\n",
      "encoder learning rate: 4.94e-06, decoder learning rate: 4.94e-06\n",
      "[291/300] At -5.0 dB, Train Loss: 1.5475822687149048 Train BER 0.08273513615131378,                  \n",
      " [291/300] At -3.0 dB, Train Loss: 0.20290842652320862 Train BER 0.008070270530879498\n",
      "Time for one full iteration is 8.3375 minutes\n",
      "encoder learning rate: 4.19e-06, decoder learning rate: 4.19e-06\n",
      "[292/300] At -5.0 dB, Train Loss: 1.5430790185928345 Train BER 0.08274053782224655,                  \n",
      " [292/300] At -3.0 dB, Train Loss: 0.19668573141098022 Train BER 0.007735135033726692\n",
      "Time for one full iteration is 8.2890 minutes\n",
      "encoder learning rate: 3.52e-06, decoder learning rate: 3.52e-06\n",
      "[293/300] At -5.0 dB, Train Loss: 1.5919448137283325 Train BER 0.08527027070522308,                  \n",
      " [293/300] At -3.0 dB, Train Loss: 0.19401361048221588 Train BER 0.007405405398458242\n",
      "Time for one full iteration is 8.4375 minutes\n",
      "encoder learning rate: 2.93e-06, decoder learning rate: 2.93e-06\n",
      "[294/300] At -5.0 dB, Train Loss: 1.5767847299575806 Train BER 0.08415134996175766,                  \n",
      " [294/300] At -3.0 dB, Train Loss: 0.19208689033985138 Train BER 0.007524324115365744\n",
      "Time for one full iteration is 8.2962 minutes\n",
      "encoder learning rate: 2.42e-06, decoder learning rate: 2.42e-06\n",
      "[295/300] At -5.0 dB, Train Loss: 1.5676333904266357 Train BER 0.08359459787607193,                  \n",
      " [295/300] At -3.0 dB, Train Loss: 0.19873106479644775 Train BER 0.007654054090380669\n",
      "Time for one full iteration is 8.3191 minutes\n",
      "encoder learning rate: 1.99e-06, decoder learning rate: 1.99e-06\n",
      "[296/300] At -5.0 dB, Train Loss: 1.5705386400222778 Train BER 0.0846216231584549,                  \n",
      " [296/300] At -3.0 dB, Train Loss: 0.2051936239004135 Train BER 0.008172973059117794\n",
      "Time for one full iteration is 8.4578 minutes\n",
      "encoder learning rate: 1.63e-06, decoder learning rate: 1.63e-06\n",
      "[297/300] At -5.0 dB, Train Loss: 1.5629390478134155 Train BER 0.082697294652462,                  \n",
      " [297/300] At -3.0 dB, Train Loss: 0.2044191211462021 Train BER 0.008372972719371319\n",
      "Time for one full iteration is 8.4044 minutes\n",
      "encoder learning rate: 1.35e-06, decoder learning rate: 1.35e-06\n",
      "[298/300] At -5.0 dB, Train Loss: 1.5707095861434937 Train BER 0.08374053984880447,                  \n",
      " [298/300] At -3.0 dB, Train Loss: 0.2079414576292038 Train BER 0.008043243549764156\n",
      "Time for one full iteration is 8.5051 minutes\n",
      "encoder learning rate: 1.16e-06, decoder learning rate: 1.16e-06\n",
      "[299/300] At -5.0 dB, Train Loss: 1.5551424026489258 Train BER 0.08310810476541519,                  \n",
      " [299/300] At -3.0 dB, Train Loss: 0.20761121809482574 Train BER 0.008681081235408783\n",
      "Time for one full iteration is 8.6555 minutes\n",
      "encoder learning rate: 1.04e-06, decoder learning rate: 1.04e-06\n",
      "[300/300] At -5.0 dB, Train Loss: 1.5645966529846191 Train BER 0.08311351388692856,                  \n",
      " [300/300] At -3.0 dB, Train Loss: 0.19627420604228973 Train BER 0.007783783599734306\n",
      "Time for one full iteration is 8.6554 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO__-3.0_Encoder_KO_-5.0_Decoder/epochs_300_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/shubham19/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/scipy/stats/_morestats.py:1882: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeppolar Shapiro test W = 0.9635136127471924, p-value = 0.0\n",
      "Gaussian Shapiro test W = 1.000008225440979, p-value = 1.0\n",
      "Polar Shapiro test W = 0.6373023986816406, p-value = 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166806, BLER: 0.435685\n",
      "Deep - BER: 0.083516, BLER: 0.507560\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072560, BLER: 0.196729\n",
      "Deep - BER: 0.030288, BLER: 0.278691\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.020039, BLER: 0.055872\n",
      "Deep - BER: 0.008077, BLER: 0.127205\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003040, BLER: 0.008584\n",
      "Deep - BER: 0.002099, BLER: 0.055634\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000196, BLER: 0.000562\n",
      "Deep - BER: 0.000707, BLER: 0.024135\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.08351605397462845, 0.03028829728625715, 0.008077135134954005, 0.0020987297296524046, 0.0007067567595222499]\n",
      "BERs of SC decoding: [0.16680562129616738, 0.07256035128980876, 0.020038837843574582, 0.0030400540521077347, 0.00019605405432594126]\n",
      "BLERs of DeepPolar: [0.5075599999999999, 0.2786909999999996, 0.1272050000000002, 0.055633999999999774, 0.02413499999999986]\n",
      "BLERs of SC decoding: [0.43568500000000004, 0.1967290000000001, 0.05587199999999996, 0.008583999999999955, 0.0005620000000000004]\n",
      "time = 428.2622946580251 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
