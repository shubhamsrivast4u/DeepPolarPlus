{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = True  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 300  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = 1.  # SNR at which encoder is trained\n",
    "dec_train_snr = -1.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.0005  # Decoder Learning rate\n",
    "enc_lr = 0.0005  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}__{enc_train_snr}_Encoder_{decoder_type}_{dec_train_snr}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/300] At -1.0 dB, Train Loss: 8.135403633117676 Train BER 0.47344323992729187,                  \n",
      " [1/300] At 1.0 dB, Train Loss: 7.992502689361572 Train BER 0.4734594523906708\n",
      "Time for one full iteration is 7.3281 minutes\n",
      "encoder learning rate: 1.00e-05, decoder learning rate: 1.00e-05\n",
      "[2/300] At -1.0 dB, Train Loss: 6.774824142456055 Train BER 0.46789729595184326,                  \n",
      " [2/300] At 1.0 dB, Train Loss: 6.6975860595703125 Train BER 0.4638864994049072\n",
      "Time for one full iteration is 7.3950 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[3/300] At -1.0 dB, Train Loss: 6.218291282653809 Train BER 0.4571243226528168,                  \n",
      " [3/300] At 1.0 dB, Train Loss: 6.173620223999023 Train BER 0.45534053444862366\n",
      "Time for one full iteration is 7.2499 minutes\n",
      "encoder learning rate: 3.00e-05, decoder learning rate: 3.00e-05\n",
      "[4/300] At -1.0 dB, Train Loss: 5.954464912414551 Train BER 0.44087567925453186,                  \n",
      " [4/300] At 1.0 dB, Train Loss: 5.908240795135498 Train BER 0.42784324288368225\n",
      "Time for one full iteration is 7.2989 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[5/300] At -1.0 dB, Train Loss: 4.965211868286133 Train BER 0.3295027017593384,                  \n",
      " [5/300] At 1.0 dB, Train Loss: 4.382772445678711 Train BER 0.28272971510887146\n",
      "Time for one full iteration is 7.4275 minutes\n",
      "encoder learning rate: 5.00e-05, decoder learning rate: 5.00e-05\n",
      "[6/300] At -1.0 dB, Train Loss: 2.547308921813965 Train BER 0.1435459405183792,                  \n",
      " [6/300] At 1.0 dB, Train Loss: 1.689671277999878 Train BER 0.09041081368923187\n",
      "Time for one full iteration is 7.5589 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[7/300] At -1.0 dB, Train Loss: 1.0595424175262451 Train BER 0.05250810831785202,                  \n",
      " [7/300] At 1.0 dB, Train Loss: 0.3911849558353424 Train BER 0.019729729741811752\n",
      "Time for one full iteration is 7.6384 minutes\n",
      "encoder learning rate: 7.00e-05, decoder learning rate: 7.00e-05\n",
      "[8/300] At -1.0 dB, Train Loss: 0.6691800951957703 Train BER 0.033151350915431976,                  \n",
      " [8/300] At 1.0 dB, Train Loss: 0.26751673221588135 Train BER 0.014994594268500805\n",
      "Time for one full iteration is 7.6832 minutes\n",
      "encoder learning rate: 8.00e-05, decoder learning rate: 8.00e-05\n",
      "[9/300] At -1.0 dB, Train Loss: 0.50986647605896 Train BER 0.025713512673974037,                  \n",
      " [9/300] At 1.0 dB, Train Loss: 0.21537849307060242 Train BER 0.01205405406653881\n",
      "Time for one full iteration is 7.5496 minutes\n",
      "encoder learning rate: 9.00e-05, decoder learning rate: 9.00e-05\n",
      "[10/300] At -1.0 dB, Train Loss: 0.345255970954895 Train BER 0.014032432809472084,                  \n",
      " [10/300] At 1.0 dB, Train Loss: 0.09090205281972885 Train BER 0.002486486453562975\n",
      "Time for one full iteration is 7.2917 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[11/300] At -1.0 dB, Train Loss: 0.24271796643733978 Train BER 0.009356756694614887,                  \n",
      " [11/300] At 1.0 dB, Train Loss: 0.039812102913856506 Train BER 0.0008810810977593064\n",
      "Time for one full iteration is 7.2744 minutes\n",
      "encoder learning rate: 1.10e-04, decoder learning rate: 1.10e-04\n",
      "[12/300] At -1.0 dB, Train Loss: 0.16337576508522034 Train BER 0.006178378593176603,                  \n",
      " [12/300] At 1.0 dB, Train Loss: 0.026581278070807457 Train BER 0.000605405424721539\n",
      "Time for one full iteration is 7.1459 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[13/300] At -1.0 dB, Train Loss: 0.12938740849494934 Train BER 0.004908108152449131,                  \n",
      " [13/300] At 1.0 dB, Train Loss: 0.01415733341127634 Train BER 0.00033513514790683985\n",
      "Time for one full iteration is 7.1839 minutes\n",
      "encoder learning rate: 1.30e-04, decoder learning rate: 1.30e-04\n",
      "[14/300] At -1.0 dB, Train Loss: 0.09168367832899094 Train BER 0.0034000000450760126,                  \n",
      " [14/300] At 1.0 dB, Train Loss: 0.010981948114931583 Train BER 0.00028648649458773434\n",
      "Time for one full iteration is 7.1972 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[15/300] At -1.0 dB, Train Loss: 0.08171319216489792 Train BER 0.002832432510331273,                  \n",
      " [15/300] At 1.0 dB, Train Loss: 0.006990507710725069 Train BER 0.00014054053463041782\n",
      "Time for one full iteration is 7.1949 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[16/300] At -1.0 dB, Train Loss: 0.05591784790158272 Train BER 0.002097297227010131,                  \n",
      " [16/300] At 1.0 dB, Train Loss: 0.004062606953084469 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 7.0421 minutes\n",
      "encoder learning rate: 1.60e-04, decoder learning rate: 1.60e-04\n",
      "[17/300] At -1.0 dB, Train Loss: 0.05116187408566475 Train BER 0.001908108126372099,                  \n",
      " [17/300] At 1.0 dB, Train Loss: 0.003345174714922905 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 7.0703 minutes\n",
      "encoder learning rate: 1.70e-04, decoder learning rate: 1.70e-04\n",
      "[18/300] At -1.0 dB, Train Loss: 0.04772558808326721 Train BER 0.001886486541479826,                  \n",
      " [18/300] At 1.0 dB, Train Loss: 0.0026262053288519382 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 7.0939 minutes\n",
      "encoder learning rate: 1.80e-04, decoder learning rate: 1.80e-04\n",
      "[19/300] At -1.0 dB, Train Loss: 0.04198875278234482 Train BER 0.0015405404847115278,                  \n",
      " [19/300] At 1.0 dB, Train Loss: 0.0023195287212729454 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 7.2217 minutes\n",
      "encoder learning rate: 1.90e-04, decoder learning rate: 1.90e-04\n",
      "[20/300] At -1.0 dB, Train Loss: 0.0335799977183342 Train BER 0.0012918919092044234,                  \n",
      " [20/300] At 1.0 dB, Train Loss: 0.0016760870348662138 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 7.2849 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[21/300] At -1.0 dB, Train Loss: 0.03555753454566002 Train BER 0.0012648648116737604,                  \n",
      " [21/300] At 1.0 dB, Train Loss: 0.0022656747605651617 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 7.2654 minutes\n",
      "encoder learning rate: 2.10e-04, decoder learning rate: 2.10e-04\n",
      "[22/300] At -1.0 dB, Train Loss: 0.028648819774389267 Train BER 0.0011297296732664108,                  \n",
      " [22/300] At 1.0 dB, Train Loss: 0.0013203267008066177 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.2962 minutes\n",
      "encoder learning rate: 2.20e-04, decoder learning rate: 2.20e-04\n",
      "[23/300] At -1.0 dB, Train Loss: 0.0302877314388752 Train BER 0.0011027026921510696,                  \n",
      " [23/300] At 1.0 dB, Train Loss: 0.0009703783434815705 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3519 minutes\n",
      "encoder learning rate: 2.30e-04, decoder learning rate: 2.30e-04\n",
      "[24/300] At -1.0 dB, Train Loss: 0.02543216198682785 Train BER 0.0009351351181976497,                  \n",
      " [24/300] At 1.0 dB, Train Loss: 0.0008111894712783396 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.5002 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[25/300] At -1.0 dB, Train Loss: 0.01839187927544117 Train BER 0.0006864864844828844,                  \n",
      " [25/300] At 1.0 dB, Train Loss: 0.00045762877562083304 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.7036 minutes\n",
      "encoder learning rate: 2.50e-04, decoder learning rate: 2.50e-04\n",
      "[26/300] At -1.0 dB, Train Loss: 0.014912890270352364 Train BER 0.00047567568253725767,                  \n",
      " [26/300] At 1.0 dB, Train Loss: 0.00039319705683737993 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3851 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[27/300] At -1.0 dB, Train Loss: 0.01712358556687832 Train BER 0.0006000000284984708,                  \n",
      " [27/300] At 1.0 dB, Train Loss: 0.00020570517517626286 Train BER 0.0\n",
      "Time for one full iteration is 7.4214 minutes\n",
      "encoder learning rate: 2.70e-04, decoder learning rate: 2.70e-04\n",
      "[28/300] At -1.0 dB, Train Loss: 0.01731141097843647 Train BER 0.0006378378602676094,                  \n",
      " [28/300] At 1.0 dB, Train Loss: 0.00012469520152080804 Train BER 0.0\n",
      "Time for one full iteration is 7.4064 minutes\n",
      "encoder learning rate: 2.80e-04, decoder learning rate: 2.80e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/300] At -1.0 dB, Train Loss: 0.016414975747466087 Train BER 0.0005837837816216052,                  \n",
      " [29/300] At 1.0 dB, Train Loss: 0.00030402609263546765 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.4541 minutes\n",
      "encoder learning rate: 2.90e-04, decoder learning rate: 2.90e-04\n",
      "[30/300] At -1.0 dB, Train Loss: 0.014316469430923462 Train BER 0.0006108108209446073,                  \n",
      " [30/300] At 1.0 dB, Train Loss: 0.00028093543369323015 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3319 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[31/300] At -1.0 dB, Train Loss: 0.008330078795552254 Train BER 0.0002648648514878005,                  \n",
      " [31/300] At 1.0 dB, Train Loss: 9.210874850396067e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3063 minutes\n",
      "encoder learning rate: 3.10e-04, decoder learning rate: 3.10e-04\n",
      "[32/300] At -1.0 dB, Train Loss: 0.02398410439491272 Train BER 0.0009837837424129248,                  \n",
      " [32/300] At 1.0 dB, Train Loss: 0.0002287236275151372 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4161 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[33/300] At -1.0 dB, Train Loss: 0.015292086638510227 Train BER 0.0006216216133907437,                  \n",
      " [33/300] At 1.0 dB, Train Loss: 0.0001077116176020354 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3572 minutes\n",
      "encoder learning rate: 3.30e-04, decoder learning rate: 3.30e-04\n",
      "[34/300] At -1.0 dB, Train Loss: 0.013080825097858906 Train BER 0.0004972973256371915,                  \n",
      " [34/300] At 1.0 dB, Train Loss: 5.4200041631702334e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.4508 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[35/300] At -1.0 dB, Train Loss: 0.010557613335549831 Train BER 0.00041621620766818523,                  \n",
      " [35/300] At 1.0 dB, Train Loss: 8.647359936730936e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3319 minutes\n",
      "encoder learning rate: 3.50e-04, decoder learning rate: 3.50e-04\n",
      "[36/300] At -1.0 dB, Train Loss: 0.013185174204409122 Train BER 0.0005351351574063301,                  \n",
      " [36/300] At 1.0 dB, Train Loss: 0.00011284564970992506 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3110 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[37/300] At -1.0 dB, Train Loss: 0.008525166660547256 Train BER 0.0003189189301338047,                  \n",
      " [37/300] At 1.0 dB, Train Loss: 5.222848631092347e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3119 minutes\n",
      "encoder learning rate: 3.70e-04, decoder learning rate: 3.70e-04\n",
      "[38/300] At -1.0 dB, Train Loss: 0.012333537451922894 Train BER 0.0005945945740677416,                  \n",
      " [38/300] At 1.0 dB, Train Loss: 4.715843897429295e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5766 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[39/300] At -1.0 dB, Train Loss: 0.00978481862694025 Train BER 0.00040540541522204876,                  \n",
      " [39/300] At 1.0 dB, Train Loss: 0.0001283739402424544 Train BER 0.0\n",
      "Time for one full iteration is 7.5144 minutes\n",
      "encoder learning rate: 3.90e-04, decoder learning rate: 3.90e-04\n",
      "[40/300] At -1.0 dB, Train Loss: 0.010000521317124367 Train BER 0.00035675676190294325,                  \n",
      " [40/300] At 1.0 dB, Train Loss: 5.7954679505201057e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5079 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[41/300] At -1.0 dB, Train Loss: 0.006396161392331123 Train BER 0.0001945945987245068,                  \n",
      " [41/300] At 1.0 dB, Train Loss: 8.356861508218572e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5685 minutes\n",
      "encoder learning rate: 4.10e-04, decoder learning rate: 4.10e-04\n",
      "[42/300] At -1.0 dB, Train Loss: 0.011083528399467468 Train BER 0.0005675675929524004,                  \n",
      " [42/300] At 1.0 dB, Train Loss: 3.480952000245452e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5929 minutes\n",
      "encoder learning rate: 4.20e-04, decoder learning rate: 4.20e-04\n",
      "[43/300] At -1.0 dB, Train Loss: 0.010027655400335789 Train BER 0.000470270257210359,                  \n",
      " [43/300] At 1.0 dB, Train Loss: 5.926933590671979e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.7273 minutes\n",
      "encoder learning rate: 4.30e-04, decoder learning rate: 4.30e-04\n",
      "[44/300] At -1.0 dB, Train Loss: 0.006069080904126167 Train BER 0.0001945945987245068,                  \n",
      " [44/300] At 1.0 dB, Train Loss: 0.0002806429984048009 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.6603 minutes\n",
      "encoder learning rate: 4.40e-04, decoder learning rate: 4.40e-04\n",
      "[45/300] At -1.0 dB, Train Loss: 0.006313007324934006 Train BER 0.00024864866281859577,                  \n",
      " [45/300] At 1.0 dB, Train Loss: 8.794633322395384e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4422 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[46/300] At -1.0 dB, Train Loss: 0.01071064081043005 Train BER 0.000470270257210359,                  \n",
      " [46/300] At 1.0 dB, Train Loss: 1.5755955246277153e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.6335 minutes\n",
      "encoder learning rate: 4.60e-04, decoder learning rate: 4.60e-04\n",
      "[47/300] At -1.0 dB, Train Loss: 0.006984776817262173 Train BER 0.0002702702768146992,                  \n",
      " [47/300] At 1.0 dB, Train Loss: 1.2623336260730866e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.4754 minutes\n",
      "encoder learning rate: 4.70e-04, decoder learning rate: 4.70e-04\n",
      "[48/300] At -1.0 dB, Train Loss: 0.010727496817708015 Train BER 0.0004324324254412204,                  \n",
      " [48/300] At 1.0 dB, Train Loss: 8.572628576075658e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.5189 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[49/300] At -1.0 dB, Train Loss: 0.007922075688838959 Train BER 0.000351351365679875,                  \n",
      " [49/300] At 1.0 dB, Train Loss: 4.945475302520208e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3233 minutes\n",
      "encoder learning rate: 4.90e-04, decoder learning rate: 4.90e-04\n",
      "[50/300] At -1.0 dB, Train Loss: 0.01684478111565113 Train BER 0.0007783783948980272,                  \n",
      " [50/300] At 1.0 dB, Train Loss: 6.185903475852683e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2466 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[51/300] At -1.0 dB, Train Loss: 0.007421251852065325 Train BER 0.00028648649458773434,                  \n",
      " [51/300] At 1.0 dB, Train Loss: 9.022156154969707e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3571 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[52/300] At -1.0 dB, Train Loss: 0.007659160532057285 Train BER 0.000351351365679875,                  \n",
      " [52/300] At 1.0 dB, Train Loss: 2.2814863768871874e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3565 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[53/300] At -1.0 dB, Train Loss: 0.009513506665825844 Train BER 0.0003459459403529763,                  \n",
      " [53/300] At 1.0 dB, Train Loss: 9.851402865024284e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4317 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[54/300] At -1.0 dB, Train Loss: 0.008625882677733898 Train BER 0.00030810810858383775,                  \n",
      " [54/300] At 1.0 dB, Train Loss: 0.00010986604320351034 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4325 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[55/300] At -1.0 dB, Train Loss: 0.007348840590566397 Train BER 0.0002756756730377674,                  \n",
      " [55/300] At 1.0 dB, Train Loss: 6.833242514403537e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2706 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[56/300] At -1.0 dB, Train Loss: 0.008225055411458015 Train BER 0.0003243243263568729,                  \n",
      " [56/300] At 1.0 dB, Train Loss: 8.182267629308626e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3055 minutes\n",
      "encoder learning rate: 4.99e-04, decoder learning rate: 4.99e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/300] At -1.0 dB, Train Loss: 0.010751974768936634 Train BER 0.00046486486098729074,                  \n",
      " [57/300] At 1.0 dB, Train Loss: 6.923769251443446e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3227 minutes\n",
      "encoder learning rate: 4.99e-04, decoder learning rate: 4.99e-04\n",
      "[58/300] At -1.0 dB, Train Loss: 0.007900725118815899 Train BER 0.0002054054057225585,                  \n",
      " [58/300] At 1.0 dB, Train Loss: 6.503741315100342e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5281 minutes\n",
      "encoder learning rate: 4.99e-04, decoder learning rate: 4.99e-04\n",
      "[59/300] At -1.0 dB, Train Loss: 0.007569593843072653 Train BER 0.000313513504806906,                  \n",
      " [59/300] At 1.0 dB, Train Loss: 1.7778726032702252e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0681 minutes\n",
      "encoder learning rate: 4.98e-04, decoder learning rate: 4.98e-04\n",
      "[60/300] At -1.0 dB, Train Loss: 0.009871152229607105 Train BER 0.0003243243263568729,                  \n",
      " [60/300] At 1.0 dB, Train Loss: 9.306522406404838e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0960 minutes\n",
      "encoder learning rate: 4.98e-04, decoder learning rate: 4.98e-04\n",
      "[61/300] At -1.0 dB, Train Loss: 0.00847769994288683 Train BER 0.00028648649458773434,                  \n",
      " [61/300] At 1.0 dB, Train Loss: 5.4597476264461875e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2327 minutes\n",
      "encoder learning rate: 4.98e-04, decoder learning rate: 4.98e-04\n",
      "[62/300] At -1.0 dB, Train Loss: 0.004784224089235067 Train BER 0.00018378377717453986,                  \n",
      " [62/300] At 1.0 dB, Train Loss: 1.778878504410386e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2532 minutes\n",
      "encoder learning rate: 4.97e-04, decoder learning rate: 4.97e-04\n",
      "[63/300] At -1.0 dB, Train Loss: 0.009230929426848888 Train BER 0.0002918918908108026,                  \n",
      " [63/300] At 1.0 dB, Train Loss: 5.036048969486728e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5748 minutes\n",
      "encoder learning rate: 4.97e-04, decoder learning rate: 4.97e-04\n",
      "[64/300] At -1.0 dB, Train Loss: 0.008199695497751236 Train BER 0.0003027027123607695,                  \n",
      " [64/300] At 1.0 dB, Train Loss: 3.582661156542599e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4037 minutes\n",
      "encoder learning rate: 4.96e-04, decoder learning rate: 4.96e-04\n",
      "[65/300] At -1.0 dB, Train Loss: 0.006650576367974281 Train BER 0.00024324323749169707,                  \n",
      " [65/300] At 1.0 dB, Train Loss: 8.854282896209043e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4367 minutes\n",
      "encoder learning rate: 4.96e-04, decoder learning rate: 4.96e-04\n",
      "[66/300] At -1.0 dB, Train Loss: 0.0038429968990385532 Train BER 0.00011891892063431442,                  \n",
      " [66/300] At 1.0 dB, Train Loss: 0.0001574060006532818 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.3868 minutes\n",
      "encoder learning rate: 4.95e-04, decoder learning rate: 4.95e-04\n",
      "[67/300] At -1.0 dB, Train Loss: 0.008511376567184925 Train BER 0.00039459459367208183,                  \n",
      " [67/300] At 1.0 dB, Train Loss: 7.110011938493699e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4987 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[68/300] At -1.0 dB, Train Loss: 0.002902328735217452 Train BER 0.00011891892063431442,                  \n",
      " [68/300] At 1.0 dB, Train Loss: 2.0615614630514756e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4699 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[69/300] At -1.0 dB, Train Loss: 0.011732353828847408 Train BER 0.00048648647498339415,                  \n",
      " [69/300] At 1.0 dB, Train Loss: 2.5079038096009754e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4208 minutes\n",
      "encoder learning rate: 4.93e-04, decoder learning rate: 4.93e-04\n",
      "[70/300] At -1.0 dB, Train Loss: 0.005039507988840342 Train BER 0.00023783784126862884,                  \n",
      " [70/300] At 1.0 dB, Train Loss: 1.2929171134601347e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4296 minutes\n",
      "encoder learning rate: 4.92e-04, decoder learning rate: 4.92e-04\n",
      "[71/300] At -1.0 dB, Train Loss: 0.008297891356050968 Train BER 0.0003837837721221149,                  \n",
      " [71/300] At 1.0 dB, Train Loss: 8.022828114917502e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.2951 minutes\n",
      "encoder learning rate: 4.91e-04, decoder learning rate: 4.91e-04\n",
      "[72/300] At -1.0 dB, Train Loss: 0.005535184871405363 Train BER 0.00022702703427057713,                  \n",
      " [72/300] At 1.0 dB, Train Loss: 1.627575147722382e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3774 minutes\n",
      "encoder learning rate: 4.91e-04, decoder learning rate: 4.91e-04\n",
      "[73/300] At -1.0 dB, Train Loss: 0.008914178237318993 Train BER 0.00047567568253725767,                  \n",
      " [73/300] At 1.0 dB, Train Loss: 2.70223717961926e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.2980 minutes\n",
      "encoder learning rate: 4.90e-04, decoder learning rate: 4.90e-04\n",
      "[74/300] At -1.0 dB, Train Loss: 0.0048412117175757885 Train BER 0.000156756752403453,                  \n",
      " [74/300] At 1.0 dB, Train Loss: 1.8296057533007115e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3396 minutes\n",
      "encoder learning rate: 4.89e-04, decoder learning rate: 4.89e-04\n",
      "[75/300] At -1.0 dB, Train Loss: 0.005526779685169458 Train BER 0.00024324323749169707,                  \n",
      " [75/300] At 1.0 dB, Train Loss: 0.0001896016619866714 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4228 minutes\n",
      "encoder learning rate: 4.88e-04, decoder learning rate: 4.88e-04\n",
      "[76/300] At -1.0 dB, Train Loss: 0.006510193459689617 Train BER 0.0002702702768146992,                  \n",
      " [76/300] At 1.0 dB, Train Loss: 1.3444349860947113e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4487 minutes\n",
      "encoder learning rate: 4.87e-04, decoder learning rate: 4.87e-04\n",
      "[77/300] At -1.0 dB, Train Loss: 0.00418913783505559 Train BER 0.00012972972763236612,                  \n",
      " [77/300] At 1.0 dB, Train Loss: 1.0856311746465508e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3301 minutes\n",
      "encoder learning rate: 4.86e-04, decoder learning rate: 4.86e-04\n",
      "[78/300] At -1.0 dB, Train Loss: 0.008203839883208275 Train BER 0.0003243243263568729,                  \n",
      " [78/300] At 1.0 dB, Train Loss: 2.6068842998938635e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.7070 minutes\n",
      "encoder learning rate: 4.85e-04, decoder learning rate: 4.85e-04\n",
      "[79/300] At -1.0 dB, Train Loss: 0.007239375729113817 Train BER 0.00023783784126862884,                  \n",
      " [79/300] At 1.0 dB, Train Loss: 0.00014764074876438826 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3390 minutes\n",
      "encoder learning rate: 4.84e-04, decoder learning rate: 4.84e-04\n",
      "[80/300] At -1.0 dB, Train Loss: 0.006701150443404913 Train BER 0.00028648649458773434,                  \n",
      " [80/300] At 1.0 dB, Train Loss: 4.83031499243225e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1636 minutes\n",
      "encoder learning rate: 4.82e-04, decoder learning rate: 4.82e-04\n",
      "[81/300] At -1.0 dB, Train Loss: 0.0040365601889789104 Train BER 0.00016216216317843646,                  \n",
      " [81/300] At 1.0 dB, Train Loss: 1.9732777218450792e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1174 minutes\n",
      "encoder learning rate: 4.81e-04, decoder learning rate: 4.81e-04\n",
      "[82/300] At -1.0 dB, Train Loss: 0.005928701721131802 Train BER 0.00024864866281859577,                  \n",
      " [82/300] At 1.0 dB, Train Loss: 7.934740096970927e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1287 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[83/300] At -1.0 dB, Train Loss: 0.003013476263731718 Train BER 9.189188858726993e-05,                  \n",
      " [83/300] At 1.0 dB, Train Loss: 1.0003446732298471e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3124 minutes\n",
      "encoder learning rate: 4.79e-04, decoder learning rate: 4.79e-04\n",
      "[84/300] At -1.0 dB, Train Loss: 0.006360577419400215 Train BER 0.0002756756730377674,                  \n",
      " [84/300] At 1.0 dB, Train Loss: 8.583364251535386e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2447 minutes\n",
      "encoder learning rate: 4.78e-04, decoder learning rate: 4.78e-04\n",
      "[85/300] At -1.0 dB, Train Loss: 0.005766157526522875 Train BER 0.00022162162349559367,                  \n",
      " [85/300] At 1.0 dB, Train Loss: 3.764320717891678e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3116 minutes\n",
      "encoder learning rate: 4.76e-04, decoder learning rate: 4.76e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86/300] At -1.0 dB, Train Loss: 0.005534071940928698 Train BER 0.0001945945987245068,                  \n",
      " [86/300] At 1.0 dB, Train Loss: 2.212482286267914e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1371 minutes\n",
      "encoder learning rate: 4.75e-04, decoder learning rate: 4.75e-04\n",
      "[87/300] At -1.0 dB, Train Loss: 0.0021203490905463696 Train BER 7.027026731520891e-05,                  \n",
      " [87/300] At 1.0 dB, Train Loss: 9.134806168731302e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.0155 minutes\n",
      "encoder learning rate: 4.74e-04, decoder learning rate: 4.74e-04\n",
      "[88/300] At -1.0 dB, Train Loss: 0.005469054449349642 Train BER 0.00023243243049364537,                  \n",
      " [88/300] At 1.0 dB, Train Loss: 8.001355126907583e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0085 minutes\n",
      "encoder learning rate: 4.72e-04, decoder learning rate: 4.72e-04\n",
      "[89/300] At -1.0 dB, Train Loss: 0.009040942415595055 Train BER 0.00037837837589904666,                  \n",
      " [89/300] At 1.0 dB, Train Loss: 9.557152952766046e-05 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.0012 minutes\n",
      "encoder learning rate: 4.71e-04, decoder learning rate: 4.71e-04\n",
      "[90/300] At -1.0 dB, Train Loss: 0.0059067849069833755 Train BER 0.00024864866281859577,                  \n",
      " [90/300] At 1.0 dB, Train Loss: 2.1603878849418834e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0302 minutes\n",
      "encoder learning rate: 4.69e-04, decoder learning rate: 4.69e-04\n",
      "[91/300] At -1.0 dB, Train Loss: 0.004048995207995176 Train BER 0.00018378377717453986,                  \n",
      " [91/300] At 1.0 dB, Train Loss: 3.888365426973905e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0235 minutes\n",
      "encoder learning rate: 4.68e-04, decoder learning rate: 4.68e-04\n",
      "[92/300] At -1.0 dB, Train Loss: 0.0023107589222490788 Train BER 9.72972993622534e-05,                  \n",
      " [92/300] At 1.0 dB, Train Loss: 5.4700412874808535e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0299 minutes\n",
      "encoder learning rate: 4.66e-04, decoder learning rate: 4.66e-04\n",
      "[93/300] At -1.0 dB, Train Loss: 0.0041160667315125465 Train BER 0.00017297297017648816,                  \n",
      " [93/300] At 1.0 dB, Train Loss: 3.017234121216461e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0845 minutes\n",
      "encoder learning rate: 4.64e-04, decoder learning rate: 4.64e-04\n",
      "[94/300] At -1.0 dB, Train Loss: 0.0035204305313527584 Train BER 0.0001351351384073496,                  \n",
      " [94/300] At 1.0 dB, Train Loss: 1.8435513993608765e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0369 minutes\n",
      "encoder learning rate: 4.63e-04, decoder learning rate: 4.63e-04\n",
      "[95/300] At -1.0 dB, Train Loss: 0.005178520921617746 Train BER 0.00018378377717453986,                  \n",
      " [95/300] At 1.0 dB, Train Loss: 4.761298896482913e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0135 minutes\n",
      "encoder learning rate: 4.61e-04, decoder learning rate: 4.61e-04\n",
      "[96/300] At -1.0 dB, Train Loss: 0.005272876936942339 Train BER 0.0002054054057225585,                  \n",
      " [96/300] At 1.0 dB, Train Loss: 0.00034983441582880914 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.0482 minutes\n",
      "encoder learning rate: 4.59e-04, decoder learning rate: 4.59e-04\n",
      "[97/300] At -1.0 dB, Train Loss: 0.006961754057556391 Train BER 0.00030810810858383775,                  \n",
      " [97/300] At 1.0 dB, Train Loss: 6.904368547111517e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0073 minutes\n",
      "encoder learning rate: 4.58e-04, decoder learning rate: 4.58e-04\n",
      "[98/300] At -1.0 dB, Train Loss: 0.004394754767417908 Train BER 0.00016216216317843646,                  \n",
      " [98/300] At 1.0 dB, Train Loss: 0.0002358648634981364 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.0145 minutes\n",
      "encoder learning rate: 4.56e-04, decoder learning rate: 4.56e-04\n",
      "[99/300] At -1.0 dB, Train Loss: 0.00548766553401947 Train BER 0.00018918918794952333,                  \n",
      " [99/300] At 1.0 dB, Train Loss: 0.0001459297927794978 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 6.9893 minutes\n",
      "encoder learning rate: 4.54e-04, decoder learning rate: 4.54e-04\n",
      "[100/300] At -1.0 dB, Train Loss: 0.006358006037771702 Train BER 0.0002054054057225585,                  \n",
      " [100/300] At 1.0 dB, Train Loss: 1.3738724192080554e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0995 minutes\n",
      "encoder learning rate: 4.52e-04, decoder learning rate: 4.52e-04\n",
      "[101/300] At -1.0 dB, Train Loss: 0.0039054143708199263 Train BER 0.000156756752403453,                  \n",
      " [101/300] At 1.0 dB, Train Loss: 4.5245660658110864e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2159 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[102/300] At -1.0 dB, Train Loss: 0.004990061279386282 Train BER 0.0002162162127206102,                  \n",
      " [102/300] At 1.0 dB, Train Loss: 9.0691009972943e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1914 minutes\n",
      "encoder learning rate: 4.49e-04, decoder learning rate: 4.49e-04\n",
      "[103/300] At -1.0 dB, Train Loss: 0.003656222252175212 Train BER 0.00014054053463041782,                  \n",
      " [103/300] At 1.0 dB, Train Loss: 1.1549555892997887e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2379 minutes\n",
      "encoder learning rate: 4.47e-04, decoder learning rate: 4.47e-04\n",
      "[104/300] At -1.0 dB, Train Loss: 0.005167407914996147 Train BER 0.00024864866281859577,                  \n",
      " [104/300] At 1.0 dB, Train Loss: 5.7738884606806096e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.2194 minutes\n",
      "encoder learning rate: 4.45e-04, decoder learning rate: 4.45e-04\n",
      "[105/300] At -1.0 dB, Train Loss: 0.0018517656717449427 Train BER 7.567567809019238e-05,                  \n",
      " [105/300] At 1.0 dB, Train Loss: 1.3977276466903277e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3949 minutes\n",
      "encoder learning rate: 4.43e-04, decoder learning rate: 4.43e-04\n",
      "[106/300] At -1.0 dB, Train Loss: 0.0032151974737644196 Train BER 0.00010270270286127925,                  \n",
      " [106/300] At 1.0 dB, Train Loss: 5.8149791584583e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4538 minutes\n",
      "encoder learning rate: 4.41e-04, decoder learning rate: 4.41e-04\n",
      "[107/300] At -1.0 dB, Train Loss: 0.005226279143244028 Train BER 0.00023783784126862884,                  \n",
      " [107/300] At 1.0 dB, Train Loss: 1.4225998711481225e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5832 minutes\n",
      "encoder learning rate: 4.39e-04, decoder learning rate: 4.39e-04\n",
      "[108/300] At -1.0 dB, Train Loss: 0.0076094516552984715 Train BER 0.0003621621581260115,                  \n",
      " [108/300] At 1.0 dB, Train Loss: 5.1629494919325225e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5304 minutes\n",
      "encoder learning rate: 4.37e-04, decoder learning rate: 4.37e-04\n",
      "[109/300] At -1.0 dB, Train Loss: 0.0034526726230978966 Train BER 0.0001351351384073496,                  \n",
      " [109/300] At 1.0 dB, Train Loss: 2.107106411131099e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5971 minutes\n",
      "encoder learning rate: 4.35e-04, decoder learning rate: 4.35e-04\n",
      "[110/300] At -1.0 dB, Train Loss: 0.003907941747456789 Train BER 0.00011891892063431442,                  \n",
      " [110/300] At 1.0 dB, Train Loss: 1.0315060535504017e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5153 minutes\n",
      "encoder learning rate: 4.32e-04, decoder learning rate: 4.32e-04\n",
      "[111/300] At -1.0 dB, Train Loss: 0.009213623590767384 Train BER 0.000351351365679875,                  \n",
      " [111/300] At 1.0 dB, Train Loss: 2.4964897420431953e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.6589 minutes\n",
      "encoder learning rate: 4.30e-04, decoder learning rate: 4.30e-04\n",
      "[112/300] At -1.0 dB, Train Loss: 0.005422687157988548 Train BER 0.00024864866281859577,                  \n",
      " [112/300] At 1.0 dB, Train Loss: 2.093461989716161e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4703 minutes\n",
      "encoder learning rate: 4.28e-04, decoder learning rate: 4.28e-04\n",
      "[113/300] At -1.0 dB, Train Loss: 0.003398218424990773 Train BER 0.00011891892063431442,                  \n",
      " [113/300] At 1.0 dB, Train Loss: 2.7789996238425374e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4793 minutes\n",
      "encoder learning rate: 4.26e-04, decoder learning rate: 4.26e-04\n",
      "[114/300] At -1.0 dB, Train Loss: 0.006129099987447262 Train BER 0.00033513514790683985,                  \n",
      " [114/300] At 1.0 dB, Train Loss: 1.2154102478234563e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4413 minutes\n",
      "encoder learning rate: 4.24e-04, decoder learning rate: 4.24e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115/300] At -1.0 dB, Train Loss: 0.005248600617051125 Train BER 0.00030810810858383775,                  \n",
      " [115/300] At 1.0 dB, Train Loss: 0.00016371661331504583 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4506 minutes\n",
      "encoder learning rate: 4.21e-04, decoder learning rate: 4.21e-04\n",
      "[116/300] At -1.0 dB, Train Loss: 0.0034798658452928066 Train BER 0.0001351351384073496,                  \n",
      " [116/300] At 1.0 dB, Train Loss: 4.733517471322557e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4564 minutes\n",
      "encoder learning rate: 4.19e-04, decoder learning rate: 4.19e-04\n",
      "[117/300] At -1.0 dB, Train Loss: 0.004200121387839317 Train BER 0.00016756757395341992,                  \n",
      " [117/300] At 1.0 dB, Train Loss: 1.0105730325449258e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3985 minutes\n",
      "encoder learning rate: 4.17e-04, decoder learning rate: 4.17e-04\n",
      "[118/300] At -1.0 dB, Train Loss: 0.0063687474466860294 Train BER 0.00025945945526473224,                  \n",
      " [118/300] At 1.0 dB, Train Loss: 3.5369466786505654e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4502 minutes\n",
      "encoder learning rate: 4.14e-04, decoder learning rate: 4.14e-04\n",
      "[119/300] At -1.0 dB, Train Loss: 0.002962967846542597 Train BER 0.00011891892063431442,                  \n",
      " [119/300] At 1.0 dB, Train Loss: 7.661413110326976e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.5618 minutes\n",
      "encoder learning rate: 4.12e-04, decoder learning rate: 4.12e-04\n",
      "[120/300] At -1.0 dB, Train Loss: 0.006555786821991205 Train BER 0.0003459459403529763,                  \n",
      " [120/300] At 1.0 dB, Train Loss: 4.3779658881248906e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0661 minutes\n",
      "encoder learning rate: 4.10e-04, decoder learning rate: 4.10e-04\n",
      "[121/300] At -1.0 dB, Train Loss: 0.004369464702904224 Train BER 0.00021081081649754196,                  \n",
      " [121/300] At 1.0 dB, Train Loss: 3.895655936503317e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1313 minutes\n",
      "encoder learning rate: 4.07e-04, decoder learning rate: 4.07e-04\n",
      "[122/300] At -1.0 dB, Train Loss: 0.002497319597750902 Train BER 8.108108158921823e-05,                  \n",
      " [122/300] At 1.0 dB, Train Loss: 9.745279385242611e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.0872 minutes\n",
      "encoder learning rate: 4.05e-04, decoder learning rate: 4.05e-04\n",
      "[123/300] At -1.0 dB, Train Loss: 0.003300103358924389 Train BER 0.00015135135618038476,                  \n",
      " [123/300] At 1.0 dB, Train Loss: 7.486385129595874e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0930 minutes\n",
      "encoder learning rate: 4.02e-04, decoder learning rate: 4.02e-04\n",
      "[124/300] At -1.0 dB, Train Loss: 0.0018703570822253823 Train BER 8.108108158921823e-05,                  \n",
      " [124/300] At 1.0 dB, Train Loss: 6.370594201143831e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1893 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[125/300] At -1.0 dB, Train Loss: 0.0056880442425608635 Train BER 0.00021081081649754196,                  \n",
      " [125/300] At 1.0 dB, Train Loss: 3.036912858078722e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1431 minutes\n",
      "encoder learning rate: 3.97e-04, decoder learning rate: 3.97e-04\n",
      "[126/300] At -1.0 dB, Train Loss: 0.0034442367032170296 Train BER 0.00014054053463041782,                  \n",
      " [126/300] At 1.0 dB, Train Loss: 1.655251480769948e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2833 minutes\n",
      "encoder learning rate: 3.95e-04, decoder learning rate: 3.95e-04\n",
      "[127/300] At -1.0 dB, Train Loss: 0.004134194925427437 Train BER 0.000156756752403453,                  \n",
      " [127/300] At 1.0 dB, Train Loss: 3.770788453039131e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2176 minutes\n",
      "encoder learning rate: 3.92e-04, decoder learning rate: 3.92e-04\n",
      "[128/300] At -1.0 dB, Train Loss: 0.00907539576292038 Train BER 0.00037837837589904666,                  \n",
      " [128/300] At 1.0 dB, Train Loss: 2.674921006473596e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2279 minutes\n",
      "encoder learning rate: 3.89e-04, decoder learning rate: 3.89e-04\n",
      "[129/300] At -1.0 dB, Train Loss: 0.001568025560118258 Train BER 3.243243190809153e-05,                  \n",
      " [129/300] At 1.0 dB, Train Loss: 7.430650839523878e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2133 minutes\n",
      "encoder learning rate: 3.87e-04, decoder learning rate: 3.87e-04\n",
      "[130/300] At -1.0 dB, Train Loss: 0.0024160409811884165 Train BER 8.648648508824408e-05,                  \n",
      " [130/300] At 1.0 dB, Train Loss: 2.6144898583879694e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0763 minutes\n",
      "encoder learning rate: 3.84e-04, decoder learning rate: 3.84e-04\n",
      "[131/300] At -1.0 dB, Train Loss: 0.002200080081820488 Train BER 8.648648508824408e-05,                  \n",
      " [131/300] At 1.0 dB, Train Loss: 1.3675493391929194e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1017 minutes\n",
      "encoder learning rate: 3.82e-04, decoder learning rate: 3.82e-04\n",
      "[132/300] At -1.0 dB, Train Loss: 0.002367973793298006 Train BER 8.108108158921823e-05,                  \n",
      " [132/300] At 1.0 dB, Train Loss: 7.440061835950473e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.0877 minutes\n",
      "encoder learning rate: 3.79e-04, decoder learning rate: 3.79e-04\n",
      "[133/300] At -1.0 dB, Train Loss: 0.004917925223708153 Train BER 0.00023243243049364537,                  \n",
      " [133/300] At 1.0 dB, Train Loss: 1.2512865623648395e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1588 minutes\n",
      "encoder learning rate: 3.76e-04, decoder learning rate: 3.76e-04\n",
      "[134/300] At -1.0 dB, Train Loss: 0.004195226822048426 Train BER 0.00021081081649754196,                  \n",
      " [134/300] At 1.0 dB, Train Loss: 4.967663699062541e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1201 minutes\n",
      "encoder learning rate: 3.73e-04, decoder learning rate: 3.73e-04\n",
      "[135/300] At -1.0 dB, Train Loss: 0.0032317820005118847 Train BER 0.00012972972763236612,                  \n",
      " [135/300] At 1.0 dB, Train Loss: 1.200423594127642e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1802 minutes\n",
      "encoder learning rate: 3.71e-04, decoder learning rate: 3.71e-04\n",
      "[136/300] At -1.0 dB, Train Loss: 0.0033838478848338127 Train BER 0.00015135135618038476,                  \n",
      " [136/300] At 1.0 dB, Train Loss: 1.4343152088258648e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2381 minutes\n",
      "encoder learning rate: 3.68e-04, decoder learning rate: 3.68e-04\n",
      "[137/300] At -1.0 dB, Train Loss: 0.003974551800638437 Train BER 0.00016216216317843646,                  \n",
      " [137/300] At 1.0 dB, Train Loss: 1.4166042774377274e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1014 minutes\n",
      "encoder learning rate: 3.65e-04, decoder learning rate: 3.65e-04\n",
      "[138/300] At -1.0 dB, Train Loss: 0.00376057717949152 Train BER 0.00012972972763236612,                  \n",
      " [138/300] At 1.0 dB, Train Loss: 6.960832251934335e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3360 minutes\n",
      "encoder learning rate: 3.62e-04, decoder learning rate: 3.62e-04\n",
      "[139/300] At -1.0 dB, Train Loss: 0.0017221816815435886 Train BER 8.108108158921823e-05,                  \n",
      " [139/300] At 1.0 dB, Train Loss: 2.287125062139239e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2490 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[140/300] At -1.0 dB, Train Loss: 0.0035257781855762005 Train BER 0.0001351351384073496,                  \n",
      " [140/300] At 1.0 dB, Train Loss: 4.388542492961278e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2521 minutes\n",
      "encoder learning rate: 3.57e-04, decoder learning rate: 3.57e-04\n",
      "[141/300] At -1.0 dB, Train Loss: 0.0026273918338119984 Train BER 8.648648508824408e-05,                  \n",
      " [141/300] At 1.0 dB, Train Loss: 2.067230161628686e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0912 minutes\n",
      "encoder learning rate: 3.54e-04, decoder learning rate: 3.54e-04\n",
      "[142/300] At -1.0 dB, Train Loss: 0.0031191057059913874 Train BER 0.00012432433140929788,                  \n",
      " [142/300] At 1.0 dB, Train Loss: 1.1045612154703122e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1783 minutes\n",
      "encoder learning rate: 3.51e-04, decoder learning rate: 3.51e-04\n",
      "[143/300] At -1.0 dB, Train Loss: 0.0030608209781348705 Train BER 0.0001459459454054013,                  \n",
      " [143/300] At 1.0 dB, Train Loss: 1.4426549341806094e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4433 minutes\n",
      "encoder learning rate: 3.48e-04, decoder learning rate: 3.48e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144/300] At -1.0 dB, Train Loss: 0.0029587396420538425 Train BER 0.00017297297017648816,                  \n",
      " [144/300] At 1.0 dB, Train Loss: 8.753738711675396e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1848 minutes\n",
      "encoder learning rate: 3.45e-04, decoder learning rate: 3.45e-04\n",
      "[145/300] At -1.0 dB, Train Loss: 0.003922038711607456 Train BER 0.000156756752403453,                  \n",
      " [145/300] At 1.0 dB, Train Loss: 3.1711397241451778e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2013 minutes\n",
      "encoder learning rate: 3.42e-04, decoder learning rate: 3.42e-04\n",
      "[146/300] At -1.0 dB, Train Loss: 0.0016657778760418296 Train BER 4.324324254412204e-05,                  \n",
      " [146/300] At 1.0 dB, Train Loss: 3.9563474274473265e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2524 minutes\n",
      "encoder learning rate: 3.39e-04, decoder learning rate: 3.39e-04\n",
      "[147/300] At -1.0 dB, Train Loss: 0.0032519774977117777 Train BER 0.0001459459454054013,                  \n",
      " [147/300] At 1.0 dB, Train Loss: 2.3181128199212253e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1676 minutes\n",
      "encoder learning rate: 3.36e-04, decoder learning rate: 3.36e-04\n",
      "[148/300] At -1.0 dB, Train Loss: 0.004488746635615826 Train BER 0.0001945945987245068,                  \n",
      " [148/300] At 1.0 dB, Train Loss: 9.958581358660012e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2477 minutes\n",
      "encoder learning rate: 3.34e-04, decoder learning rate: 3.34e-04\n",
      "[149/300] At -1.0 dB, Train Loss: 0.0031082327477633953 Train BER 0.0001081081063603051,                  \n",
      " [149/300] At 1.0 dB, Train Loss: 1.8727107544691535e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2009 minutes\n",
      "encoder learning rate: 3.31e-04, decoder learning rate: 3.31e-04\n",
      "[150/300] At -1.0 dB, Train Loss: 0.0013787630014121532 Train BER 5.945946031715721e-05,                  \n",
      " [150/300] At 1.0 dB, Train Loss: 2.1305497739376733e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2285 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[151/300] At -1.0 dB, Train Loss: 0.001916512381285429 Train BER 0.00010270270286127925,                  \n",
      " [151/300] At 1.0 dB, Train Loss: 9.39900417051831e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.0818 minutes\n",
      "encoder learning rate: 3.25e-04, decoder learning rate: 3.25e-04\n",
      "[152/300] At -1.0 dB, Train Loss: 0.002002858556807041 Train BER 5.405405318015255e-05,                  \n",
      " [152/300] At 1.0 dB, Train Loss: 1.2647319636016618e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2169 minutes\n",
      "encoder learning rate: 3.22e-04, decoder learning rate: 3.22e-04\n",
      "[153/300] At -1.0 dB, Train Loss: 0.005061460193246603 Train BER 0.00022702703427057713,                  \n",
      " [153/300] At 1.0 dB, Train Loss: 4.79238451589481e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2575 minutes\n",
      "encoder learning rate: 3.19e-04, decoder learning rate: 3.19e-04\n",
      "[154/300] At -1.0 dB, Train Loss: 0.0017819699132815003 Train BER 5.945946031715721e-05,                  \n",
      " [154/300] At 1.0 dB, Train Loss: 7.166845534811728e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1905 minutes\n",
      "encoder learning rate: 3.16e-04, decoder learning rate: 3.16e-04\n",
      "[155/300] At -1.0 dB, Train Loss: 0.004427483305335045 Train BER 0.00017837838095147163,                  \n",
      " [155/300] At 1.0 dB, Train Loss: 7.0218020482570864e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.5065 minutes\n",
      "encoder learning rate: 3.13e-04, decoder learning rate: 3.13e-04\n",
      "[156/300] At -1.0 dB, Train Loss: 0.0014568665064871311 Train BER 7.027026731520891e-05,                  \n",
      " [156/300] At 1.0 dB, Train Loss: 2.1244961772026727e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1739 minutes\n",
      "encoder learning rate: 3.10e-04, decoder learning rate: 3.10e-04\n",
      "[157/300] At -1.0 dB, Train Loss: 0.003976102918386459 Train BER 0.00018918918794952333,                  \n",
      " [157/300] At 1.0 dB, Train Loss: 2.1173866571189137e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2857 minutes\n",
      "encoder learning rate: 3.06e-04, decoder learning rate: 3.06e-04\n",
      "[158/300] At -1.0 dB, Train Loss: 0.004840277601033449 Train BER 0.00021081081649754196,                  \n",
      " [158/300] At 1.0 dB, Train Loss: 1.6379553926526569e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1911 minutes\n",
      "encoder learning rate: 3.03e-04, decoder learning rate: 3.03e-04\n",
      "[159/300] At -1.0 dB, Train Loss: 0.004461875185370445 Train BER 0.00017297297017648816,                  \n",
      " [159/300] At 1.0 dB, Train Loss: 1.2912212241644738e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1291 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[160/300] At -1.0 dB, Train Loss: 0.0023711593821644783 Train BER 6.486486381618306e-05,                  \n",
      " [160/300] At 1.0 dB, Train Loss: 9.588310376784648e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1105 minutes\n",
      "encoder learning rate: 2.97e-04, decoder learning rate: 2.97e-04\n",
      "[161/300] At -1.0 dB, Train Loss: 0.001555261085741222 Train BER 5.405405318015255e-05,                  \n",
      " [161/300] At 1.0 dB, Train Loss: 1.165962930826936e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2592 minutes\n",
      "encoder learning rate: 2.94e-04, decoder learning rate: 2.94e-04\n",
      "[162/300] At -1.0 dB, Train Loss: 0.002589273499324918 Train BER 9.72972993622534e-05,                  \n",
      " [162/300] At 1.0 dB, Train Loss: 7.488461051252671e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2501 minutes\n",
      "encoder learning rate: 2.91e-04, decoder learning rate: 2.91e-04\n",
      "[163/300] At -1.0 dB, Train Loss: 0.0021872606594115496 Train BER 7.027026731520891e-05,                  \n",
      " [163/300] At 1.0 dB, Train Loss: 1.236600155607448e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1716 minutes\n",
      "encoder learning rate: 2.88e-04, decoder learning rate: 2.88e-04\n",
      "[164/300] At -1.0 dB, Train Loss: 0.004670462105423212 Train BER 0.0002648648514878005,                  \n",
      " [164/300] At 1.0 dB, Train Loss: 8.077047823462635e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1510 minutes\n",
      "encoder learning rate: 2.85e-04, decoder learning rate: 2.85e-04\n",
      "[165/300] At -1.0 dB, Train Loss: 0.0022764678578823805 Train BER 5.405405318015255e-05,                  \n",
      " [165/300] At 1.0 dB, Train Loss: 1.4804209058638662e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2864 minutes\n",
      "encoder learning rate: 2.82e-04, decoder learning rate: 2.82e-04\n",
      "[166/300] At -1.0 dB, Train Loss: 0.0020249253138899803 Train BER 0.00010270270286127925,                  \n",
      " [166/300] At 1.0 dB, Train Loss: 9.333335810879362e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.0946 minutes\n",
      "encoder learning rate: 2.79e-04, decoder learning rate: 2.79e-04\n",
      "[167/300] At -1.0 dB, Train Loss: 0.0026420794893056154 Train BER 0.0001081081063603051,                  \n",
      " [167/300] At 1.0 dB, Train Loss: 1.473052975597966e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1394 minutes\n",
      "encoder learning rate: 2.76e-04, decoder learning rate: 2.76e-04\n",
      "[168/300] At -1.0 dB, Train Loss: 0.0013796527637168765 Train BER 7.567567809019238e-05,                  \n",
      " [168/300] At 1.0 dB, Train Loss: 8.878670996637084e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1417 minutes\n",
      "encoder learning rate: 2.72e-04, decoder learning rate: 2.72e-04\n",
      "[169/300] At -1.0 dB, Train Loss: 0.002314699348062277 Train BER 7.567567809019238e-05,                  \n",
      " [169/300] At 1.0 dB, Train Loss: 2.7756707368098432e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1612 minutes\n",
      "encoder learning rate: 2.69e-04, decoder learning rate: 2.69e-04\n",
      "[170/300] At -1.0 dB, Train Loss: 0.002498260699212551 Train BER 6.486486381618306e-05,                  \n",
      " [170/300] At 1.0 dB, Train Loss: 6.5489016378705855e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3822 minutes\n",
      "encoder learning rate: 2.66e-04, decoder learning rate: 2.66e-04\n",
      "[171/300] At -1.0 dB, Train Loss: 0.0032742100302129984 Train BER 0.0002054054057225585,                  \n",
      " [171/300] At 1.0 dB, Train Loss: 7.860630262257473e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2247 minutes\n",
      "encoder learning rate: 2.63e-04, decoder learning rate: 2.63e-04\n",
      "[172/300] At -1.0 dB, Train Loss: 0.001143201719969511 Train BER 5.405405318015255e-05,                  \n",
      " [172/300] At 1.0 dB, Train Loss: 5.223415428190492e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1707 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173/300] At -1.0 dB, Train Loss: 0.0025416517164558172 Train BER 0.0001081081063603051,                  \n",
      " [173/300] At 1.0 dB, Train Loss: 8.51347408570291e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1848 minutes\n",
      "encoder learning rate: 2.57e-04, decoder learning rate: 2.57e-04\n",
      "[174/300] At -1.0 dB, Train Loss: 0.0030316489282995462 Train BER 0.00014054053463041782,                  \n",
      " [174/300] At 1.0 dB, Train Loss: 8.378550774068572e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2408 minutes\n",
      "encoder learning rate: 2.54e-04, decoder learning rate: 2.54e-04\n",
      "[175/300] At -1.0 dB, Train Loss: 0.004803390707820654 Train BER 0.00018378377717453986,                  \n",
      " [175/300] At 1.0 dB, Train Loss: 2.8100823328713886e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1489 minutes\n",
      "encoder learning rate: 2.51e-04, decoder learning rate: 2.51e-04\n",
      "[176/300] At -1.0 dB, Train Loss: 0.004291734658181667 Train BER 0.00019999999494757503,                  \n",
      " [176/300] At 1.0 dB, Train Loss: 3.441271837800741e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1928 minutes\n",
      "encoder learning rate: 2.47e-04, decoder learning rate: 2.47e-04\n",
      "[177/300] At -1.0 dB, Train Loss: 0.003425568575039506 Train BER 0.00017837838095147163,                  \n",
      " [177/300] At 1.0 dB, Train Loss: 3.117535015917383e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1935 minutes\n",
      "encoder learning rate: 2.44e-04, decoder learning rate: 2.44e-04\n",
      "[178/300] At -1.0 dB, Train Loss: 0.0027896107640117407 Train BER 0.00012972972763236612,                  \n",
      " [178/300] At 1.0 dB, Train Loss: 4.249975518177962e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1432 minutes\n",
      "encoder learning rate: 2.41e-04, decoder learning rate: 2.41e-04\n",
      "[179/300] At -1.0 dB, Train Loss: 0.0025558164343237877 Train BER 0.00012972972763236612,                  \n",
      " [179/300] At 1.0 dB, Train Loss: 7.694065971008968e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4248 minutes\n",
      "encoder learning rate: 2.38e-04, decoder learning rate: 2.38e-04\n",
      "[180/300] At -1.0 dB, Train Loss: 0.0017813933081924915 Train BER 4.86486496811267e-05,                  \n",
      " [180/300] At 1.0 dB, Train Loss: 6.761787858522439e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2797 minutes\n",
      "encoder learning rate: 2.35e-04, decoder learning rate: 2.35e-04\n",
      "[181/300] At -1.0 dB, Train Loss: 0.004833118990063667 Train BER 0.0002162162127206102,                  \n",
      " [181/300] At 1.0 dB, Train Loss: 4.559634817269398e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1035 minutes\n",
      "encoder learning rate: 2.32e-04, decoder learning rate: 2.32e-04\n",
      "[182/300] At -1.0 dB, Train Loss: 0.0013514162274077535 Train BER 5.945946031715721e-05,                  \n",
      " [182/300] At 1.0 dB, Train Loss: 1.1844471146105207e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2595 minutes\n",
      "encoder learning rate: 2.29e-04, decoder learning rate: 2.29e-04\n",
      "[183/300] At -1.0 dB, Train Loss: 0.005401587579399347 Train BER 0.0003189189301338047,                  \n",
      " [183/300] At 1.0 dB, Train Loss: 2.128999767592177e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1431 minutes\n",
      "encoder learning rate: 2.25e-04, decoder learning rate: 2.25e-04\n",
      "[184/300] At -1.0 dB, Train Loss: 0.0027145373169332743 Train BER 0.00016756757395341992,                  \n",
      " [184/300] At 1.0 dB, Train Loss: 6.810439572291216e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2004 minutes\n",
      "encoder learning rate: 2.22e-04, decoder learning rate: 2.22e-04\n",
      "[185/300] At -1.0 dB, Train Loss: 0.002409047447144985 Train BER 9.189188858726993e-05,                  \n",
      " [185/300] At 1.0 dB, Train Loss: 1.2325294846959878e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2696 minutes\n",
      "encoder learning rate: 2.19e-04, decoder learning rate: 2.19e-04\n",
      "[186/300] At -1.0 dB, Train Loss: 0.0027611511759459972 Train BER 0.00015135135618038476,                  \n",
      " [186/300] At 1.0 dB, Train Loss: 1.0800439440572518e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1910 minutes\n",
      "encoder learning rate: 2.16e-04, decoder learning rate: 2.16e-04\n",
      "[187/300] At -1.0 dB, Train Loss: 0.006730737630277872 Train BER 0.0002918918908108026,                  \n",
      " [187/300] At 1.0 dB, Train Loss: 4.6506133912771475e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1185 minutes\n",
      "encoder learning rate: 2.13e-04, decoder learning rate: 2.13e-04\n",
      "[188/300] At -1.0 dB, Train Loss: 0.0027425973676145077 Train BER 0.00012972972763236612,                  \n",
      " [188/300] At 1.0 dB, Train Loss: 2.2165362679515965e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2490 minutes\n",
      "encoder learning rate: 2.10e-04, decoder learning rate: 2.10e-04\n",
      "[189/300] At -1.0 dB, Train Loss: 0.0028638984076678753 Train BER 0.00014054053463041782,                  \n",
      " [189/300] At 1.0 dB, Train Loss: 6.562508474416973e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2836 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[190/300] At -1.0 dB, Train Loss: 0.0032415008172392845 Train BER 0.000156756752403453,                  \n",
      " [190/300] At 1.0 dB, Train Loss: 5.073655415799294e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1742 minutes\n",
      "encoder learning rate: 2.04e-04, decoder learning rate: 2.04e-04\n",
      "[191/300] At -1.0 dB, Train Loss: 0.0038279895670711994 Train BER 0.00016216216317843646,                  \n",
      " [191/300] At 1.0 dB, Train Loss: 2.249931640108116e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1347 minutes\n",
      "encoder learning rate: 2.01e-04, decoder learning rate: 2.01e-04\n",
      "[192/300] At -1.0 dB, Train Loss: 0.0036948113702237606 Train BER 0.00019999999494757503,                  \n",
      " [192/300] At 1.0 dB, Train Loss: 7.37735717848409e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1428 minutes\n",
      "encoder learning rate: 1.98e-04, decoder learning rate: 1.98e-04\n",
      "[193/300] At -1.0 dB, Train Loss: 0.002978410106152296 Train BER 0.00017837838095147163,                  \n",
      " [193/300] At 1.0 dB, Train Loss: 7.862119559831626e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2033 minutes\n",
      "encoder learning rate: 1.95e-04, decoder learning rate: 1.95e-04\n",
      "[194/300] At -1.0 dB, Train Loss: 0.003753252560272813 Train BER 0.0001945945987245068,                  \n",
      " [194/300] At 1.0 dB, Train Loss: 2.0531990685412893e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2126 minutes\n",
      "encoder learning rate: 1.91e-04, decoder learning rate: 1.91e-04\n",
      "[195/300] At -1.0 dB, Train Loss: 0.00559341860935092 Train BER 0.00023783784126862884,                  \n",
      " [195/300] At 1.0 dB, Train Loss: 4.6702990630365093e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.0794 minutes\n",
      "encoder learning rate: 1.88e-04, decoder learning rate: 1.88e-04\n",
      "[196/300] At -1.0 dB, Train Loss: 0.001724286237731576 Train BER 7.567567809019238e-05,                  \n",
      " [196/300] At 1.0 dB, Train Loss: 3.283984369772952e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3050 minutes\n",
      "encoder learning rate: 1.85e-04, decoder learning rate: 1.85e-04\n",
      "[197/300] At -1.0 dB, Train Loss: 0.0013488181866705418 Train BER 5.945946031715721e-05,                  \n",
      " [197/300] At 1.0 dB, Train Loss: 1.699447057035286e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2115 minutes\n",
      "encoder learning rate: 1.82e-04, decoder learning rate: 1.82e-04\n",
      "[198/300] At -1.0 dB, Train Loss: 0.004022576380521059 Train BER 0.00016756757395341992,                  \n",
      " [198/300] At 1.0 dB, Train Loss: 3.449104895025812e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2995 minutes\n",
      "encoder learning rate: 1.79e-04, decoder learning rate: 1.79e-04\n",
      "[199/300] At -1.0 dB, Train Loss: 0.0014543116558343172 Train BER 5.405405318015255e-05,                  \n",
      " [199/300] At 1.0 dB, Train Loss: 1.8802593331201933e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2214 minutes\n",
      "encoder learning rate: 1.76e-04, decoder learning rate: 1.76e-04\n",
      "[200/300] At -1.0 dB, Train Loss: 0.00639876164495945 Train BER 0.0002918918908108026,                  \n",
      " [200/300] At 1.0 dB, Train Loss: 9.674458851804957e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2959 minutes\n",
      "encoder learning rate: 1.73e-04, decoder learning rate: 1.73e-04\n",
      "[201/300] At -1.0 dB, Train Loss: 0.001894923159852624 Train BER 9.189188858726993e-05,                  \n",
      " [201/300] At 1.0 dB, Train Loss: 6.525544336000166e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2648 minutes\n",
      "encoder learning rate: 1.70e-04, decoder learning rate: 1.70e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202/300] At -1.0 dB, Train Loss: 0.003178313374519348 Train BER 0.0001351351384073496,                  \n",
      " [202/300] At 1.0 dB, Train Loss: 3.3396870549040614e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4332 minutes\n",
      "encoder learning rate: 1.67e-04, decoder learning rate: 1.67e-04\n",
      "[203/300] At -1.0 dB, Train Loss: 0.0038691675290465355 Train BER 0.0002054054057225585,                  \n",
      " [203/300] At 1.0 dB, Train Loss: 1.1245994073760812e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3112 minutes\n",
      "encoder learning rate: 1.65e-04, decoder learning rate: 1.65e-04\n",
      "[204/300] At -1.0 dB, Train Loss: 0.003721170127391815 Train BER 0.00017297297017648816,                  \n",
      " [204/300] At 1.0 dB, Train Loss: 2.359404334129067e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2122 minutes\n",
      "encoder learning rate: 1.62e-04, decoder learning rate: 1.62e-04\n",
      "[205/300] At -1.0 dB, Train Loss: 0.0014871351886540651 Train BER 7.567567809019238e-05,                  \n",
      " [205/300] At 1.0 dB, Train Loss: 2.7490409593156073e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2433 minutes\n",
      "encoder learning rate: 1.59e-04, decoder learning rate: 1.59e-04\n",
      "[206/300] At -1.0 dB, Train Loss: 0.0015638970071449876 Train BER 3.783783904509619e-05,                  \n",
      " [206/300] At 1.0 dB, Train Loss: 6.934572411410045e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1391 minutes\n",
      "encoder learning rate: 1.56e-04, decoder learning rate: 1.56e-04\n",
      "[207/300] At -1.0 dB, Train Loss: 0.003832586808130145 Train BER 0.00016216216317843646,                  \n",
      " [207/300] At 1.0 dB, Train Loss: 4.356291469775897e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1139 minutes\n",
      "encoder learning rate: 1.53e-04, decoder learning rate: 1.53e-04\n",
      "[208/300] At -1.0 dB, Train Loss: 0.005137531086802483 Train BER 0.00024864866281859577,                  \n",
      " [208/300] At 1.0 dB, Train Loss: 1.5286997268049163e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1353 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[209/300] At -1.0 dB, Train Loss: 0.0008592885569669306 Train BER 2.7027026590076275e-05,                  \n",
      " [209/300] At 1.0 dB, Train Loss: 1.145123223977862e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1756 minutes\n",
      "encoder learning rate: 1.47e-04, decoder learning rate: 1.47e-04\n",
      "[210/300] At -1.0 dB, Train Loss: 0.002062817569822073 Train BER 0.00010270270286127925,                  \n",
      " [210/300] At 1.0 dB, Train Loss: 1.4232333342079073e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2152 minutes\n",
      "encoder learning rate: 1.44e-04, decoder learning rate: 1.44e-04\n",
      "[211/300] At -1.0 dB, Train Loss: 0.0019403684418648481 Train BER 7.567567809019238e-05,                  \n",
      " [211/300] At 1.0 dB, Train Loss: 1.2624628880075761e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2302 minutes\n",
      "encoder learning rate: 1.41e-04, decoder learning rate: 1.41e-04\n",
      "[212/300] At -1.0 dB, Train Loss: 0.002268158132210374 Train BER 6.486486381618306e-05,                  \n",
      " [212/300] At 1.0 dB, Train Loss: 1.26681379697402e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2834 minutes\n",
      "encoder learning rate: 1.39e-04, decoder learning rate: 1.39e-04\n",
      "[213/300] At -1.0 dB, Train Loss: 0.0022223545238375664 Train BER 0.00012432433140929788,                  \n",
      " [213/300] At 1.0 dB, Train Loss: 1.0699028507588082e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3490 minutes\n",
      "encoder learning rate: 1.36e-04, decoder learning rate: 1.36e-04\n",
      "[214/300] At -1.0 dB, Train Loss: 0.001886052661575377 Train BER 8.108108158921823e-05,                  \n",
      " [214/300] At 1.0 dB, Train Loss: 8.830610340737621e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3192 minutes\n",
      "encoder learning rate: 1.33e-04, decoder learning rate: 1.33e-04\n",
      "[215/300] At -1.0 dB, Train Loss: 0.0011721816845238209 Train BER 7.027026731520891e-05,                  \n",
      " [215/300] At 1.0 dB, Train Loss: 7.411093747577979e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2031 minutes\n",
      "encoder learning rate: 1.30e-04, decoder learning rate: 1.30e-04\n",
      "[216/300] At -1.0 dB, Train Loss: 0.000748818158172071 Train BER 4.324324254412204e-05,                  \n",
      " [216/300] At 1.0 dB, Train Loss: 8.701542810740648e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2477 minutes\n",
      "encoder learning rate: 1.28e-04, decoder learning rate: 1.28e-04\n",
      "[217/300] At -1.0 dB, Train Loss: 0.0018596990266814828 Train BER 0.00010270270286127925,                  \n",
      " [217/300] At 1.0 dB, Train Loss: 4.237634129822254e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3995 minutes\n",
      "encoder learning rate: 1.25e-04, decoder learning rate: 1.25e-04\n",
      "[218/300] At -1.0 dB, Train Loss: 0.0008247936493717134 Train BER 1.6216215954045765e-05,                  \n",
      " [218/300] At 1.0 dB, Train Loss: 1.2462811355362646e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2908 minutes\n",
      "encoder learning rate: 1.22e-04, decoder learning rate: 1.22e-04\n",
      "[219/300] At -1.0 dB, Train Loss: 0.002729421481490135 Train BER 7.567567809019238e-05,                  \n",
      " [219/300] At 1.0 dB, Train Loss: 1.5622648788848892e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3147 minutes\n",
      "encoder learning rate: 1.19e-04, decoder learning rate: 1.19e-04\n",
      "[220/300] At -1.0 dB, Train Loss: 0.0022847838699817657 Train BER 0.0001081081063603051,                  \n",
      " [220/300] At 1.0 dB, Train Loss: 2.377599957981147e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3494 minutes\n",
      "encoder learning rate: 1.17e-04, decoder learning rate: 1.17e-04\n",
      "[221/300] At -1.0 dB, Train Loss: 0.0008242052863352001 Train BER 1.081081063603051e-05,                  \n",
      " [221/300] At 1.0 dB, Train Loss: 1.370264362776652e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2242 minutes\n",
      "encoder learning rate: 1.14e-04, decoder learning rate: 1.14e-04\n",
      "[222/300] At -1.0 dB, Train Loss: 0.0013546374393627048 Train BER 4.86486496811267e-05,                  \n",
      " [222/300] At 1.0 dB, Train Loss: 1.2673184528466663e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1780 minutes\n",
      "encoder learning rate: 1.12e-04, decoder learning rate: 1.12e-04\n",
      "[223/300] At -1.0 dB, Train Loss: 0.003274091985076666 Train BER 0.00014054053463041782,                  \n",
      " [223/300] At 1.0 dB, Train Loss: 6.454151275647746e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.1628 minutes\n",
      "encoder learning rate: 1.09e-04, decoder learning rate: 1.09e-04\n",
      "[224/300] At -1.0 dB, Train Loss: 0.003965305630117655 Train BER 0.0001351351384073496,                  \n",
      " [224/300] At 1.0 dB, Train Loss: 1.9050454511670978e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.5444 minutes\n",
      "encoder learning rate: 1.06e-04, decoder learning rate: 1.06e-04\n",
      "[225/300] At -1.0 dB, Train Loss: 0.0003951530670747161 Train BER 5.405405318015255e-06,                  \n",
      " [225/300] At 1.0 dB, Train Loss: 4.0493264918950445e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3617 minutes\n",
      "encoder learning rate: 1.04e-04, decoder learning rate: 1.04e-04\n",
      "[226/300] At -1.0 dB, Train Loss: 0.0008861338137649 Train BER 4.324324254412204e-05,                  \n",
      " [226/300] At 1.0 dB, Train Loss: 2.2338265353027964e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3901 minutes\n",
      "encoder learning rate: 1.01e-04, decoder learning rate: 1.01e-04\n",
      "[227/300] At -1.0 dB, Train Loss: 0.0019462122581899166 Train BER 8.108108158921823e-05,                  \n",
      " [227/300] At 1.0 dB, Train Loss: 1.9385802261240315e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2311 minutes\n",
      "encoder learning rate: 9.88e-05, decoder learning rate: 9.88e-05\n",
      "[228/300] At -1.0 dB, Train Loss: 0.0006890976219438016 Train BER 1.6216215954045765e-05,                  \n",
      " [228/300] At 1.0 dB, Train Loss: 1.989535007851373e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3189 minutes\n",
      "encoder learning rate: 9.63e-05, decoder learning rate: 9.63e-05\n",
      "[229/300] At -1.0 dB, Train Loss: 0.0019383952021598816 Train BER 9.72972993622534e-05,                  \n",
      " [229/300] At 1.0 dB, Train Loss: 6.924507488292875e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2552 minutes\n",
      "encoder learning rate: 9.39e-05, decoder learning rate: 9.39e-05\n",
      "[230/300] At -1.0 dB, Train Loss: 0.002564844908192754 Train BER 0.00012972972763236612,                  \n",
      " [230/300] At 1.0 dB, Train Loss: 2.817085089645843e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2717 minutes\n",
      "encoder learning rate: 9.15e-05, decoder learning rate: 9.15e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[231/300] At -1.0 dB, Train Loss: 0.0019405123312026262 Train BER 7.567567809019238e-05,                  \n",
      " [231/300] At 1.0 dB, Train Loss: 4.533684659691062e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2186 minutes\n",
      "encoder learning rate: 8.91e-05, decoder learning rate: 8.91e-05\n",
      "[232/300] At -1.0 dB, Train Loss: 0.003596610389649868 Train BER 0.00015135135618038476,                  \n",
      " [232/300] At 1.0 dB, Train Loss: 2.581256012490485e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2315 minutes\n",
      "encoder learning rate: 8.67e-05, decoder learning rate: 8.67e-05\n",
      "[233/300] At -1.0 dB, Train Loss: 0.0019188737496733665 Train BER 0.0001081081063603051,                  \n",
      " [233/300] At 1.0 dB, Train Loss: 5.916172085562721e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1628 minutes\n",
      "encoder learning rate: 8.43e-05, decoder learning rate: 8.43e-05\n",
      "[234/300] At -1.0 dB, Train Loss: 0.0025447974912822247 Train BER 0.00010270270286127925,                  \n",
      " [234/300] At 1.0 dB, Train Loss: 1.3874301885152818e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.1857 minutes\n",
      "encoder learning rate: 8.20e-05, decoder learning rate: 8.20e-05\n",
      "[235/300] At -1.0 dB, Train Loss: 0.001596897142007947 Train BER 9.72972993622534e-05,                  \n",
      " [235/300] At 1.0 dB, Train Loss: 4.92866547574522e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2760 minutes\n",
      "encoder learning rate: 7.97e-05, decoder learning rate: 7.97e-05\n",
      "[236/300] At -1.0 dB, Train Loss: 0.003880943637341261 Train BER 0.00012972972763236612,                  \n",
      " [236/300] At 1.0 dB, Train Loss: 9.609509561414598e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2718 minutes\n",
      "encoder learning rate: 7.74e-05, decoder learning rate: 7.74e-05\n",
      "[237/300] At -1.0 dB, Train Loss: 0.001410631462931633 Train BER 5.945946031715721e-05,                  \n",
      " [237/300] At 1.0 dB, Train Loss: 1.3505620017895126e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2713 minutes\n",
      "encoder learning rate: 7.52e-05, decoder learning rate: 7.52e-05\n",
      "[238/300] At -1.0 dB, Train Loss: 0.0021586690563708544 Train BER 8.108108158921823e-05,                  \n",
      " [238/300] At 1.0 dB, Train Loss: 5.77996900119615e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2180 minutes\n",
      "encoder learning rate: 7.30e-05, decoder learning rate: 7.30e-05\n",
      "[239/300] At -1.0 dB, Train Loss: 0.0018963087350130081 Train BER 5.405405318015255e-05,                  \n",
      " [239/300] At 1.0 dB, Train Loss: 8.438266377197579e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2462 minutes\n",
      "encoder learning rate: 7.08e-05, decoder learning rate: 7.08e-05\n",
      "[240/300] At -1.0 dB, Train Loss: 0.0025070987176150084 Train BER 9.189188858726993e-05,                  \n",
      " [240/300] At 1.0 dB, Train Loss: 3.6288940918893786e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3695 minutes\n",
      "encoder learning rate: 6.86e-05, decoder learning rate: 6.86e-05\n",
      "[241/300] At -1.0 dB, Train Loss: 0.0010322078596800566 Train BER 3.783783904509619e-05,                  \n",
      " [241/300] At 1.0 dB, Train Loss: 2.0780763065886276e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4187 minutes\n",
      "encoder learning rate: 6.65e-05, decoder learning rate: 6.65e-05\n",
      "[242/300] At -1.0 dB, Train Loss: 0.003625248558819294 Train BER 0.0001459459454054013,                  \n",
      " [242/300] At 1.0 dB, Train Loss: 7.30260296677443e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2958 minutes\n",
      "encoder learning rate: 6.44e-05, decoder learning rate: 6.44e-05\n",
      "[243/300] At -1.0 dB, Train Loss: 0.0034580021165311337 Train BER 0.0001351351384073496,                  \n",
      " [243/300] At 1.0 dB, Train Loss: 1.2572274499689229e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2604 minutes\n",
      "encoder learning rate: 6.23e-05, decoder learning rate: 6.23e-05\n",
      "[244/300] At -1.0 dB, Train Loss: 0.001971602439880371 Train BER 9.189188858726993e-05,                  \n",
      " [244/300] At 1.0 dB, Train Loss: 4.566461200283811e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3375 minutes\n",
      "encoder learning rate: 6.03e-05, decoder learning rate: 6.03e-05\n",
      "[245/300] At -1.0 dB, Train Loss: 0.0005373460007831454 Train BER 2.7027026590076275e-05,                  \n",
      " [245/300] At 1.0 dB, Train Loss: 1.1890089126609382e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2000 minutes\n",
      "encoder learning rate: 5.83e-05, decoder learning rate: 5.83e-05\n",
      "[246/300] At -1.0 dB, Train Loss: 0.0028797334525734186 Train BER 0.0001081081063603051,                  \n",
      " [246/300] At 1.0 dB, Train Loss: 2.1179903342272155e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3381 minutes\n",
      "encoder learning rate: 5.63e-05, decoder learning rate: 5.63e-05\n",
      "[247/300] At -1.0 dB, Train Loss: 0.001904751406982541 Train BER 7.567567809019238e-05,                  \n",
      " [247/300] At 1.0 dB, Train Loss: 3.4388602898616227e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2868 minutes\n",
      "encoder learning rate: 5.43e-05, decoder learning rate: 5.43e-05\n",
      "[248/300] At -1.0 dB, Train Loss: 0.0007260374841280282 Train BER 2.162162127206102e-05,                  \n",
      " [248/300] At 1.0 dB, Train Loss: 9.179596236208454e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2237 minutes\n",
      "encoder learning rate: 5.24e-05, decoder learning rate: 5.24e-05\n",
      "[249/300] At -1.0 dB, Train Loss: 0.0010699055856093764 Train BER 2.7027026590076275e-05,                  \n",
      " [249/300] At 1.0 dB, Train Loss: 1.2640578006539727e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2166 minutes\n",
      "encoder learning rate: 5.05e-05, decoder learning rate: 5.05e-05\n",
      "[250/300] At -1.0 dB, Train Loss: 0.0002883851993829012 Train BER 5.405405318015255e-06,                  \n",
      " [250/300] At 1.0 dB, Train Loss: 1.9378411764137127e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2532 minutes\n",
      "encoder learning rate: 4.87e-05, decoder learning rate: 4.87e-05\n",
      "[251/300] At -1.0 dB, Train Loss: 0.0036104286555200815 Train BER 0.00011891892063431442,                  \n",
      " [251/300] At 1.0 dB, Train Loss: 1.2539815088530304e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4067 minutes\n",
      "encoder learning rate: 4.68e-05, decoder learning rate: 4.68e-05\n",
      "[252/300] At -1.0 dB, Train Loss: 0.003807109547778964 Train BER 0.00016216216317843646,                  \n",
      " [252/300] At 1.0 dB, Train Loss: 2.9985432092871633e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4474 minutes\n",
      "encoder learning rate: 4.50e-05, decoder learning rate: 4.50e-05\n",
      "[253/300] At -1.0 dB, Train Loss: 0.004207070916891098 Train BER 0.0001945945987245068,                  \n",
      " [253/300] At 1.0 dB, Train Loss: 4.993762922822498e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3606 minutes\n",
      "encoder learning rate: 4.33e-05, decoder learning rate: 4.33e-05\n",
      "[254/300] At -1.0 dB, Train Loss: 0.0004282169684302062 Train BER 0.0,                  \n",
      " [254/300] At 1.0 dB, Train Loss: 2.475011115166126e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3656 minutes\n",
      "encoder learning rate: 4.15e-05, decoder learning rate: 4.15e-05\n",
      "[255/300] At -1.0 dB, Train Loss: 0.0007525304099544883 Train BER 2.162162127206102e-05,                  \n",
      " [255/300] At 1.0 dB, Train Loss: 3.287563004050753e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3463 minutes\n",
      "encoder learning rate: 3.98e-05, decoder learning rate: 3.98e-05\n",
      "[256/300] At -1.0 dB, Train Loss: 0.0031875881832093 Train BER 9.189188858726993e-05,                  \n",
      " [256/300] At 1.0 dB, Train Loss: 1.3603819297713926e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4979 minutes\n",
      "encoder learning rate: 3.82e-05, decoder learning rate: 3.82e-05\n",
      "[257/300] At -1.0 dB, Train Loss: 0.0022945443633943796 Train BER 8.108108158921823e-05,                  \n",
      " [257/300] At 1.0 dB, Train Loss: 1.402434463670943e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4654 minutes\n",
      "encoder learning rate: 3.65e-05, decoder learning rate: 3.65e-05\n",
      "[258/300] At -1.0 dB, Train Loss: 0.001037363545037806 Train BER 3.243243190809153e-05,                  \n",
      " [258/300] At 1.0 dB, Train Loss: 8.254774570559675e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3794 minutes\n",
      "encoder learning rate: 3.50e-05, decoder learning rate: 3.50e-05\n",
      "[259/300] At -1.0 dB, Train Loss: 0.002823814982548356 Train BER 0.00017837838095147163,                  \n",
      " [259/300] At 1.0 dB, Train Loss: 5.040440555603709e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4765 minutes\n",
      "encoder learning rate: 3.34e-05, decoder learning rate: 3.34e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260/300] At -1.0 dB, Train Loss: 0.0017742719501256943 Train BER 6.486486381618306e-05,                  \n",
      " [260/300] At 1.0 dB, Train Loss: 4.359122613095678e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.6139 minutes\n",
      "encoder learning rate: 3.19e-05, decoder learning rate: 3.19e-05\n",
      "[261/300] At -1.0 dB, Train Loss: 0.0028346129693090916 Train BER 0.00012972972763236612,                  \n",
      " [261/300] At 1.0 dB, Train Loss: 8.210106443584664e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.5520 minutes\n",
      "encoder learning rate: 3.04e-05, decoder learning rate: 3.04e-05\n",
      "[262/300] At -1.0 dB, Train Loss: 0.0029207917395979166 Train BER 0.00012972972763236612,                  \n",
      " [262/300] At 1.0 dB, Train Loss: 2.6778309347719187e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4192 minutes\n",
      "encoder learning rate: 2.89e-05, decoder learning rate: 2.89e-05\n",
      "[263/300] At -1.0 dB, Train Loss: 0.0031934024300426245 Train BER 0.000156756752403453,                  \n",
      " [263/300] At 1.0 dB, Train Loss: 1.417116254742723e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3181 minutes\n",
      "encoder learning rate: 2.75e-05, decoder learning rate: 2.75e-05\n",
      "[264/300] At -1.0 dB, Train Loss: 0.0032773385755717754 Train BER 0.00018918918794952333,                  \n",
      " [264/300] At 1.0 dB, Train Loss: 1.2379784948279848e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.5097 minutes\n",
      "encoder learning rate: 2.61e-05, decoder learning rate: 2.61e-05\n",
      "[265/300] At -1.0 dB, Train Loss: 0.0012346033472567797 Train BER 6.486486381618306e-05,                  \n",
      " [265/300] At 1.0 dB, Train Loss: 2.5149211069219746e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.5039 minutes\n",
      "encoder learning rate: 2.47e-05, decoder learning rate: 2.47e-05\n",
      "[266/300] At -1.0 dB, Train Loss: 0.0016778940334916115 Train BER 7.027026731520891e-05,                  \n",
      " [266/300] At 1.0 dB, Train Loss: 5.7142933655995876e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3837 minutes\n",
      "encoder learning rate: 2.34e-05, decoder learning rate: 2.34e-05\n",
      "[267/300] At -1.0 dB, Train Loss: 0.0009737950167618692 Train BER 4.324324254412204e-05,                  \n",
      " [267/300] At 1.0 dB, Train Loss: 4.799590556103794e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3910 minutes\n",
      "encoder learning rate: 2.21e-05, decoder learning rate: 2.21e-05\n",
      "[268/300] At -1.0 dB, Train Loss: 0.0035756395664066076 Train BER 0.00016216216317843646,                  \n",
      " [268/300] At 1.0 dB, Train Loss: 1.1286505241514533e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3138 minutes\n",
      "encoder learning rate: 2.09e-05, decoder learning rate: 2.09e-05\n",
      "[269/300] At -1.0 dB, Train Loss: 0.0035623370204120874 Train BER 0.00012972972763236612,                  \n",
      " [269/300] At 1.0 dB, Train Loss: 5.300711904965283e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3867 minutes\n",
      "encoder learning rate: 1.97e-05, decoder learning rate: 1.97e-05\n",
      "[270/300] At -1.0 dB, Train Loss: 0.0016801536548882723 Train BER 7.027026731520891e-05,                  \n",
      " [270/300] At 1.0 dB, Train Loss: 1.863515421973716e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4252 minutes\n",
      "encoder learning rate: 1.85e-05, decoder learning rate: 1.85e-05\n",
      "[271/300] At -1.0 dB, Train Loss: 0.0010568598518148065 Train BER 2.7027026590076275e-05,                  \n",
      " [271/300] At 1.0 dB, Train Loss: 2.2922549192117003e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3999 minutes\n",
      "encoder learning rate: 1.74e-05, decoder learning rate: 1.74e-05\n",
      "[272/300] At -1.0 dB, Train Loss: 0.003159805666655302 Train BER 0.00018378377717453986,                  \n",
      " [272/300] At 1.0 dB, Train Loss: 6.520514489238849e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4267 minutes\n",
      "encoder learning rate: 1.63e-05, decoder learning rate: 1.63e-05\n",
      "[273/300] At -1.0 dB, Train Loss: 0.000530105666257441 Train BER 1.081081063603051e-05,                  \n",
      " [273/300] At 1.0 dB, Train Loss: 1.8397643088974291e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3742 minutes\n",
      "encoder learning rate: 1.52e-05, decoder learning rate: 1.52e-05\n",
      "[274/300] At -1.0 dB, Train Loss: 0.0008628942887298763 Train BER 3.783783904509619e-05,                  \n",
      " [274/300] At 1.0 dB, Train Loss: 1.174972908302152e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3532 minutes\n",
      "encoder learning rate: 1.42e-05, decoder learning rate: 1.42e-05\n",
      "[275/300] At -1.0 dB, Train Loss: 0.0008871283498592675 Train BER 2.7027026590076275e-05,                  \n",
      " [275/300] At 1.0 dB, Train Loss: 3.359184120199643e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3906 minutes\n",
      "encoder learning rate: 1.32e-05, decoder learning rate: 1.32e-05\n",
      "[276/300] At -1.0 dB, Train Loss: 0.0030562763568013906 Train BER 0.0001351351384073496,                  \n",
      " [276/300] At 1.0 dB, Train Loss: 5.791539479105268e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3964 minutes\n",
      "encoder learning rate: 1.23e-05, decoder learning rate: 1.23e-05\n",
      "[277/300] At -1.0 dB, Train Loss: 0.003226093016564846 Train BER 0.0001459459454054013,                  \n",
      " [277/300] At 1.0 dB, Train Loss: 4.9948625928664114e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.5765 minutes\n",
      "encoder learning rate: 1.13e-05, decoder learning rate: 1.13e-05\n",
      "[278/300] At -1.0 dB, Train Loss: 0.002106506610289216 Train BER 9.72972993622534e-05,                  \n",
      " [278/300] At 1.0 dB, Train Loss: 7.376631288025237e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2703 minutes\n",
      "encoder learning rate: 1.05e-05, decoder learning rate: 1.05e-05\n",
      "[279/300] At -1.0 dB, Train Loss: 0.0012793677160516381 Train BER 5.945946031715721e-05,                  \n",
      " [279/300] At 1.0 dB, Train Loss: 5.947295449004741e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4145 minutes\n",
      "encoder learning rate: 9.64e-06, decoder learning rate: 9.64e-06\n",
      "[280/300] At -1.0 dB, Train Loss: 0.0036244296934455633 Train BER 0.00018378377717453986,                  \n",
      " [280/300] At 1.0 dB, Train Loss: 5.5596083257114515e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3732 minutes\n",
      "encoder learning rate: 8.84e-06, decoder learning rate: 8.84e-06\n",
      "[281/300] At -1.0 dB, Train Loss: 0.0034486697986721992 Train BER 0.00012972972763236612,                  \n",
      " [281/300] At 1.0 dB, Train Loss: 3.878140887536574e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4418 minutes\n",
      "encoder learning rate: 8.08e-06, decoder learning rate: 8.08e-06\n",
      "[282/300] At -1.0 dB, Train Loss: 0.0019182797987014055 Train BER 7.567567809019238e-05,                  \n",
      " [282/300] At 1.0 dB, Train Loss: 6.753625712008215e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.5872 minutes\n",
      "encoder learning rate: 7.36e-06, decoder learning rate: 7.36e-06\n",
      "[283/300] At -1.0 dB, Train Loss: 0.0007186994189396501 Train BER 2.7027026590076275e-05,                  \n",
      " [283/300] At 1.0 dB, Train Loss: 3.350897372911277e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3916 minutes\n",
      "encoder learning rate: 6.67e-06, decoder learning rate: 6.67e-06\n",
      "[284/300] At -1.0 dB, Train Loss: 0.0008491621119901538 Train BER 2.7027026590076275e-05,                  \n",
      " [284/300] At 1.0 dB, Train Loss: 4.2516268194958684e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4105 minutes\n",
      "encoder learning rate: 6.03e-06, decoder learning rate: 6.03e-06\n",
      "[285/300] At -1.0 dB, Train Loss: 0.002214195905253291 Train BER 8.648648508824408e-05,                  \n",
      " [285/300] At 1.0 dB, Train Loss: 5.008552079743822e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.5651 minutes\n",
      "encoder learning rate: 5.42e-06, decoder learning rate: 5.42e-06\n",
      "[286/300] At -1.0 dB, Train Loss: 0.001159288571216166 Train BER 4.324324254412204e-05,                  \n",
      " [286/300] At 1.0 dB, Train Loss: 3.529658556544746e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4857 minutes\n",
      "encoder learning rate: 4.85e-06, decoder learning rate: 4.85e-06\n",
      "[287/300] At -1.0 dB, Train Loss: 0.0010285486932843924 Train BER 3.783783904509619e-05,                  \n",
      " [287/300] At 1.0 dB, Train Loss: 1.1695126886479557e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.4379 minutes\n",
      "encoder learning rate: 4.32e-06, decoder learning rate: 4.32e-06\n",
      "[288/300] At -1.0 dB, Train Loss: 0.0016197414370253682 Train BER 7.567567809019238e-05,                  \n",
      " [288/300] At 1.0 dB, Train Loss: 4.828496571462892e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.5796 minutes\n",
      "encoder learning rate: 3.83e-06, decoder learning rate: 3.83e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[289/300] At -1.0 dB, Train Loss: 0.0029737502336502075 Train BER 0.00014054053463041782,                  \n",
      " [289/300] At 1.0 dB, Train Loss: 1.1262821431046177e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3574 minutes\n",
      "encoder learning rate: 3.38e-06, decoder learning rate: 3.38e-06\n",
      "[290/300] At -1.0 dB, Train Loss: 0.003115682862699032 Train BER 0.0001459459454054013,                  \n",
      " [290/300] At 1.0 dB, Train Loss: 5.748191824750393e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2874 minutes\n",
      "encoder learning rate: 2.97e-06, decoder learning rate: 2.97e-06\n",
      "[291/300] At -1.0 dB, Train Loss: 0.0032823069486767054 Train BER 0.000156756752403453,                  \n",
      " [291/300] At 1.0 dB, Train Loss: 2.0320178464316996e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3428 minutes\n",
      "encoder learning rate: 2.59e-06, decoder learning rate: 2.59e-06\n",
      "[292/300] At -1.0 dB, Train Loss: 0.000585290661547333 Train BER 3.243243190809153e-05,                  \n",
      " [292/300] At 1.0 dB, Train Loss: 1.5625578271283302e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3627 minutes\n",
      "encoder learning rate: 2.26e-06, decoder learning rate: 2.26e-06\n",
      "[293/300] At -1.0 dB, Train Loss: 0.001053971122018993 Train BER 5.945946031715721e-05,                  \n",
      " [293/300] At 1.0 dB, Train Loss: 1.4853949892312812e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3019 minutes\n",
      "encoder learning rate: 1.96e-06, decoder learning rate: 1.96e-06\n",
      "[294/300] At -1.0 dB, Train Loss: 0.0032301039900630713 Train BER 0.00012972972763236612,                  \n",
      " [294/300] At 1.0 dB, Train Loss: 4.6066176651038404e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4011 minutes\n",
      "encoder learning rate: 1.71e-06, decoder learning rate: 1.71e-06\n",
      "[295/300] At -1.0 dB, Train Loss: 0.0036524226889014244 Train BER 0.00016216216317843646,                  \n",
      " [295/300] At 1.0 dB, Train Loss: 2.1223538624326466e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.3690 minutes\n",
      "encoder learning rate: 1.49e-06, decoder learning rate: 1.49e-06\n",
      "[296/300] At -1.0 dB, Train Loss: 0.003694657701998949 Train BER 0.00015135135618038476,                  \n",
      " [296/300] At 1.0 dB, Train Loss: 4.972815077053383e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.4850 minutes\n",
      "encoder learning rate: 1.32e-06, decoder learning rate: 1.32e-06\n",
      "[297/300] At -1.0 dB, Train Loss: 0.000853244389872998 Train BER 4.324324254412204e-05,                  \n",
      " [297/300] At 1.0 dB, Train Loss: 5.865592811460374e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.4463 minutes\n",
      "encoder learning rate: 1.18e-06, decoder learning rate: 1.18e-06\n",
      "[298/300] At -1.0 dB, Train Loss: 0.0012970577226951718 Train BER 4.86486496811267e-05,                  \n",
      " [298/300] At 1.0 dB, Train Loss: 3.214589696654002e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.3269 minutes\n",
      "encoder learning rate: 1.08e-06, decoder learning rate: 1.08e-06\n",
      "[299/300] At -1.0 dB, Train Loss: 0.001988571835681796 Train BER 9.72972993622534e-05,                  \n",
      " [299/300] At 1.0 dB, Train Loss: 7.571478022327938e-07 Train BER 0.0\n",
      "Time for one full iteration is 7.2564 minutes\n",
      "encoder learning rate: 1.02e-06, decoder learning rate: 1.02e-06\n",
      "[300/300] At -1.0 dB, Train Loss: 0.0015348338056355715 Train BER 6.486486381618306e-05,                  \n",
      " [300/300] At 1.0 dB, Train Loss: 1.3120487665219116e-06 Train BER 0.0\n",
      "Time for one full iteration is 7.2898 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO__1.0_Encoder_KO_-1.0_Decoder/epochs_300_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/shubham/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/scipy/stats/_morestats.py:1882: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeppolar Shapiro test W = 0.8387881517410278, p-value = 0.0\n",
      "Gaussian Shapiro test W = 1.0000056028366089, p-value = 1.0\n",
      "Polar Shapiro test W = 0.6371783018112183, p-value = 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166342, BLER: 0.434906\n",
      "Deep - BER: 0.148442, BLER: 0.543532\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072584, BLER: 0.197038\n",
      "Deep - BER: 0.057467, BLER: 0.250810\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.019857, BLER: 0.055388\n",
      "Deep - BER: 0.012743, BLER: 0.068288\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003020, BLER: 0.008548\n",
      "Deep - BER: 0.001508, BLER: 0.010485\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000200, BLER: 0.000564\n",
      "Deep - BER: 0.000084, BLER: 0.000905\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.14844216206669808, 0.057467459447681904, 0.012743081108666957, 0.0015075675695152312, 8.437837830024364e-05]\n",
      "BERs of SC decoding: [0.16634175692498684, 0.0725841621235013, 0.019856999988667665, 0.003019675668794662, 0.00020002702720739763]\n",
      "BLERs of DeepPolar: [0.5435319999999988, 0.25081000000000037, 0.06828799999999989, 0.010484999999999925, 0.0009050000000000006]\n",
      "BLERs of SC decoding: [0.4349059999999999, 0.1970379999999995, 0.05538799999999986, 0.008547999999999955, 0.0005640000000000004]\n",
      "time = 367.5534640391668 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
