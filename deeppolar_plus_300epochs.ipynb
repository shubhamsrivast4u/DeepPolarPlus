{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = False  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 300  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = 0.  # SNR at which encoder is trained\n",
    "dec_train_snr = -2.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.001  # Decoder Learning rate\n",
    "enc_lr = 0.001  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}__{enc_train_snr}_Encoder_{decoder_type}_{dec_train_snr}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/300] At -2.0 dB, Train Loss: 8.268878936767578 Train BER 0.47841620445251465,                  \n",
      " [1/300] At 0.0 dB, Train Loss: 8.076854705810547 Train BER 0.47417837381362915\n",
      "Time for one full iteration is 8.2538 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[2/300] At -2.0 dB, Train Loss: 6.441169738769531 Train BER 0.46536216139793396,                  \n",
      " [2/300] At 0.0 dB, Train Loss: 6.366254806518555 Train BER 0.46329188346862793\n",
      "Time for one full iteration is 8.1895 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[3/300] At -2.0 dB, Train Loss: 5.996266841888428 Train BER 0.45796215534210205,                  \n",
      " [3/300] At 0.0 dB, Train Loss: 5.962064266204834 Train BER 0.44794052839279175\n",
      "Time for one full iteration is 8.1876 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[4/300] At -2.0 dB, Train Loss: 5.570695877075195 Train BER 0.4036000072956085,                  \n",
      " [4/300] At 0.0 dB, Train Loss: 5.31818962097168 Train BER 0.37116217613220215\n",
      "Time for one full iteration is 8.1491 minutes\n",
      "encoder learning rate: 8.00e-05, decoder learning rate: 8.00e-05\n",
      "[5/300] At -2.0 dB, Train Loss: 2.9704794883728027 Train BER 0.17334595322608948,                  \n",
      " [5/300] At 0.0 dB, Train Loss: 1.9692540168762207 Train BER 0.10821080952882767\n",
      "Time for one full iteration is 8.0912 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[6/300] At -2.0 dB, Train Loss: 1.498498797416687 Train BER 0.07401621341705322,                  \n",
      " [6/300] At 0.0 dB, Train Loss: 0.5157926678657532 Train BER 0.024005405604839325\n",
      "Time for one full iteration is 8.1738 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[7/300] At -2.0 dB, Train Loss: 0.914097249507904 Train BER 0.04525405541062355,                  \n",
      " [7/300] At 0.0 dB, Train Loss: 0.28315258026123047 Train BER 0.01591351442039013\n",
      "Time for one full iteration is 8.2096 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[8/300] At -2.0 dB, Train Loss: 0.6388899087905884 Train BER 0.032054055482149124,                  \n",
      " [8/300] At 0.0 dB, Train Loss: 0.22864027321338654 Train BER 0.01243783812969923\n",
      "Time for one full iteration is 8.1718 minutes\n",
      "encoder learning rate: 1.60e-04, decoder learning rate: 1.60e-04\n",
      "[9/300] At -2.0 dB, Train Loss: 0.38999807834625244 Train BER 0.015902703627943993,                  \n",
      " [9/300] At 0.0 dB, Train Loss: 0.08514156192541122 Train BER 0.0022810811642557383\n",
      "Time for one full iteration is 8.1535 minutes\n",
      "encoder learning rate: 1.80e-04, decoder learning rate: 1.80e-04\n",
      "[10/300] At -2.0 dB, Train Loss: 0.28949546813964844 Train BER 0.011816216632723808,                  \n",
      " [10/300] At 0.0 dB, Train Loss: 0.027354879304766655 Train BER 0.0006594594451598823\n",
      "Time for one full iteration is 8.1376 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[11/300] At -2.0 dB, Train Loss: 0.22516149282455444 Train BER 0.009264864958822727,                  \n",
      " [11/300] At 0.0 dB, Train Loss: 0.0172106996178627 Train BER 0.0004972973256371915\n",
      "Time for one full iteration is 8.1314 minutes\n",
      "encoder learning rate: 2.20e-04, decoder learning rate: 2.20e-04\n",
      "[12/300] At -2.0 dB, Train Loss: 0.192117378115654 Train BER 0.0076270271092653275,                  \n",
      " [12/300] At 0.0 dB, Train Loss: 0.009373013861477375 Train BER 0.0002162162127206102\n",
      "Time for one full iteration is 8.1590 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[13/300] At -2.0 dB, Train Loss: 0.15593160688877106 Train BER 0.006232432555407286,                  \n",
      " [13/300] At 0.0 dB, Train Loss: 0.009662328287959099 Train BER 0.000313513504806906\n",
      "Time for one full iteration is 8.2041 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[14/300] At -2.0 dB, Train Loss: 0.14705120027065277 Train BER 0.006237837951630354,                  \n",
      " [14/300] At 0.0 dB, Train Loss: 0.005793377757072449 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 8.2463 minutes\n",
      "encoder learning rate: 2.80e-04, decoder learning rate: 2.80e-04\n",
      "[15/300] At -2.0 dB, Train Loss: 0.13409879803657532 Train BER 0.005216216202825308,                  \n",
      " [15/300] At 0.0 dB, Train Loss: 0.004165983758866787 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.4182 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[16/300] At -2.0 dB, Train Loss: 0.12263567000627518 Train BER 0.004967567510902882,                  \n",
      " [16/300] At 0.0 dB, Train Loss: 0.0033945548348128796 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.1687 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[17/300] At -2.0 dB, Train Loss: 0.11527253687381744 Train BER 0.005021621473133564,                  \n",
      " [17/300] At 0.0 dB, Train Loss: 0.0034449577797204256 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 8.1921 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[18/300] At -2.0 dB, Train Loss: 0.10536490380764008 Train BER 0.004443243145942688,                  \n",
      " [18/300] At 0.0 dB, Train Loss: 0.00247612944804132 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.2074 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[19/300] At -2.0 dB, Train Loss: 0.0863884910941124 Train BER 0.0037135134916752577,                  \n",
      " [19/300] At 0.0 dB, Train Loss: 0.002720597432926297 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.1707 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[20/300] At -2.0 dB, Train Loss: 0.08233023434877396 Train BER 0.0033783784601837397,                  \n",
      " [20/300] At 0.0 dB, Train Loss: 0.001536159892566502 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.1451 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[21/300] At -2.0 dB, Train Loss: 0.07557497918605804 Train BER 0.003043243195861578,                  \n",
      " [21/300] At 0.0 dB, Train Loss: 0.0012040531728416681 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.1560 minutes\n",
      "encoder learning rate: 4.20e-04, decoder learning rate: 4.20e-04\n",
      "[22/300] At -2.0 dB, Train Loss: 0.07285255938768387 Train BER 0.0030594593845307827,                  \n",
      " [22/300] At 0.0 dB, Train Loss: 0.0013316249242052436 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.1117 minutes\n",
      "encoder learning rate: 4.40e-04, decoder learning rate: 4.40e-04\n",
      "[23/300] At -2.0 dB, Train Loss: 0.0627695620059967 Train BER 0.002654053969308734,                  \n",
      " [23/300] At 0.0 dB, Train Loss: 0.0008047003066167235 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.1782 minutes\n",
      "encoder learning rate: 4.60e-04, decoder learning rate: 4.60e-04\n",
      "[24/300] At -2.0 dB, Train Loss: 0.07450773566961288 Train BER 0.00314054056070745,                  \n",
      " [24/300] At 0.0 dB, Train Loss: 0.0018551519606262445 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.1778 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[25/300] At -2.0 dB, Train Loss: 0.06848228722810745 Train BER 0.0031081081833690405,                  \n",
      " [25/300] At 0.0 dB, Train Loss: 0.0010938902851194143 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.1712 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[26/300] At -2.0 dB, Train Loss: 0.06495430320501328 Train BER 0.0031297297682613134,                  \n",
      " [26/300] At 0.0 dB, Train Loss: 0.0009569712565280497 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.1549 minutes\n",
      "encoder learning rate: 5.20e-04, decoder learning rate: 5.20e-04\n",
      "[27/300] At -2.0 dB, Train Loss: 0.0645587295293808 Train BER 0.003000000026077032,                  \n",
      " [27/300] At 0.0 dB, Train Loss: 0.0013095686445012689 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.1082 minutes\n",
      "encoder learning rate: 5.40e-04, decoder learning rate: 5.40e-04\n",
      "[28/300] At -2.0 dB, Train Loss: 0.0656597688794136 Train BER 0.0029135134536772966,                  \n",
      " [28/300] At 0.0 dB, Train Loss: 0.0008762830402702093 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.1715 minutes\n",
      "encoder learning rate: 5.60e-04, decoder learning rate: 5.60e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/300] At -2.0 dB, Train Loss: 0.05322422832250595 Train BER 0.0021405406296253204,                  \n",
      " [29/300] At 0.0 dB, Train Loss: 0.0007292599184438586 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.2002 minutes\n",
      "encoder learning rate: 5.80e-04, decoder learning rate: 5.80e-04\n",
      "[30/300] At -2.0 dB, Train Loss: 0.06499107927083969 Train BER 0.002848648699000478,                  \n",
      " [30/300] At 0.0 dB, Train Loss: 0.0005542723229154944 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.1932 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[31/300] At -2.0 dB, Train Loss: 0.058745335787534714 Train BER 0.002632432384416461,                  \n",
      " [31/300] At 0.0 dB, Train Loss: 0.000610561459325254 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.1841 minutes\n",
      "encoder learning rate: 6.20e-04, decoder learning rate: 6.20e-04\n",
      "[32/300] At -2.0 dB, Train Loss: 0.0560605488717556 Train BER 0.0023729729000478983,                  \n",
      " [32/300] At 0.0 dB, Train Loss: 0.0006562107591889799 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.1823 minutes\n",
      "encoder learning rate: 6.40e-04, decoder learning rate: 6.40e-04\n",
      "[33/300] At -2.0 dB, Train Loss: 0.05128375440835953 Train BER 0.0019621620886027813,                  \n",
      " [33/300] At 0.0 dB, Train Loss: 0.001407806295901537 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.1683 minutes\n",
      "encoder learning rate: 6.60e-04, decoder learning rate: 6.60e-04\n",
      "[34/300] At -2.0 dB, Train Loss: 0.04777396097779274 Train BER 0.001989189302548766,                  \n",
      " [34/300] At 0.0 dB, Train Loss: 0.0008341700304299593 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.1456 minutes\n",
      "encoder learning rate: 6.80e-04, decoder learning rate: 6.80e-04\n",
      "[35/300] At -2.0 dB, Train Loss: 0.05255281925201416 Train BER 0.002345945918932557,                  \n",
      " [35/300] At 0.0 dB, Train Loss: 0.000812103389762342 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.0861 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n",
      "[36/300] At -2.0 dB, Train Loss: 0.05379379168152809 Train BER 0.0023297297302633524,                  \n",
      " [36/300] At 0.0 dB, Train Loss: 0.0005891709006391466 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.1619 minutes\n",
      "encoder learning rate: 7.20e-04, decoder learning rate: 7.20e-04\n",
      "[37/300] At -2.0 dB, Train Loss: 0.04401093348860741 Train BER 0.001767567591741681,                  \n",
      " [37/300] At 0.0 dB, Train Loss: 0.0006114736315794289 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.1850 minutes\n",
      "encoder learning rate: 7.40e-04, decoder learning rate: 7.40e-04\n",
      "[38/300] At -2.0 dB, Train Loss: 0.05566159635782242 Train BER 0.0023729729000478983,                  \n",
      " [38/300] At 0.0 dB, Train Loss: 0.0009369266917929053 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.1423 minutes\n",
      "encoder learning rate: 7.60e-04, decoder learning rate: 7.60e-04\n",
      "[39/300] At -2.0 dB, Train Loss: 0.04586372897028923 Train BER 0.0016324324533343315,                  \n",
      " [39/300] At 0.0 dB, Train Loss: 0.0004341586318332702 Train BER 0.0\n",
      "Time for one full iteration is 8.1128 minutes\n",
      "encoder learning rate: 7.80e-04, decoder learning rate: 7.80e-04\n",
      "[40/300] At -2.0 dB, Train Loss: 0.05357956141233444 Train BER 0.0024324324913322926,                  \n",
      " [40/300] At 0.0 dB, Train Loss: 0.0018615848384797573 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.1326 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[41/300] At -2.0 dB, Train Loss: 0.05118411034345627 Train BER 0.002302702749148011,                  \n",
      " [41/300] At 0.0 dB, Train Loss: 0.0009620123310014606 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.0734 minutes\n",
      "encoder learning rate: 8.20e-04, decoder learning rate: 8.20e-04\n",
      "[42/300] At -2.0 dB, Train Loss: 0.05000267177820206 Train BER 0.002248648554086685,                  \n",
      " [42/300] At 0.0 dB, Train Loss: 0.001082774717360735 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.2090 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n",
      "[43/300] At -2.0 dB, Train Loss: 0.05341348797082901 Train BER 0.0023513513151556253,                  \n",
      " [43/300] At 0.0 dB, Train Loss: 0.0011071162298321724 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.1779 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[44/300] At -2.0 dB, Train Loss: 0.050444286316633224 Train BER 0.0022918919567018747,                  \n",
      " [44/300] At 0.0 dB, Train Loss: 0.0016359714791178703 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.2007 minutes\n",
      "encoder learning rate: 8.80e-04, decoder learning rate: 8.80e-04\n",
      "[45/300] At -2.0 dB, Train Loss: 0.06550680100917816 Train BER 0.003021621610969305,                  \n",
      " [45/300] At 0.0 dB, Train Loss: 0.0021234084852039814 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.1724 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[46/300] At -2.0 dB, Train Loss: 0.06252222508192062 Train BER 0.002772972919046879,                  \n",
      " [46/300] At 0.0 dB, Train Loss: 0.0011706090299412608 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.2159 minutes\n",
      "encoder learning rate: 9.20e-04, decoder learning rate: 9.20e-04\n",
      "[47/300] At -2.0 dB, Train Loss: 0.06033305078744888 Train BER 0.0028054052963852882,                  \n",
      " [47/300] At 0.0 dB, Train Loss: 0.001574613037519157 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.2470 minutes\n",
      "encoder learning rate: 9.40e-04, decoder learning rate: 9.40e-04\n",
      "[48/300] At -2.0 dB, Train Loss: 0.05472138151526451 Train BER 0.002486486453562975,                  \n",
      " [48/300] At 0.0 dB, Train Loss: 0.0008273328421637416 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.2088 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[49/300] At -2.0 dB, Train Loss: 0.04707514867186546 Train BER 0.002032432472333312,                  \n",
      " [49/300] At 0.0 dB, Train Loss: 0.0061223870143294334 Train BER 0.00024864866281859577\n",
      "Time for one full iteration is 8.1850 minutes\n",
      "encoder learning rate: 9.80e-04, decoder learning rate: 9.80e-04\n",
      "[50/300] At -2.0 dB, Train Loss: 0.055285099893808365 Train BER 0.0024270270951092243,                  \n",
      " [50/300] At 0.0 dB, Train Loss: 0.0048265838995575905 Train BER 0.00018918918794952333\n",
      "Time for one full iteration is 8.1862 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[51/300] At -2.0 dB, Train Loss: 0.09021802246570587 Train BER 0.0037837838754057884,                  \n",
      " [51/300] At 0.0 dB, Train Loss: 0.05942525342106819 Train BER 0.0021621622145175934\n",
      "Time for one full iteration is 8.1843 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[52/300] At -2.0 dB, Train Loss: 0.20775340497493744 Train BER 0.008729729801416397,                  \n",
      " [52/300] At 0.0 dB, Train Loss: 0.2192830592393875 Train BER 0.009416216053068638\n",
      "Time for one full iteration is 8.2515 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[53/300] At -2.0 dB, Train Loss: 0.4184773862361908 Train BER 0.019864864647388458,                  \n",
      " [53/300] At 0.0 dB, Train Loss: 0.2582665681838989 Train BER 0.010713513940572739\n",
      "Time for one full iteration is 8.9551 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[54/300] At -2.0 dB, Train Loss: 0.419075071811676 Train BER 0.01878378354012966,                  \n",
      " [54/300] At 0.0 dB, Train Loss: 0.2135230451822281 Train BER 0.00869189202785492\n",
      "Time for one full iteration is 9.0878 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[55/300] At -2.0 dB, Train Loss: 0.46154290437698364 Train BER 0.022162161767482758,                  \n",
      " [55/300] At 0.0 dB, Train Loss: 0.35033541917800903 Train BER 0.015805404633283615\n",
      "Time for one full iteration is 9.0540 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[56/300] At -2.0 dB, Train Loss: 0.44362008571624756 Train BER 0.02127026952803135,                  \n",
      " [56/300] At 0.0 dB, Train Loss: 0.3423317074775696 Train BER 0.015005405060946941\n",
      "Time for one full iteration is 9.0366 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/300] At -2.0 dB, Train Loss: 0.44582653045654297 Train BER 0.021675676107406616,                  \n",
      " [57/300] At 0.0 dB, Train Loss: 0.3626437485218048 Train BER 0.016291892156004906\n",
      "Time for one full iteration is 8.9108 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[58/300] At -2.0 dB, Train Loss: 0.4240967929363251 Train BER 0.019232433289289474,                  \n",
      " [58/300] At 0.0 dB, Train Loss: 0.32629284262657166 Train BER 0.01396756712347269\n",
      "Time for one full iteration is 8.9146 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[59/300] At -2.0 dB, Train Loss: 0.38176655769348145 Train BER 0.017859458923339844,                  \n",
      " [59/300] At 0.0 dB, Train Loss: 0.3148677349090576 Train BER 0.0142000000923872\n",
      "Time for one full iteration is 8.7929 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[60/300] At -2.0 dB, Train Loss: 0.4079778790473938 Train BER 0.019086485728621483,                  \n",
      " [60/300] At 0.0 dB, Train Loss: 0.32903072237968445 Train BER 0.012335134670138359\n",
      "Time for one full iteration is 8.7902 minutes\n",
      "encoder learning rate: 9.96e-04, decoder learning rate: 9.96e-04\n",
      "[61/300] At -2.0 dB, Train Loss: 0.2834404706954956 Train BER 0.013075675815343857,                  \n",
      " [61/300] At 0.0 dB, Train Loss: 0.09741721302270889 Train BER 0.004129729699343443\n",
      "Time for one full iteration is 8.7633 minutes\n",
      "encoder learning rate: 9.95e-04, decoder learning rate: 9.95e-04\n",
      "[62/300] At -2.0 dB, Train Loss: 0.2890629172325134 Train BER 0.014270270243287086,                  \n",
      " [62/300] At 0.0 dB, Train Loss: 0.3226800262928009 Train BER 0.007594594731926918\n",
      "Time for one full iteration is 8.8140 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[63/300] At -2.0 dB, Train Loss: 0.1849377304315567 Train BER 0.008821621537208557,                  \n",
      " [63/300] At 0.0 dB, Train Loss: 0.009230075404047966 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.8191 minutes\n",
      "encoder learning rate: 9.93e-04, decoder learning rate: 9.93e-04\n",
      "[64/300] At -2.0 dB, Train Loss: 0.13895602524280548 Train BER 0.006962161976844072,                  \n",
      " [64/300] At 0.0 dB, Train Loss: 0.004750925116240978 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 8.8120 minutes\n",
      "encoder learning rate: 9.92e-04, decoder learning rate: 9.92e-04\n",
      "[65/300] At -2.0 dB, Train Loss: 0.07785827666521072 Train BER 0.003697297303006053,                  \n",
      " [65/300] At 0.0 dB, Train Loss: 0.0011298997560516 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.9758 minutes\n",
      "encoder learning rate: 9.91e-04, decoder learning rate: 9.91e-04\n",
      "[66/300] At -2.0 dB, Train Loss: 0.08553986251354218 Train BER 0.004102702718228102,                  \n",
      " [66/300] At 0.0 dB, Train Loss: 0.0028088330291211605 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 9.0368 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[67/300] At -2.0 dB, Train Loss: 0.05600876733660698 Train BER 0.0026702701579779387,                  \n",
      " [67/300] At 0.0 dB, Train Loss: 0.000653332332149148 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7893 minutes\n",
      "encoder learning rate: 9.89e-04, decoder learning rate: 9.89e-04\n",
      "[68/300] At -2.0 dB, Train Loss: 0.05836820974946022 Train BER 0.002751351334154606,                  \n",
      " [68/300] At 0.0 dB, Train Loss: 0.0007648598402738571 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8021 minutes\n",
      "encoder learning rate: 9.87e-04, decoder learning rate: 9.87e-04\n",
      "[69/300] At -2.0 dB, Train Loss: 0.053500741720199585 Train BER 0.002400000113993883,                  \n",
      " [69/300] At 0.0 dB, Train Loss: 0.0004978668875992298 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8129 minutes\n",
      "encoder learning rate: 9.86e-04, decoder learning rate: 9.86e-04\n",
      "[70/300] At -2.0 dB, Train Loss: 0.055395688861608505 Train BER 0.002443243283778429,                  \n",
      " [70/300] At 0.0 dB, Train Loss: 0.0005561519647017121 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.8125 minutes\n",
      "encoder learning rate: 9.84e-04, decoder learning rate: 9.84e-04\n",
      "[71/300] At -2.0 dB, Train Loss: 0.05384661629796028 Train BER 0.002583783818408847,                  \n",
      " [71/300] At 0.0 dB, Train Loss: 0.0007605324499309063 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.8653 minutes\n",
      "encoder learning rate: 9.83e-04, decoder learning rate: 9.83e-04\n",
      "[72/300] At -2.0 dB, Train Loss: 0.049894992262125015 Train BER 0.002243243157863617,                  \n",
      " [72/300] At 0.0 dB, Train Loss: 0.00040045997593551874 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0692 minutes\n",
      "encoder learning rate: 9.81e-04, decoder learning rate: 9.81e-04\n",
      "[73/300] At -2.0 dB, Train Loss: 0.055178772658109665 Train BER 0.0029135134536772966,                  \n",
      " [73/300] At 0.0 dB, Train Loss: 0.001029210863634944 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 9.0532 minutes\n",
      "encoder learning rate: 9.79e-04, decoder learning rate: 9.79e-04\n",
      "[74/300] At -2.0 dB, Train Loss: 0.06739214062690735 Train BER 0.0033729730639606714,                  \n",
      " [74/300] At 0.0 dB, Train Loss: 0.0005923827411606908 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0532 minutes\n",
      "encoder learning rate: 9.77e-04, decoder learning rate: 9.77e-04\n",
      "[75/300] At -2.0 dB, Train Loss: 0.06139505282044411 Train BER 0.0030756755731999874,                  \n",
      " [75/300] At 0.0 dB, Train Loss: 0.0005663257325068116 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0406 minutes\n",
      "encoder learning rate: 9.76e-04, decoder learning rate: 9.76e-04\n",
      "[76/300] At -2.0 dB, Train Loss: 0.0518597774207592 Train BER 0.002464864868670702,                  \n",
      " [76/300] At 0.0 dB, Train Loss: 0.0005723083741031587 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0801 minutes\n",
      "encoder learning rate: 9.74e-04, decoder learning rate: 9.74e-04\n",
      "[77/300] At -2.0 dB, Train Loss: 0.046335093677043915 Train BER 0.00236756750382483,                  \n",
      " [77/300] At 0.0 dB, Train Loss: 0.001181064173579216 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 9.0542 minutes\n",
      "encoder learning rate: 9.72e-04, decoder learning rate: 9.72e-04\n",
      "[78/300] At -2.0 dB, Train Loss: 0.04453623667359352 Train BER 0.002156756818294525,                  \n",
      " [78/300] At 0.0 dB, Train Loss: 0.0003451452648732811 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0608 minutes\n",
      "encoder learning rate: 9.69e-04, decoder learning rate: 9.69e-04\n",
      "[79/300] At -2.0 dB, Train Loss: 0.04395034536719322 Train BER 0.0020000000949949026,                  \n",
      " [79/300] At 0.0 dB, Train Loss: 0.0002435719216009602 Train BER 0.0\n",
      "Time for one full iteration is 9.0537 minutes\n",
      "encoder learning rate: 9.67e-04, decoder learning rate: 9.67e-04\n",
      "[80/300] At -2.0 dB, Train Loss: 0.04605654627084732 Train BER 0.0022054053843021393,                  \n",
      " [80/300] At 0.0 dB, Train Loss: 0.00029885972617194057 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0766 minutes\n",
      "encoder learning rate: 9.65e-04, decoder learning rate: 9.65e-04\n",
      "[81/300] At -2.0 dB, Train Loss: 0.039194539189338684 Train BER 0.0018432432552799582,                  \n",
      " [81/300] At 0.0 dB, Train Loss: 0.000226898308028467 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.1004 minutes\n",
      "encoder learning rate: 9.63e-04, decoder learning rate: 9.63e-04\n",
      "[82/300] At -2.0 dB, Train Loss: 0.03351205587387085 Train BER 0.001470270217396319,                  \n",
      " [82/300] At 0.0 dB, Train Loss: 0.0003020900476258248 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0153 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[83/300] At -2.0 dB, Train Loss: 0.05735503509640694 Train BER 0.002751351334154606,                  \n",
      " [83/300] At 0.0 dB, Train Loss: 0.0005351250874809921 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0842 minutes\n",
      "encoder learning rate: 9.58e-04, decoder learning rate: 9.58e-04\n",
      "[84/300] At -2.0 dB, Train Loss: 0.04688279330730438 Train BER 0.002081081038340926,                  \n",
      " [84/300] At 0.0 dB, Train Loss: 0.00016221222176682204 Train BER 0.0\n",
      "Time for one full iteration is 9.0634 minutes\n",
      "encoder learning rate: 9.55e-04, decoder learning rate: 9.55e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/300] At -2.0 dB, Train Loss: 0.040009498596191406 Train BER 0.0018972973339259624,                  \n",
      " [85/300] At 0.0 dB, Train Loss: 0.0014702746411785483 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 9.0473 minutes\n",
      "encoder learning rate: 9.52e-04, decoder learning rate: 9.52e-04\n",
      "[86/300] At -2.0 dB, Train Loss: 0.054492492228746414 Train BER 0.002545946044847369,                  \n",
      " [86/300] At 0.0 dB, Train Loss: 0.0001843673671828583 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0599 minutes\n",
      "encoder learning rate: 9.50e-04, decoder learning rate: 9.50e-04\n",
      "[87/300] At -2.0 dB, Train Loss: 0.039133328944444656 Train BER 0.0019189189188182354,                  \n",
      " [87/300] At 0.0 dB, Train Loss: 0.00013651304470840842 Train BER 0.0\n",
      "Time for one full iteration is 9.0801 minutes\n",
      "encoder learning rate: 9.47e-04, decoder learning rate: 9.47e-04\n",
      "[88/300] At -2.0 dB, Train Loss: 0.03718999773263931 Train BER 0.0016972973244264722,                  \n",
      " [88/300] At 0.0 dB, Train Loss: 0.00017157864931505173 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0600 minutes\n",
      "encoder learning rate: 9.44e-04, decoder learning rate: 9.44e-04\n",
      "[89/300] At -2.0 dB, Train Loss: 0.030878804624080658 Train BER 0.0013243242865428329,                  \n",
      " [89/300] At 0.0 dB, Train Loss: 0.00011817341874120757 Train BER 0.0\n",
      "Time for one full iteration is 9.0171 minutes\n",
      "encoder learning rate: 9.41e-04, decoder learning rate: 9.41e-04\n",
      "[90/300] At -2.0 dB, Train Loss: 0.04204469546675682 Train BER 0.0019945946987718344,                  \n",
      " [90/300] At 0.0 dB, Train Loss: 0.001858155126683414 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 9.0732 minutes\n",
      "encoder learning rate: 9.38e-04, decoder learning rate: 9.38e-04\n",
      "[91/300] At -2.0 dB, Train Loss: 0.03704345226287842 Train BER 0.0015837837709113955,                  \n",
      " [91/300] At 0.0 dB, Train Loss: 0.0007685808232054114 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 9.0431 minutes\n",
      "encoder learning rate: 9.35e-04, decoder learning rate: 9.35e-04\n",
      "[92/300] At -2.0 dB, Train Loss: 0.026041680946946144 Train BER 0.0011513513745740056,                  \n",
      " [92/300] At 0.0 dB, Train Loss: 0.000492118124384433 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0010 minutes\n",
      "encoder learning rate: 9.32e-04, decoder learning rate: 9.32e-04\n",
      "[93/300] At -2.0 dB, Train Loss: 0.028629271313548088 Train BER 0.0013459459878504276,                  \n",
      " [93/300] At 0.0 dB, Train Loss: 0.0003028892388101667 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0732 minutes\n",
      "encoder learning rate: 9.29e-04, decoder learning rate: 9.29e-04\n",
      "[94/300] At -2.0 dB, Train Loss: 0.04183468967676163 Train BER 0.0020378378685563803,                  \n",
      " [94/300] At 0.0 dB, Train Loss: 9.784421126823872e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0802 minutes\n",
      "encoder learning rate: 9.26e-04, decoder learning rate: 9.26e-04\n",
      "[95/300] At -2.0 dB, Train Loss: 0.038529492914676666 Train BER 0.0016594594344496727,                  \n",
      " [95/300] At 0.0 dB, Train Loss: 0.00012135523138567805 Train BER 0.0\n",
      "Time for one full iteration is 9.0672 minutes\n",
      "encoder learning rate: 9.22e-04, decoder learning rate: 9.22e-04\n",
      "[96/300] At -2.0 dB, Train Loss: 0.04397697001695633 Train BER 0.0021081080194562674,                  \n",
      " [96/300] At 0.0 dB, Train Loss: 0.00010083505912916735 Train BER 0.0\n",
      "Time for one full iteration is 9.0466 minutes\n",
      "encoder learning rate: 9.19e-04, decoder learning rate: 9.19e-04\n",
      "[97/300] At -2.0 dB, Train Loss: 0.03809646889567375 Train BER 0.0019459458999335766,                  \n",
      " [97/300] At 0.0 dB, Train Loss: 0.00016151605814229697 Train BER 0.0\n",
      "Time for one full iteration is 9.0432 minutes\n",
      "encoder learning rate: 9.15e-04, decoder learning rate: 9.15e-04\n",
      "[98/300] At -2.0 dB, Train Loss: 0.03398217260837555 Train BER 0.0014648648211732507,                  \n",
      " [98/300] At 0.0 dB, Train Loss: 8.79408253240399e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0305 minutes\n",
      "encoder learning rate: 9.12e-04, decoder learning rate: 9.12e-04\n",
      "[99/300] At -2.0 dB, Train Loss: 0.03830621764063835 Train BER 0.0018054053653031588,                  \n",
      " [99/300] At 0.0 dB, Train Loss: 8.130217611324042e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0456 minutes\n",
      "encoder learning rate: 9.08e-04, decoder learning rate: 9.08e-04\n",
      "[100/300] At -2.0 dB, Train Loss: 0.03753991425037384 Train BER 0.0017945945728570223,                  \n",
      " [100/300] At 0.0 dB, Train Loss: 0.00016417897131759673 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0118 minutes\n",
      "encoder learning rate: 9.05e-04, decoder learning rate: 9.05e-04\n",
      "[101/300] At -2.0 dB, Train Loss: 0.037335120141506195 Train BER 0.0016972973244264722,                  \n",
      " [101/300] At 0.0 dB, Train Loss: 9.916460840031505e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0406 minutes\n",
      "encoder learning rate: 9.01e-04, decoder learning rate: 9.01e-04\n",
      "[102/300] At -2.0 dB, Train Loss: 0.03540272265672684 Train BER 0.0015621621860191226,                  \n",
      " [102/300] At 0.0 dB, Train Loss: 0.00014766919775865972 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0472 minutes\n",
      "encoder learning rate: 8.97e-04, decoder learning rate: 8.97e-04\n",
      "[103/300] At -2.0 dB, Train Loss: 0.03648751229047775 Train BER 0.001810810761526227,                  \n",
      " [103/300] At 0.0 dB, Train Loss: 0.00014652847312390804 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0671 minutes\n",
      "encoder learning rate: 8.93e-04, decoder learning rate: 8.93e-04\n",
      "[104/300] At -2.0 dB, Train Loss: 0.03954637423157692 Train BER 0.0019027027301490307,                  \n",
      " [104/300] At 0.0 dB, Train Loss: 0.00150440470315516 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 9.0972 minutes\n",
      "encoder learning rate: 8.89e-04, decoder learning rate: 8.89e-04\n",
      "[105/300] At -2.0 dB, Train Loss: 0.041607994586229324 Train BER 0.002118918811902404,                  \n",
      " [105/300] At 0.0 dB, Train Loss: 0.00011010841262759641 Train BER 0.0\n",
      "Time for one full iteration is 9.0721 minutes\n",
      "encoder learning rate: 8.85e-04, decoder learning rate: 8.85e-04\n",
      "[106/300] At -2.0 dB, Train Loss: 0.02438921108841896 Train BER 0.0010756757110357285,                  \n",
      " [106/300] At 0.0 dB, Train Loss: 0.00010012237908085808 Train BER 0.0\n",
      "Time for one full iteration is 9.0303 minutes\n",
      "encoder learning rate: 8.81e-04, decoder learning rate: 8.81e-04\n",
      "[107/300] At -2.0 dB, Train Loss: 0.03340250998735428 Train BER 0.001713513513095677,                  \n",
      " [107/300] At 0.0 dB, Train Loss: 7.300583820324391e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0492 minutes\n",
      "encoder learning rate: 8.77e-04, decoder learning rate: 8.77e-04\n",
      "[108/300] At -2.0 dB, Train Loss: 0.029341040179133415 Train BER 0.0013081080978736281,                  \n",
      " [108/300] At 0.0 dB, Train Loss: 6.44500833004713e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9590 minutes\n",
      "encoder learning rate: 8.73e-04, decoder learning rate: 8.73e-04\n",
      "[109/300] At -2.0 dB, Train Loss: 0.036113690584897995 Train BER 0.0018270270666107535,                  \n",
      " [109/300] At 0.0 dB, Train Loss: 0.0001852772693382576 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9920 minutes\n",
      "encoder learning rate: 8.69e-04, decoder learning rate: 8.69e-04\n",
      "[110/300] At -2.0 dB, Train Loss: 0.03668439760804176 Train BER 0.001740540494211018,                  \n",
      " [110/300] At 0.0 dB, Train Loss: 0.00041099137160927057 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0797 minutes\n",
      "encoder learning rate: 8.65e-04, decoder learning rate: 8.65e-04\n",
      "[111/300] At -2.0 dB, Train Loss: 0.027036570012569427 Train BER 0.0013405405916273594,                  \n",
      " [111/300] At 0.0 dB, Train Loss: 0.000165287681738846 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0358 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[112/300] At -2.0 dB, Train Loss: 0.03757863491773605 Train BER 0.001767567591741681,                  \n",
      " [112/300] At 0.0 dB, Train Loss: 0.0003567981766536832 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0507 minutes\n",
      "encoder learning rate: 8.56e-04, decoder learning rate: 8.56e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/300] At -2.0 dB, Train Loss: 0.031645797193050385 Train BER 0.0014864865224808455,                  \n",
      " [113/300] At 0.0 dB, Train Loss: 6.121105252532288e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0754 minutes\n",
      "encoder learning rate: 8.51e-04, decoder learning rate: 8.51e-04\n",
      "[114/300] At -2.0 dB, Train Loss: 0.04278375580906868 Train BER 0.0021081080194562674,                  \n",
      " [114/300] At 0.0 dB, Train Loss: 0.00023595185484737158 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0631 minutes\n",
      "encoder learning rate: 8.47e-04, decoder learning rate: 8.47e-04\n",
      "[115/300] At -2.0 dB, Train Loss: 0.02854447439312935 Train BER 0.0013405405916273594,                  \n",
      " [115/300] At 0.0 dB, Train Loss: 0.0005112052895128727 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 9.0753 minutes\n",
      "encoder learning rate: 8.42e-04, decoder learning rate: 8.42e-04\n",
      "[116/300] At -2.0 dB, Train Loss: 0.041155047714710236 Train BER 0.002124324208125472,                  \n",
      " [116/300] At 0.0 dB, Train Loss: 0.00019332117517478764 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0292 minutes\n",
      "encoder learning rate: 8.38e-04, decoder learning rate: 8.38e-04\n",
      "[117/300] At -2.0 dB, Train Loss: 0.03036544844508171 Train BER 0.0013513513840734959,                  \n",
      " [117/300] At 0.0 dB, Train Loss: 4.8063320718938485e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0634 minutes\n",
      "encoder learning rate: 8.33e-04, decoder learning rate: 8.33e-04\n",
      "[118/300] At -2.0 dB, Train Loss: 0.027200045064091682 Train BER 0.0013243242865428329,                  \n",
      " [118/300] At 0.0 dB, Train Loss: 0.00012229241838213056 Train BER 0.0\n",
      "Time for one full iteration is 9.0819 minutes\n",
      "encoder learning rate: 8.28e-04, decoder learning rate: 8.28e-04\n",
      "[119/300] At -2.0 dB, Train Loss: 0.0459035262465477 Train BER 0.0022540539503097534,                  \n",
      " [119/300] At 0.0 dB, Train Loss: 0.0001664379087742418 Train BER 0.0\n",
      "Time for one full iteration is 9.0527 minutes\n",
      "encoder learning rate: 8.24e-04, decoder learning rate: 8.24e-04\n",
      "[120/300] At -2.0 dB, Train Loss: 0.034160323441028595 Train BER 0.0017297297017648816,                  \n",
      " [120/300] At 0.0 dB, Train Loss: 0.00014871024177409708 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0620 minutes\n",
      "encoder learning rate: 8.19e-04, decoder learning rate: 8.19e-04\n",
      "[121/300] At -2.0 dB, Train Loss: 0.03018352761864662 Train BER 0.001378378365188837,                  \n",
      " [121/300] At 0.0 dB, Train Loss: 0.00015530445671174675 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0829 minutes\n",
      "encoder learning rate: 8.14e-04, decoder learning rate: 8.14e-04\n",
      "[122/300] At -2.0 dB, Train Loss: 0.02718936651945114 Train BER 0.0012378378305584192,                  \n",
      " [122/300] At 0.0 dB, Train Loss: 0.0002781640214379877 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0480 minutes\n",
      "encoder learning rate: 8.09e-04, decoder learning rate: 8.09e-04\n",
      "[123/300] At -2.0 dB, Train Loss: 0.02661215141415596 Train BER 0.001135135185904801,                  \n",
      " [123/300] At 0.0 dB, Train Loss: 8.213253750000149e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0110 minutes\n",
      "encoder learning rate: 8.04e-04, decoder learning rate: 8.04e-04\n",
      "[124/300] At -2.0 dB, Train Loss: 0.03344496339559555 Train BER 0.0016162162646651268,                  \n",
      " [124/300] At 0.0 dB, Train Loss: 0.00017259889864362776 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9822 minutes\n",
      "encoder learning rate: 7.99e-04, decoder learning rate: 7.99e-04\n",
      "[125/300] At -2.0 dB, Train Loss: 0.03222905844449997 Train BER 0.0015621621860191226,                  \n",
      " [125/300] At 0.0 dB, Train Loss: 0.00023742315534036607 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9900 minutes\n",
      "encoder learning rate: 7.94e-04, decoder learning rate: 7.94e-04\n",
      "[126/300] At -2.0 dB, Train Loss: 0.027353761717677116 Train BER 0.001232432434335351,                  \n",
      " [126/300] At 0.0 dB, Train Loss: 0.00014914404891896993 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9755 minutes\n",
      "encoder learning rate: 7.89e-04, decoder learning rate: 7.89e-04\n",
      "[127/300] At -2.0 dB, Train Loss: 0.03407192975282669 Train BER 0.0016162162646651268,                  \n",
      " [127/300] At 0.0 dB, Train Loss: 0.00022289705520961434 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9419 minutes\n",
      "encoder learning rate: 7.84e-04, decoder learning rate: 7.84e-04\n",
      "[128/300] At -2.0 dB, Train Loss: 0.02937196008861065 Train BER 0.001378378365188837,                  \n",
      " [128/300] At 0.0 dB, Train Loss: 0.000178124537342228 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9942 minutes\n",
      "encoder learning rate: 7.79e-04, decoder learning rate: 7.79e-04\n",
      "[129/300] At -2.0 dB, Train Loss: 0.03122679330408573 Train BER 0.0014378378400579095,                  \n",
      " [129/300] At 0.0 dB, Train Loss: 7.604974962305278e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9896 minutes\n",
      "encoder learning rate: 7.73e-04, decoder learning rate: 7.73e-04\n",
      "[130/300] At -2.0 dB, Train Loss: 0.03307582437992096 Train BER 0.001470270217396319,                  \n",
      " [130/300] At 0.0 dB, Train Loss: 6.306960131041706e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9742 minutes\n",
      "encoder learning rate: 7.68e-04, decoder learning rate: 7.68e-04\n",
      "[131/300] At -2.0 dB, Train Loss: 0.029841246083378792 Train BER 0.0016162162646651268,                  \n",
      " [131/300] At 0.0 dB, Train Loss: 6.648334237979725e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9886 minutes\n",
      "encoder learning rate: 7.63e-04, decoder learning rate: 7.63e-04\n",
      "[132/300] At -2.0 dB, Train Loss: 0.02741134539246559 Train BER 0.0012000000569969416,                  \n",
      " [132/300] At 0.0 dB, Train Loss: 8.867042924975976e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0475 minutes\n",
      "encoder learning rate: 7.57e-04, decoder learning rate: 7.57e-04\n",
      "[133/300] At -2.0 dB, Train Loss: 0.03735194355249405 Train BER 0.0017459458904340863,                  \n",
      " [133/300] At 0.0 dB, Train Loss: 0.00020445096015464514 Train BER 0.0\n",
      "Time for one full iteration is 8.8881 minutes\n",
      "encoder learning rate: 7.52e-04, decoder learning rate: 7.52e-04\n",
      "[134/300] At -2.0 dB, Train Loss: 0.024679450318217278 Train BER 0.0011027026921510696,                  \n",
      " [134/300] At 0.0 dB, Train Loss: 8.334606536664069e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9978 minutes\n",
      "encoder learning rate: 7.47e-04, decoder learning rate: 7.47e-04\n",
      "[135/300] At -2.0 dB, Train Loss: 0.02830689586699009 Train BER 0.0015243242960423231,                  \n",
      " [135/300] At 0.0 dB, Train Loss: 0.00017588688933756202 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9315 minutes\n",
      "encoder learning rate: 7.41e-04, decoder learning rate: 7.41e-04\n",
      "[136/300] At -2.0 dB, Train Loss: 0.03398817405104637 Train BER 0.0015513513935729861,                  \n",
      " [136/300] At 0.0 dB, Train Loss: 0.00028871907852590084 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.6730 minutes\n",
      "encoder learning rate: 7.36e-04, decoder learning rate: 7.36e-04\n",
      "[137/300] At -2.0 dB, Train Loss: 0.03819458931684494 Train BER 0.0018540540477260947,                  \n",
      " [137/300] At 0.0 dB, Train Loss: 0.00012217761832289398 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6675 minutes\n",
      "encoder learning rate: 7.30e-04, decoder learning rate: 7.30e-04\n",
      "[138/300] At -2.0 dB, Train Loss: 0.03303174674510956 Train BER 0.001643243245780468,                  \n",
      " [138/300] At 0.0 dB, Train Loss: 3.4875374694820493e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6687 minutes\n",
      "encoder learning rate: 7.24e-04, decoder learning rate: 7.24e-04\n",
      "[139/300] At -2.0 dB, Train Loss: 0.023095151409506798 Train BER 0.0011243242770433426,                  \n",
      " [139/300] At 0.0 dB, Train Loss: 0.00013788104115519673 Train BER 0.0\n",
      "Time for one full iteration is 8.7486 minutes\n",
      "encoder learning rate: 7.19e-04, decoder learning rate: 7.19e-04\n",
      "[140/300] At -2.0 dB, Train Loss: 0.03073040582239628 Train BER 0.0015891891671344638,                  \n",
      " [140/300] At 0.0 dB, Train Loss: 6.730478344252333e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7476 minutes\n",
      "encoder learning rate: 7.13e-04, decoder learning rate: 7.13e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/300] At -2.0 dB, Train Loss: 0.030393607914447784 Train BER 0.0013621621765196323,                  \n",
      " [141/300] At 0.0 dB, Train Loss: 0.00012502640311140567 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7368 minutes\n",
      "encoder learning rate: 7.07e-04, decoder learning rate: 7.07e-04\n",
      "[142/300] At -2.0 dB, Train Loss: 0.030519859865307808 Train BER 0.0014648648211732507,                  \n",
      " [142/300] At 0.0 dB, Train Loss: 6.171713175717741e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7360 minutes\n",
      "encoder learning rate: 7.02e-04, decoder learning rate: 7.02e-04\n",
      "[143/300] At -2.0 dB, Train Loss: 0.025550415739417076 Train BER 0.001183783751912415,                  \n",
      " [143/300] At 0.0 dB, Train Loss: 9.412764484295622e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0729 minutes\n",
      "encoder learning rate: 6.96e-04, decoder learning rate: 6.96e-04\n",
      "[144/300] At -2.0 dB, Train Loss: 0.027429521083831787 Train BER 0.0012972973054274917,                  \n",
      " [144/300] At 0.0 dB, Train Loss: 7.495205500163138e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0286 minutes\n",
      "encoder learning rate: 6.90e-04, decoder learning rate: 6.90e-04\n",
      "[145/300] At -2.0 dB, Train Loss: 0.028288157656788826 Train BER 0.0014540540287271142,                  \n",
      " [145/300] At 0.0 dB, Train Loss: 5.795118704554625e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0589 minutes\n",
      "encoder learning rate: 6.84e-04, decoder learning rate: 6.84e-04\n",
      "[146/300] At -2.0 dB, Train Loss: 0.027140717953443527 Train BER 0.0011891891481354833,                  \n",
      " [146/300] At 0.0 dB, Train Loss: 4.477213224163279e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1733 minutes\n",
      "encoder learning rate: 6.79e-04, decoder learning rate: 6.79e-04\n",
      "[147/300] At -2.0 dB, Train Loss: 0.03025655634701252 Train BER 0.001448648632504046,                  \n",
      " [147/300] At 0.0 dB, Train Loss: 0.0003942830371670425 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0282 minutes\n",
      "encoder learning rate: 6.73e-04, decoder learning rate: 6.73e-04\n",
      "[148/300] At -2.0 dB, Train Loss: 0.027451366186141968 Train BER 0.0012972973054274917,                  \n",
      " [148/300] At 0.0 dB, Train Loss: 3.866146653308533e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0457 minutes\n",
      "encoder learning rate: 6.67e-04, decoder learning rate: 6.67e-04\n",
      "[149/300] At -2.0 dB, Train Loss: 0.03221193328499794 Train BER 0.0014378378400579095,                  \n",
      " [149/300] At 0.0 dB, Train Loss: 0.0003547349770087749 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9991 minutes\n",
      "encoder learning rate: 6.61e-04, decoder learning rate: 6.61e-04\n",
      "[150/300] At -2.0 dB, Train Loss: 0.026548204943537712 Train BER 0.0013189188903197646,                  \n",
      " [150/300] At 0.0 dB, Train Loss: 9.369750478072092e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1406 minutes\n",
      "encoder learning rate: 6.55e-04, decoder learning rate: 6.55e-04\n",
      "[151/300] At -2.0 dB, Train Loss: 0.02906307764351368 Train BER 0.0015459459973499179,                  \n",
      " [151/300] At 0.0 dB, Train Loss: 7.282000296982005e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1094 minutes\n",
      "encoder learning rate: 6.49e-04, decoder learning rate: 6.49e-04\n",
      "[152/300] At -2.0 dB, Train Loss: 0.04088712856173515 Train BER 0.0020378378685563803,                  \n",
      " [152/300] At 0.0 dB, Train Loss: 0.0009438804117962718 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.7921 minutes\n",
      "encoder learning rate: 6.43e-04, decoder learning rate: 6.43e-04\n",
      "[153/300] At -2.0 dB, Train Loss: 0.02841498702764511 Train BER 0.0013891891576349735,                  \n",
      " [153/300] At 0.0 dB, Train Loss: 6.89521839376539e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4406 minutes\n",
      "encoder learning rate: 6.37e-04, decoder learning rate: 6.37e-04\n",
      "[154/300] At -2.0 dB, Train Loss: 0.02444108948111534 Train BER 0.0011729729594662786,                  \n",
      " [154/300] At 0.0 dB, Train Loss: 0.0001804789644666016 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4391 minutes\n",
      "encoder learning rate: 6.31e-04, decoder learning rate: 6.31e-04\n",
      "[155/300] At -2.0 dB, Train Loss: 0.02943076379597187 Train BER 0.0013081080978736281,                  \n",
      " [155/300] At 0.0 dB, Train Loss: 7.187257870100439e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4271 minutes\n",
      "encoder learning rate: 6.25e-04, decoder learning rate: 6.25e-04\n",
      "[156/300] At -2.0 dB, Train Loss: 0.02565447799861431 Train BER 0.0010972972959280014,                  \n",
      " [156/300] At 0.0 dB, Train Loss: 3.943050978705287e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4456 minutes\n",
      "encoder learning rate: 6.19e-04, decoder learning rate: 6.19e-04\n",
      "[157/300] At -2.0 dB, Train Loss: 0.02833184227347374 Train BER 0.00139999995008111,                  \n",
      " [157/300] At 0.0 dB, Train Loss: 4.431962224771269e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4260 minutes\n",
      "encoder learning rate: 6.13e-04, decoder learning rate: 6.13e-04\n",
      "[158/300] At -2.0 dB, Train Loss: 0.024478111416101456 Train BER 0.0011729729594662786,                  \n",
      " [158/300] At 0.0 dB, Train Loss: 0.0001219174955622293 Train BER 0.0\n",
      "Time for one full iteration is 8.4719 minutes\n",
      "encoder learning rate: 6.06e-04, decoder learning rate: 6.06e-04\n",
      "[159/300] At -2.0 dB, Train Loss: 0.023439845070242882 Train BER 0.0010216216323897243,                  \n",
      " [159/300] At 0.0 dB, Train Loss: 0.000104099242889788 Train BER 0.0\n",
      "Time for one full iteration is 8.4558 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[160/300] At -2.0 dB, Train Loss: 0.029018059372901917 Train BER 0.0015405404847115278,                  \n",
      " [160/300] At 0.0 dB, Train Loss: 6.627987022511661e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4405 minutes\n",
      "encoder learning rate: 5.94e-04, decoder learning rate: 5.94e-04\n",
      "[161/300] At -2.0 dB, Train Loss: 0.025195801630616188 Train BER 0.0012216216418892145,                  \n",
      " [161/300] At 0.0 dB, Train Loss: 4.9101468903245404e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4640 minutes\n",
      "encoder learning rate: 5.88e-04, decoder learning rate: 5.88e-04\n",
      "[162/300] At -2.0 dB, Train Loss: 0.03185824304819107 Train BER 0.0016486486420035362,                  \n",
      " [162/300] At 0.0 dB, Train Loss: 3.3846106816781685e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4800 minutes\n",
      "encoder learning rate: 5.82e-04, decoder learning rate: 5.82e-04\n",
      "[163/300] At -2.0 dB, Train Loss: 0.03520637005567551 Train BER 0.0015513513935729861,                  \n",
      " [163/300] At 0.0 dB, Train Loss: 2.324359775229823e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4492 minutes\n",
      "encoder learning rate: 5.76e-04, decoder learning rate: 5.76e-04\n",
      "[164/300] At -2.0 dB, Train Loss: 0.03018222749233246 Train BER 0.0014108108589425683,                  \n",
      " [164/300] At 0.0 dB, Train Loss: 5.046831938670948e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4371 minutes\n",
      "encoder learning rate: 5.69e-04, decoder learning rate: 5.69e-04\n",
      "[165/300] At -2.0 dB, Train Loss: 0.026573874056339264 Train BER 0.001329729682765901,                  \n",
      " [165/300] At 0.0 dB, Train Loss: 5.374974716687575e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5146 minutes\n",
      "encoder learning rate: 5.63e-04, decoder learning rate: 5.63e-04\n",
      "[166/300] At -2.0 dB, Train Loss: 0.02144721709191799 Train BER 0.0010486486135050654,                  \n",
      " [166/300] At 0.0 dB, Train Loss: 4.9969894462265074e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4307 minutes\n",
      "encoder learning rate: 5.57e-04, decoder learning rate: 5.57e-04\n",
      "[167/300] At -2.0 dB, Train Loss: 0.02147173136472702 Train BER 0.000994594651274383,                  \n",
      " [167/300] At 0.0 dB, Train Loss: 2.0288403902668506e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4381 minutes\n",
      "encoder learning rate: 5.51e-04, decoder learning rate: 5.51e-04\n",
      "[168/300] At -2.0 dB, Train Loss: 0.025951828807592392 Train BER 0.0011081080883741379,                  \n",
      " [168/300] At 0.0 dB, Train Loss: 2.8638394724112004e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4510 minutes\n",
      "encoder learning rate: 5.44e-04, decoder learning rate: 5.44e-04\n",
      "[169/300] At -2.0 dB, Train Loss: 0.028879337012767792 Train BER 0.001497297314926982,                  \n",
      " [169/300] At 0.0 dB, Train Loss: 3.858493801089935e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4417 minutes\n",
      "encoder learning rate: 5.38e-04, decoder learning rate: 5.38e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170/300] At -2.0 dB, Train Loss: 0.034435730427503586 Train BER 0.0016054053558036685,                  \n",
      " [170/300] At 0.0 dB, Train Loss: 5.893534398637712e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4719 minutes\n",
      "encoder learning rate: 5.32e-04, decoder learning rate: 5.32e-04\n",
      "[171/300] At -2.0 dB, Train Loss: 0.026049256324768066 Train BER 0.0012486486230045557,                  \n",
      " [171/300] At 0.0 dB, Train Loss: 3.532984192133881e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4460 minutes\n",
      "encoder learning rate: 5.26e-04, decoder learning rate: 5.26e-04\n",
      "[172/300] At -2.0 dB, Train Loss: 0.023514220491051674 Train BER 0.0010216216323897243,                  \n",
      " [172/300] At 0.0 dB, Train Loss: 4.037195321870968e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4360 minutes\n",
      "encoder learning rate: 5.19e-04, decoder learning rate: 5.19e-04\n",
      "[173/300] At -2.0 dB, Train Loss: 0.03474212810397148 Train BER 0.001740540494211018,                  \n",
      " [173/300] At 0.0 dB, Train Loss: 5.5023781897034496e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4531 minutes\n",
      "encoder learning rate: 5.13e-04, decoder learning rate: 5.13e-04\n",
      "[174/300] At -2.0 dB, Train Loss: 0.031013747677206993 Train BER 0.0014540540287271142,                  \n",
      " [174/300] At 0.0 dB, Train Loss: 0.0011219566222280264 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.4533 minutes\n",
      "encoder learning rate: 5.07e-04, decoder learning rate: 5.07e-04\n",
      "[175/300] At -2.0 dB, Train Loss: 0.0217927023768425 Train BER 0.0009729729499667883,                  \n",
      " [175/300] At 0.0 dB, Train Loss: 5.1522347348509356e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4612 minutes\n",
      "encoder learning rate: 5.01e-04, decoder learning rate: 5.01e-04\n",
      "[176/300] At -2.0 dB, Train Loss: 0.031237337738275528 Train BER 0.0015459459973499179,                  \n",
      " [176/300] At 0.0 dB, Train Loss: 4.961711965734139e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4545 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[177/300] At -2.0 dB, Train Loss: 0.0357707180082798 Train BER 0.001881081028841436,                  \n",
      " [177/300] At 0.0 dB, Train Loss: 8.831030572764575e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4624 minutes\n",
      "encoder learning rate: 4.88e-04, decoder learning rate: 4.88e-04\n",
      "[178/300] At -2.0 dB, Train Loss: 0.025867365300655365 Train BER 0.0012000000569969416,                  \n",
      " [178/300] At 0.0 dB, Train Loss: 3.817555261775851e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4578 minutes\n",
      "encoder learning rate: 4.82e-04, decoder learning rate: 4.82e-04\n",
      "[179/300] At -2.0 dB, Train Loss: 0.030558189377188683 Train BER 0.0015027027111500502,                  \n",
      " [179/300] At 0.0 dB, Train Loss: 2.495240914868191e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5162 minutes\n",
      "encoder learning rate: 4.75e-04, decoder learning rate: 4.75e-04\n",
      "[180/300] At -2.0 dB, Train Loss: 0.031224003061652184 Train BER 0.0015567567897960544,                  \n",
      " [180/300] At 0.0 dB, Train Loss: 6.518498412333429e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5206 minutes\n",
      "encoder learning rate: 4.69e-04, decoder learning rate: 4.69e-04\n",
      "[181/300] At -2.0 dB, Train Loss: 0.023305771872401237 Train BER 0.0010216216323897243,                  \n",
      " [181/300] At 0.0 dB, Train Loss: 3.870535874739289e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4429 minutes\n",
      "encoder learning rate: 4.63e-04, decoder learning rate: 4.63e-04\n",
      "[182/300] At -2.0 dB, Train Loss: 0.019928576424717903 Train BER 0.0009567567612975836,                  \n",
      " [182/300] At 0.0 dB, Train Loss: 8.557020919397473e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4669 minutes\n",
      "encoder learning rate: 4.57e-04, decoder learning rate: 4.57e-04\n",
      "[183/300] At -2.0 dB, Train Loss: 0.023887502029538155 Train BER 0.0010756757110357285,                  \n",
      " [183/300] At 0.0 dB, Train Loss: 0.00017046653374563903 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4646 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[184/300] At -2.0 dB, Train Loss: 0.02397855743765831 Train BER 0.0010702703148126602,                  \n",
      " [184/300] At 0.0 dB, Train Loss: 3.5487664717948064e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4555 minutes\n",
      "encoder learning rate: 4.44e-04, decoder learning rate: 4.44e-04\n",
      "[185/300] At -2.0 dB, Train Loss: 0.038410212844610214 Train BER 0.001810810761526227,                  \n",
      " [185/300] At 0.0 dB, Train Loss: 0.00030646787490695715 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4752 minutes\n",
      "encoder learning rate: 4.38e-04, decoder learning rate: 4.38e-04\n",
      "[186/300] At -2.0 dB, Train Loss: 0.02434520423412323 Train BER 0.0012648648116737604,                  \n",
      " [186/300] At 0.0 dB, Train Loss: 3.575116352294572e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4423 minutes\n",
      "encoder learning rate: 4.32e-04, decoder learning rate: 4.32e-04\n",
      "[187/300] At -2.0 dB, Train Loss: 0.023454703390598297 Train BER 0.0010486486135050654,                  \n",
      " [187/300] At 0.0 dB, Train Loss: 0.00013291547656990588 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4428 minutes\n",
      "encoder learning rate: 4.25e-04, decoder learning rate: 4.25e-04\n",
      "[188/300] At -2.0 dB, Train Loss: 0.027401793748140335 Train BER 0.0012972973054274917,                  \n",
      " [188/300] At 0.0 dB, Train Loss: 2.590102121757809e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4539 minutes\n",
      "encoder learning rate: 4.19e-04, decoder learning rate: 4.19e-04\n",
      "[189/300] At -2.0 dB, Train Loss: 0.019155945628881454 Train BER 0.0008972972864285111,                  \n",
      " [189/300] At 0.0 dB, Train Loss: 3.485507477307692e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4305 minutes\n",
      "encoder learning rate: 4.13e-04, decoder learning rate: 4.13e-04\n",
      "[190/300] At -2.0 dB, Train Loss: 0.027185747399926186 Train BER 0.001356756780296564,                  \n",
      " [190/300] At 0.0 dB, Train Loss: 0.0004174902569502592 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4477 minutes\n",
      "encoder learning rate: 4.07e-04, decoder learning rate: 4.07e-04\n",
      "[191/300] At -2.0 dB, Train Loss: 0.026314465329051018 Train BER 0.0012702703243121505,                  \n",
      " [191/300] At 0.0 dB, Train Loss: 3.307946462882683e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4575 minutes\n",
      "encoder learning rate: 4.01e-04, decoder learning rate: 4.01e-04\n",
      "[192/300] At -2.0 dB, Train Loss: 0.023996613919734955 Train BER 0.0011297296732664108,                  \n",
      " [192/300] At 0.0 dB, Train Loss: 5.192326716496609e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4617 minutes\n",
      "encoder learning rate: 3.95e-04, decoder learning rate: 3.95e-04\n",
      "[193/300] At -2.0 dB, Train Loss: 0.030022524297237396 Train BER 0.001448648632504046,                  \n",
      " [193/300] At 0.0 dB, Train Loss: 1.2242074262758251e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4484 minutes\n",
      "encoder learning rate: 3.88e-04, decoder learning rate: 3.88e-04\n",
      "[194/300] At -2.0 dB, Train Loss: 0.02974250540137291 Train BER 0.0013837837614119053,                  \n",
      " [194/300] At 0.0 dB, Train Loss: 6.693221803288907e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4602 minutes\n",
      "encoder learning rate: 3.82e-04, decoder learning rate: 3.82e-04\n",
      "[195/300] At -2.0 dB, Train Loss: 0.03061932697892189 Train BER 0.0013891891576349735,                  \n",
      " [195/300] At 0.0 dB, Train Loss: 8.126794273266569e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4351 minutes\n",
      "encoder learning rate: 3.76e-04, decoder learning rate: 3.76e-04\n",
      "[196/300] At -2.0 dB, Train Loss: 0.028733056038618088 Train BER 0.0013351350789889693,                  \n",
      " [196/300] At 0.0 dB, Train Loss: 2.091065289278049e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4340 minutes\n",
      "encoder learning rate: 3.70e-04, decoder learning rate: 3.70e-04\n",
      "[197/300] At -2.0 dB, Train Loss: 0.026259353384375572 Train BER 0.0013081080978736281,                  \n",
      " [197/300] At 0.0 dB, Train Loss: 5.922750642639585e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4415 minutes\n",
      "encoder learning rate: 3.64e-04, decoder learning rate: 3.64e-04\n",
      "[198/300] At -2.0 dB, Train Loss: 0.02178957499563694 Train BER 0.0012270270381122828,                  \n",
      " [198/300] At 0.0 dB, Train Loss: 0.0001537267817184329 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4516 minutes\n",
      "encoder learning rate: 3.58e-04, decoder learning rate: 3.58e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/300] At -2.0 dB, Train Loss: 0.024628816172480583 Train BER 0.00139999995008111,                  \n",
      " [199/300] At 0.0 dB, Train Loss: 2.31002541113412e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4521 minutes\n",
      "encoder learning rate: 3.52e-04, decoder learning rate: 3.52e-04\n",
      "[200/300] At -2.0 dB, Train Loss: 0.029016099870204926 Train BER 0.0012648648116737604,                  \n",
      " [200/300] At 0.0 dB, Train Loss: 0.00011649949010461569 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4574 minutes\n",
      "encoder learning rate: 3.46e-04, decoder learning rate: 3.46e-04\n",
      "[201/300] At -2.0 dB, Train Loss: 0.019910374656319618 Train BER 0.0010270270286127925,                  \n",
      " [201/300] At 0.0 dB, Train Loss: 0.00023480170057155192 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.5102 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[202/300] At -2.0 dB, Train Loss: 0.03186948597431183 Train BER 0.0015243242960423231,                  \n",
      " [202/300] At 0.0 dB, Train Loss: 8.071277261478826e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6663 minutes\n",
      "encoder learning rate: 3.34e-04, decoder learning rate: 3.34e-04\n",
      "[203/300] At -2.0 dB, Train Loss: 0.023245548829436302 Train BER 0.000940540514420718,                  \n",
      " [203/300] At 0.0 dB, Train Loss: 2.5841116439551115e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6800 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[204/300] At -2.0 dB, Train Loss: 0.02955286204814911 Train BER 0.0013351350789889693,                  \n",
      " [204/300] At 0.0 dB, Train Loss: 3.519180609146133e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5844 minutes\n",
      "encoder learning rate: 3.22e-04, decoder learning rate: 3.22e-04\n",
      "[205/300] At -2.0 dB, Train Loss: 0.02619660645723343 Train BER 0.0012594594154506922,                  \n",
      " [205/300] At 0.0 dB, Train Loss: 0.0001026288082357496 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.5474 minutes\n",
      "encoder learning rate: 3.17e-04, decoder learning rate: 3.17e-04\n",
      "[206/300] At -2.0 dB, Train Loss: 0.020675648003816605 Train BER 0.000870270247105509,                  \n",
      " [206/300] At 0.0 dB, Train Loss: 1.2941644854436163e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5935 minutes\n",
      "encoder learning rate: 3.11e-04, decoder learning rate: 3.11e-04\n",
      "[207/300] At -2.0 dB, Train Loss: 0.025981558486819267 Train BER 0.0013135134940966964,                  \n",
      " [207/300] At 0.0 dB, Train Loss: 4.8274785513058305e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5897 minutes\n",
      "encoder learning rate: 3.05e-04, decoder learning rate: 3.05e-04\n",
      "[208/300] At -2.0 dB, Train Loss: 0.022579817101359367 Train BER 0.001016216236166656,                  \n",
      " [208/300] At 0.0 dB, Train Loss: 4.753475877805613e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6042 minutes\n",
      "encoder learning rate: 2.99e-04, decoder learning rate: 2.99e-04\n",
      "[209/300] At -2.0 dB, Train Loss: 0.02248338982462883 Train BER 0.0010432432172819972,                  \n",
      " [209/300] At 0.0 dB, Train Loss: 4.426207669894211e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6227 minutes\n",
      "encoder learning rate: 2.94e-04, decoder learning rate: 2.94e-04\n",
      "[210/300] At -2.0 dB, Train Loss: 0.027058430016040802 Train BER 0.0013351350789889693,                  \n",
      " [210/300] At 0.0 dB, Train Loss: 3.5326647775946185e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0684 minutes\n",
      "encoder learning rate: 2.88e-04, decoder learning rate: 2.88e-04\n",
      "[211/300] At -2.0 dB, Train Loss: 0.02473825216293335 Train BER 0.001162162167020142,                  \n",
      " [211/300] At 0.0 dB, Train Loss: 2.7796762879006565e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0760 minutes\n",
      "encoder learning rate: 2.82e-04, decoder learning rate: 2.82e-04\n",
      "[212/300] At -2.0 dB, Train Loss: 0.02637050859630108 Train BER 0.001329729682765901,                  \n",
      " [212/300] At 0.0 dB, Train Loss: 6.64051913190633e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9963 minutes\n",
      "encoder learning rate: 2.77e-04, decoder learning rate: 2.77e-04\n",
      "[213/300] At -2.0 dB, Train Loss: 0.037736184895038605 Train BER 0.0018486486515030265,                  \n",
      " [213/300] At 0.0 dB, Train Loss: 1.680811692494899e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8423 minutes\n",
      "encoder learning rate: 2.71e-04, decoder learning rate: 2.71e-04\n",
      "[214/300] At -2.0 dB, Train Loss: 0.028078563511371613 Train BER 0.0014594594249501824,                  \n",
      " [214/300] At 0.0 dB, Train Loss: 1.1144409654662013e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1260 minutes\n",
      "encoder learning rate: 2.65e-04, decoder learning rate: 2.65e-04\n",
      "[215/300] At -2.0 dB, Train Loss: 0.03036910481750965 Train BER 0.0015081081073731184,                  \n",
      " [215/300] At 0.0 dB, Train Loss: 3.6296471080277115e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0558 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[216/300] At -2.0 dB, Train Loss: 0.03081561252474785 Train BER 0.0015567567897960544,                  \n",
      " [216/300] At 0.0 dB, Train Loss: 2.895835496019572e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1037 minutes\n",
      "encoder learning rate: 2.54e-04, decoder learning rate: 2.54e-04\n",
      "[217/300] At -2.0 dB, Train Loss: 0.023880140855908394 Train BER 0.0011189188808202744,                  \n",
      " [217/300] At 0.0 dB, Train Loss: 2.631861025292892e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9532 minutes\n",
      "encoder learning rate: 2.49e-04, decoder learning rate: 2.49e-04\n",
      "[218/300] At -2.0 dB, Train Loss: 0.02904871106147766 Train BER 0.0012648648116737604,                  \n",
      " [218/300] At 0.0 dB, Train Loss: 3.065339842578396e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1038 minutes\n",
      "encoder learning rate: 2.44e-04, decoder learning rate: 2.44e-04\n",
      "[219/300] At -2.0 dB, Train Loss: 0.024108892306685448 Train BER 0.0012054054532200098,                  \n",
      " [219/300] At 0.0 dB, Train Loss: 2.1774781998828985e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0727 minutes\n",
      "encoder learning rate: 2.38e-04, decoder learning rate: 2.38e-04\n",
      "[220/300] At -2.0 dB, Train Loss: 0.028068747371435165 Train BER 0.0012162162456661463,                  \n",
      " [220/300] At 0.0 dB, Train Loss: 2.088968722091522e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0131 minutes\n",
      "encoder learning rate: 2.33e-04, decoder learning rate: 2.33e-04\n",
      "[221/300] At -2.0 dB, Train Loss: 0.025490308180451393 Train BER 0.0012864865129813552,                  \n",
      " [221/300] At 0.0 dB, Train Loss: 4.313428871682845e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9962 minutes\n",
      "encoder learning rate: 2.28e-04, decoder learning rate: 2.28e-04\n",
      "[222/300] At -2.0 dB, Train Loss: 0.02515181340277195 Train BER 0.0012648648116737604,                  \n",
      " [222/300] At 0.0 dB, Train Loss: 3.3484731829958037e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0896 minutes\n",
      "encoder learning rate: 2.22e-04, decoder learning rate: 2.22e-04\n",
      "[223/300] At -2.0 dB, Train Loss: 0.020649107173085213 Train BER 0.000989189138635993,                  \n",
      " [223/300] At 0.0 dB, Train Loss: 3.997595194960013e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0807 minutes\n",
      "encoder learning rate: 2.17e-04, decoder learning rate: 2.17e-04\n",
      "[224/300] At -2.0 dB, Train Loss: 0.02257634699344635 Train BER 0.0011459459783509374,                  \n",
      " [224/300] At 0.0 dB, Train Loss: 5.5916752899065614e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0977 minutes\n",
      "encoder learning rate: 2.12e-04, decoder learning rate: 2.12e-04\n",
      "[225/300] At -2.0 dB, Train Loss: 0.01969926804304123 Train BER 0.0008270270191133022,                  \n",
      " [225/300] At 0.0 dB, Train Loss: 3.111193291260861e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0001 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[226/300] At -2.0 dB, Train Loss: 0.025279635563492775 Train BER 0.0012972973054274917,                  \n",
      " [226/300] At 0.0 dB, Train Loss: 3.7783804145874456e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8831 minutes\n",
      "encoder learning rate: 2.02e-04, decoder learning rate: 2.02e-04\n",
      "[227/300] At -2.0 dB, Train Loss: 0.018884535878896713 Train BER 0.0008918918902054429,                  \n",
      " [227/300] At 0.0 dB, Train Loss: 1.317213536822237e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0727 minutes\n",
      "encoder learning rate: 1.97e-04, decoder learning rate: 1.97e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228/300] At -2.0 dB, Train Loss: 0.022665074095129967 Train BER 0.0012054054532200098,                  \n",
      " [228/300] At 0.0 dB, Train Loss: 6.714957271469757e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0708 minutes\n",
      "encoder learning rate: 1.92e-04, decoder learning rate: 1.92e-04\n",
      "[229/300] At -2.0 dB, Train Loss: 0.019088130444288254 Train BER 0.0008270270191133022,                  \n",
      " [229/300] At 0.0 dB, Train Loss: 1.9584478650358506e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9306 minutes\n",
      "encoder learning rate: 1.87e-04, decoder learning rate: 1.87e-04\n",
      "[230/300] At -2.0 dB, Train Loss: 0.02222013846039772 Train BER 0.001037837821058929,                  \n",
      " [230/300] At 0.0 dB, Train Loss: 0.00022930624254513532 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.1007 minutes\n",
      "encoder learning rate: 1.82e-04, decoder learning rate: 1.82e-04\n",
      "[231/300] At -2.0 dB, Train Loss: 0.01870425045490265 Train BER 0.0009351351181976497,                  \n",
      " [231/300] At 0.0 dB, Train Loss: 2.636329190863762e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1140 minutes\n",
      "encoder learning rate: 1.77e-04, decoder learning rate: 1.77e-04\n",
      "[232/300] At -2.0 dB, Train Loss: 0.019583554938435555 Train BER 0.0009621621575206518,                  \n",
      " [232/300] At 0.0 dB, Train Loss: 4.086501940037124e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0934 minutes\n",
      "encoder learning rate: 1.73e-04, decoder learning rate: 1.73e-04\n",
      "[233/300] At -2.0 dB, Train Loss: 0.024190828204154968 Train BER 0.0012594594154506922,                  \n",
      " [233/300] At 0.0 dB, Train Loss: 4.043304215883836e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0114 minutes\n",
      "encoder learning rate: 1.68e-04, decoder learning rate: 1.68e-04\n",
      "[234/300] At -2.0 dB, Train Loss: 0.032898712903261185 Train BER 0.0015783783746883273,                  \n",
      " [234/300] At 0.0 dB, Train Loss: 0.00023917137878015637 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0716 minutes\n",
      "encoder learning rate: 1.63e-04, decoder learning rate: 1.63e-04\n",
      "[235/300] At -2.0 dB, Train Loss: 0.02375611662864685 Train BER 0.0010054054437205195,                  \n",
      " [235/300] At 0.0 dB, Train Loss: 1.603960663487669e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0468 minutes\n",
      "encoder learning rate: 1.59e-04, decoder learning rate: 1.59e-04\n",
      "[236/300] At -2.0 dB, Train Loss: 0.020521139726042747 Train BER 0.0010108108399435878,                  \n",
      " [236/300] At 0.0 dB, Train Loss: 0.0001920585782499984 Train BER 0.0\n",
      "Time for one full iteration is 9.0253 minutes\n",
      "encoder learning rate: 1.54e-04, decoder learning rate: 1.54e-04\n",
      "[237/300] At -2.0 dB, Train Loss: 0.029739564284682274 Train BER 0.0015837837709113955,                  \n",
      " [237/300] At 0.0 dB, Train Loss: 8.738667020224966e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8798 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[238/300] At -2.0 dB, Train Loss: 0.023548755794763565 Train BER 0.0012054054532200098,                  \n",
      " [238/300] At 0.0 dB, Train Loss: 8.039801468839869e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8937 minutes\n",
      "encoder learning rate: 1.45e-04, decoder learning rate: 1.45e-04\n",
      "[239/300] At -2.0 dB, Train Loss: 0.023945413529872894 Train BER 0.0011081080883741379,                  \n",
      " [239/300] At 0.0 dB, Train Loss: 2.5680581529741175e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9614 minutes\n",
      "encoder learning rate: 1.41e-04, decoder learning rate: 1.41e-04\n",
      "[240/300] At -2.0 dB, Train Loss: 0.02460476942360401 Train BER 0.0011459459783509374,                  \n",
      " [240/300] At 0.0 dB, Train Loss: 0.0002129590284312144 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.1449 minutes\n",
      "encoder learning rate: 1.36e-04, decoder learning rate: 1.36e-04\n",
      "[241/300] At -2.0 dB, Train Loss: 0.021970653906464577 Train BER 0.0010216216323897243,                  \n",
      " [241/300] At 0.0 dB, Train Loss: 8.466769213555381e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1123 minutes\n",
      "encoder learning rate: 1.32e-04, decoder learning rate: 1.32e-04\n",
      "[242/300] At -2.0 dB, Train Loss: 0.025340601801872253 Train BER 0.0012162162456661463,                  \n",
      " [242/300] At 0.0 dB, Train Loss: 4.041755892103538e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8787 minutes\n",
      "encoder learning rate: 1.28e-04, decoder learning rate: 1.28e-04\n",
      "[243/300] At -2.0 dB, Train Loss: 0.01803131029009819 Train BER 0.0008594594546593726,                  \n",
      " [243/300] At 0.0 dB, Train Loss: 2.1897187252761796e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8890 minutes\n",
      "encoder learning rate: 1.24e-04, decoder learning rate: 1.24e-04\n",
      "[244/300] At -2.0 dB, Train Loss: 0.02131935954093933 Train BER 0.0011189188808202744,                  \n",
      " [244/300] At 0.0 dB, Train Loss: 1.9829132725135423e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8941 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[245/300] At -2.0 dB, Train Loss: 0.022906102240085602 Train BER 0.0010270270286127925,                  \n",
      " [245/300] At 0.0 dB, Train Loss: 3.873089372063987e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8942 minutes\n",
      "encoder learning rate: 1.16e-04, decoder learning rate: 1.16e-04\n",
      "[246/300] At -2.0 dB, Train Loss: 0.021203145384788513 Train BER 0.000994594651274383,                  \n",
      " [246/300] At 0.0 dB, Train Loss: 0.00013376260176301003 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8978 minutes\n",
      "encoder learning rate: 1.12e-04, decoder learning rate: 1.12e-04\n",
      "[247/300] At -2.0 dB, Train Loss: 0.020695341750979424 Train BER 0.0008594594546593726,                  \n",
      " [247/300] At 0.0 dB, Train Loss: 0.00010623088019201532 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9354 minutes\n",
      "encoder learning rate: 1.08e-04, decoder learning rate: 1.08e-04\n",
      "[248/300] At -2.0 dB, Train Loss: 0.02492004632949829 Train BER 0.0014108108589425683,                  \n",
      " [248/300] At 0.0 dB, Train Loss: 0.000512425322085619 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.9499 minutes\n",
      "encoder learning rate: 1.04e-04, decoder learning rate: 1.04e-04\n",
      "[249/300] At -2.0 dB, Train Loss: 0.019855519756674767 Train BER 0.0008918918902054429,                  \n",
      " [249/300] At 0.0 dB, Train Loss: 1.1983343938481994e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9001 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[250/300] At -2.0 dB, Train Loss: 0.025784416124224663 Train BER 0.0013351350789889693,                  \n",
      " [250/300] At 0.0 dB, Train Loss: 3.78845252271276e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9704 minutes\n",
      "encoder learning rate: 9.64e-05, decoder learning rate: 9.64e-05\n",
      "[251/300] At -2.0 dB, Train Loss: 0.021937619894742966 Train BER 0.001162162167020142,                  \n",
      " [251/300] At 0.0 dB, Train Loss: 1.9093584342044778e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9647 minutes\n",
      "encoder learning rate: 9.27e-05, decoder learning rate: 9.27e-05\n",
      "[252/300] At -2.0 dB, Train Loss: 0.022365054115653038 Train BER 0.0010810811072587967,                  \n",
      " [252/300] At 0.0 dB, Train Loss: 1.976661224034615e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9042 minutes\n",
      "encoder learning rate: 8.91e-05, decoder learning rate: 8.91e-05\n",
      "[253/300] At -2.0 dB, Train Loss: 0.02420874685049057 Train BER 0.0011675675632432103,                  \n",
      " [253/300] At 0.0 dB, Train Loss: 3.8864309317432344e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9666 minutes\n",
      "encoder learning rate: 8.56e-05, decoder learning rate: 8.56e-05\n",
      "[254/300] At -2.0 dB, Train Loss: 0.02107221633195877 Train BER 0.0009513513650745153,                  \n",
      " [254/300] At 0.0 dB, Train Loss: 0.0002460101677570492 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9586 minutes\n",
      "encoder learning rate: 8.22e-05, decoder learning rate: 8.22e-05\n",
      "[255/300] At -2.0 dB, Train Loss: 0.026184523478150368 Train BER 0.0012432432267814875,                  \n",
      " [255/300] At 0.0 dB, Train Loss: 9.609201697458047e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.9292 minutes\n",
      "encoder learning rate: 7.88e-05, decoder learning rate: 7.88e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256/300] At -2.0 dB, Train Loss: 0.02304014377295971 Train BER 0.0012594594154506922,                  \n",
      " [256/300] At 0.0 dB, Train Loss: 1.7232541722478345e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9722 minutes\n",
      "encoder learning rate: 7.54e-05, decoder learning rate: 7.54e-05\n",
      "[257/300] At -2.0 dB, Train Loss: 0.026301447302103043 Train BER 0.00130270270165056,                  \n",
      " [257/300] At 0.0 dB, Train Loss: 1.4733535863342695e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9515 minutes\n",
      "encoder learning rate: 7.22e-05, decoder learning rate: 7.22e-05\n",
      "[258/300] At -2.0 dB, Train Loss: 0.017741406336426735 Train BER 0.0008540540584363043,                  \n",
      " [258/300] At 0.0 dB, Train Loss: 5.817054625367746e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9187 minutes\n",
      "encoder learning rate: 6.90e-05, decoder learning rate: 6.90e-05\n",
      "[259/300] At -2.0 dB, Train Loss: 0.02011396363377571 Train BER 0.0008810810977593064,                  \n",
      " [259/300] At 0.0 dB, Train Loss: 0.00010728094639489427 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8992 minutes\n",
      "encoder learning rate: 6.58e-05, decoder learning rate: 6.58e-05\n",
      "[260/300] At -2.0 dB, Train Loss: 0.019424330443143845 Train BER 0.0008864864939823747,                  \n",
      " [260/300] At 0.0 dB, Train Loss: 4.5854591007810086e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9145 minutes\n",
      "encoder learning rate: 6.28e-05, decoder learning rate: 6.28e-05\n",
      "[261/300] At -2.0 dB, Train Loss: 0.02243453823029995 Train BER 0.0010486486135050654,                  \n",
      " [261/300] At 0.0 dB, Train Loss: 1.2656642866204493e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8979 minutes\n",
      "encoder learning rate: 5.98e-05, decoder learning rate: 5.98e-05\n",
      "[262/300] At -2.0 dB, Train Loss: 0.022310057654976845 Train BER 0.0011783783556893468,                  \n",
      " [262/300] At 0.0 dB, Train Loss: 0.0005507557070814073 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.9287 minutes\n",
      "encoder learning rate: 5.69e-05, decoder learning rate: 5.69e-05\n",
      "[263/300] At -2.0 dB, Train Loss: 0.02031402476131916 Train BER 0.0009081081370823085,                  \n",
      " [263/300] At 0.0 dB, Train Loss: 0.0001193024727399461 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9364 minutes\n",
      "encoder learning rate: 5.40e-05, decoder learning rate: 5.40e-05\n",
      "[264/300] At -2.0 dB, Train Loss: 0.019137725234031677 Train BER 0.0009459459688514471,                  \n",
      " [264/300] At 0.0 dB, Train Loss: 2.0842047888436355e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9295 minutes\n",
      "encoder learning rate: 5.12e-05, decoder learning rate: 5.12e-05\n",
      "[265/300] At -2.0 dB, Train Loss: 0.02519604004919529 Train BER 0.0012486486230045557,                  \n",
      " [265/300] At 0.0 dB, Train Loss: 2.1209740225458518e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9958 minutes\n",
      "encoder learning rate: 4.85e-05, decoder learning rate: 4.85e-05\n",
      "[266/300] At -2.0 dB, Train Loss: 0.01869433932006359 Train BER 0.0008540540584363043,                  \n",
      " [266/300] At 0.0 dB, Train Loss: 3.49582769558765e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9445 minutes\n",
      "encoder learning rate: 4.59e-05, decoder learning rate: 4.59e-05\n",
      "[267/300] At -2.0 dB, Train Loss: 0.023443477228283882 Train BER 0.0011513513745740056,                  \n",
      " [267/300] At 0.0 dB, Train Loss: 4.612372140400112e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8793 minutes\n",
      "encoder learning rate: 4.33e-05, decoder learning rate: 4.33e-05\n",
      "[268/300] At -2.0 dB, Train Loss: 0.026533471420407295 Train BER 0.0013459459878504276,                  \n",
      " [268/300] At 0.0 dB, Train Loss: 2.590537042124197e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9330 minutes\n",
      "encoder learning rate: 4.08e-05, decoder learning rate: 4.08e-05\n",
      "[269/300] At -2.0 dB, Train Loss: 0.018832307308912277 Train BER 0.0007837837911210954,                  \n",
      " [269/300] At 0.0 dB, Train Loss: 2.7857424356625415e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9530 minutes\n",
      "encoder learning rate: 3.84e-05, decoder learning rate: 3.84e-05\n",
      "[270/300] At -2.0 dB, Train Loss: 0.014095966704189777 Train BER 0.0005351351574063301,                  \n",
      " [270/300] At 0.0 dB, Train Loss: 5.392234743339941e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9593 minutes\n",
      "encoder learning rate: 3.61e-05, decoder learning rate: 3.61e-05\n",
      "[271/300] At -2.0 dB, Train Loss: 0.02137281745672226 Train BER 0.0010810811072587967,                  \n",
      " [271/300] At 0.0 dB, Train Loss: 9.424332529306412e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9821 minutes\n",
      "encoder learning rate: 3.38e-05, decoder learning rate: 3.38e-05\n",
      "[272/300] At -2.0 dB, Train Loss: 0.018466155976057053 Train BER 0.0008540540584363043,                  \n",
      " [272/300] At 0.0 dB, Train Loss: 1.3966689039079938e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9308 minutes\n",
      "encoder learning rate: 3.16e-05, decoder learning rate: 3.16e-05\n",
      "[273/300] At -2.0 dB, Train Loss: 0.017835253849625587 Train BER 0.0007621621480211616,                  \n",
      " [273/300] At 0.0 dB, Train Loss: 5.350070568965748e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9420 minutes\n",
      "encoder learning rate: 2.95e-05, decoder learning rate: 2.95e-05\n",
      "[274/300] At -2.0 dB, Train Loss: 0.027481023222208023 Train BER 0.0014324324438348413,                  \n",
      " [274/300] At 0.0 dB, Train Loss: 6.153956928756088e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9319 minutes\n",
      "encoder learning rate: 2.74e-05, decoder learning rate: 2.74e-05\n",
      "[275/300] At -2.0 dB, Train Loss: 0.024893920868635178 Train BER 0.001329729682765901,                  \n",
      " [275/300] At 0.0 dB, Train Loss: 7.432878919644281e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9060 minutes\n",
      "encoder learning rate: 2.54e-05, decoder learning rate: 2.54e-05\n",
      "[276/300] At -2.0 dB, Train Loss: 0.016910454258322716 Train BER 0.0008810810977593064,                  \n",
      " [276/300] At 0.0 dB, Train Loss: 5.7954181102104485e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9747 minutes\n",
      "encoder learning rate: 2.35e-05, decoder learning rate: 2.35e-05\n",
      "[277/300] At -2.0 dB, Train Loss: 0.020096654072403908 Train BER 0.000989189138635993,                  \n",
      " [277/300] At 0.0 dB, Train Loss: 1.8084669136442244e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9011 minutes\n",
      "encoder learning rate: 2.17e-05, decoder learning rate: 2.17e-05\n",
      "[278/300] At -2.0 dB, Train Loss: 0.021624578163027763 Train BER 0.0010054054437205195,                  \n",
      " [278/300] At 0.0 dB, Train Loss: 1.5807763702468947e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9777 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[279/300] At -2.0 dB, Train Loss: 0.02312404103577137 Train BER 0.0010810811072587967,                  \n",
      " [279/300] At 0.0 dB, Train Loss: 0.00030616504955105484 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9676 minutes\n",
      "encoder learning rate: 1.83e-05, decoder learning rate: 1.83e-05\n",
      "[280/300] At -2.0 dB, Train Loss: 0.022560415789484978 Train BER 0.0011675675632432103,                  \n",
      " [280/300] At 0.0 dB, Train Loss: 2.1788946469314396e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1880 minutes\n",
      "encoder learning rate: 1.67e-05, decoder learning rate: 1.67e-05\n",
      "[281/300] At -2.0 dB, Train Loss: 0.02196802943944931 Train BER 0.0011027026921510696,                  \n",
      " [281/300] At 0.0 dB, Train Loss: 2.3186950784293003e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1017 minutes\n",
      "encoder learning rate: 1.52e-05, decoder learning rate: 1.52e-05\n",
      "[282/300] At -2.0 dB, Train Loss: 0.017660723999142647 Train BER 0.0008540540584363043,                  \n",
      " [282/300] At 0.0 dB, Train Loss: 0.00014255312271416187 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.2169 minutes\n",
      "encoder learning rate: 1.37e-05, decoder learning rate: 1.37e-05\n",
      "[283/300] At -2.0 dB, Train Loss: 0.017551757395267487 Train BER 0.0007729729986749589,                  \n",
      " [283/300] At 0.0 dB, Train Loss: 7.841684418963268e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.2614 minutes\n",
      "encoder learning rate: 1.24e-05, decoder learning rate: 1.24e-05\n",
      "[284/300] At -2.0 dB, Train Loss: 0.02649547904729843 Train BER 0.0013135134940966964,                  \n",
      " [284/300] At 0.0 dB, Train Loss: 3.159866537316702e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1221 minutes\n",
      "encoder learning rate: 1.11e-05, decoder learning rate: 1.11e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285/300] At -2.0 dB, Train Loss: 0.018562473356723785 Train BER 0.0009783783461898565,                  \n",
      " [285/300] At 0.0 dB, Train Loss: 0.00015105762577150017 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.1249 minutes\n",
      "encoder learning rate: 9.85e-06, decoder learning rate: 9.85e-06\n",
      "[286/300] At -2.0 dB, Train Loss: 0.025292333215475082 Train BER 0.0011891891481354833,                  \n",
      " [286/300] At 0.0 dB, Train Loss: 7.905554957687855e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.1019 minutes\n",
      "encoder learning rate: 8.71e-06, decoder learning rate: 8.71e-06\n",
      "[287/300] At -2.0 dB, Train Loss: 0.02245575562119484 Train BER 0.0010972972959280014,                  \n",
      " [287/300] At 0.0 dB, Train Loss: 3.872856541420333e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1840 minutes\n",
      "encoder learning rate: 7.65e-06, decoder learning rate: 7.65e-06\n",
      "[288/300] At -2.0 dB, Train Loss: 0.018970653414726257 Train BER 0.0009567567612975836,                  \n",
      " [288/300] At 0.0 dB, Train Loss: 4.095883559784852e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0379 minutes\n",
      "encoder learning rate: 6.67e-06, decoder learning rate: 6.67e-06\n",
      "[289/300] At -2.0 dB, Train Loss: 0.015561160631477833 Train BER 0.0006702702958136797,                  \n",
      " [289/300] At 0.0 dB, Train Loss: 6.828871119068936e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0776 minutes\n",
      "encoder learning rate: 5.76e-06, decoder learning rate: 5.76e-06\n",
      "[290/300] At -2.0 dB, Train Loss: 0.0209522545337677 Train BER 0.0009135135333053768,                  \n",
      " [290/300] At 0.0 dB, Train Loss: 3.0785377020947635e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1592 minutes\n",
      "encoder learning rate: 4.94e-06, decoder learning rate: 4.94e-06\n",
      "[291/300] At -2.0 dB, Train Loss: 0.022572187706828117 Train BER 0.001135135185904801,                  \n",
      " [291/300] At 0.0 dB, Train Loss: 4.672526483773254e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0621 minutes\n",
      "encoder learning rate: 4.19e-06, decoder learning rate: 4.19e-06\n",
      "[292/300] At -2.0 dB, Train Loss: 0.02063448168337345 Train BER 0.0009621621575206518,                  \n",
      " [292/300] At 0.0 dB, Train Loss: 1.3172166291042231e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0317 minutes\n",
      "encoder learning rate: 3.52e-06, decoder learning rate: 3.52e-06\n",
      "[293/300] At -2.0 dB, Train Loss: 0.02105952799320221 Train BER 0.0010702703148126602,                  \n",
      " [293/300] At 0.0 dB, Train Loss: 6.000218854751438e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8668 minutes\n",
      "encoder learning rate: 2.93e-06, decoder learning rate: 2.93e-06\n",
      "[294/300] At -2.0 dB, Train Loss: 0.02294897846877575 Train BER 0.0011891891481354833,                  \n",
      " [294/300] At 0.0 dB, Train Loss: 3.964202187489718e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8925 minutes\n",
      "encoder learning rate: 2.42e-06, decoder learning rate: 2.42e-06\n",
      "[295/300] At -2.0 dB, Train Loss: 0.02991294302046299 Train BER 0.0015459459973499179,                  \n",
      " [295/300] At 0.0 dB, Train Loss: 8.066902410064358e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8580 minutes\n",
      "encoder learning rate: 1.99e-06, decoder learning rate: 1.99e-06\n",
      "[296/300] At -2.0 dB, Train Loss: 0.020326191559433937 Train BER 0.0009837837424129248,                  \n",
      " [296/300] At 0.0 dB, Train Loss: 4.7436693421332166e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0049 minutes\n",
      "encoder learning rate: 1.63e-06, decoder learning rate: 1.63e-06\n",
      "[297/300] At -2.0 dB, Train Loss: 0.017499249428510666 Train BER 0.0008432432659901679,                  \n",
      " [297/300] At 0.0 dB, Train Loss: 1.951332887983881e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9020 minutes\n",
      "encoder learning rate: 1.35e-06, decoder learning rate: 1.35e-06\n",
      "[298/300] At -2.0 dB, Train Loss: 0.01767657697200775 Train BER 0.0008162162266671658,                  \n",
      " [298/300] At 0.0 dB, Train Loss: 5.273498027236201e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8812 minutes\n",
      "encoder learning rate: 1.16e-06, decoder learning rate: 1.16e-06\n",
      "[299/300] At -2.0 dB, Train Loss: 0.017902081832289696 Train BER 0.0008648648508824408,                  \n",
      " [299/300] At 0.0 dB, Train Loss: 0.0001891767024062574 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.8304 minutes\n",
      "encoder learning rate: 1.04e-06, decoder learning rate: 1.04e-06\n",
      "[300/300] At -2.0 dB, Train Loss: 0.02109118551015854 Train BER 0.0009567567612975836,                  \n",
      " [300/300] At 0.0 dB, Train Loss: 1.4880209164402913e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7011 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO__0.0_Encoder_KO_-2.0_Decoder/epochs_300_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166454, BLER: 0.435074\n",
      "Deep - BER: 0.128219, BLER: 0.482486\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072448, BLER: 0.196732\n",
      "Deep - BER: 0.046305, BLER: 0.209292\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.020022, BLER: 0.055692\n",
      "Deep - BER: 0.009701, BLER: 0.054652\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003030, BLER: 0.008506\n",
      "Deep - BER: 0.001055, BLER: 0.008172\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000208, BLER: 0.000577\n",
      "Deep - BER: 0.000056, BLER: 0.000674\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.12821886488050221, 0.04630454054474831, 0.00970062163611874, 0.0010551891915474697, 5.5513513321784556e-05]\n",
      "BERs of SC decoding: [0.16645354059338568, 0.07244778386875987, 0.020021567540243267, 0.0030299729665130143, 0.0002075675677151594]\n",
      "BLERs of DeepPolar: [0.482486000000001, 0.20929200000000003, 0.05465199999999987, 0.00817199999999996, 0.0006740000000000004]\n",
      "BLERs of SC decoding: [0.4350739999999997, 0.19673200000000013, 0.0556919999999999, 0.008505999999999953, 0.0005770000000000004]\n",
      "time = 401.15560228824614 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
