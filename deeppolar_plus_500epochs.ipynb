{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = False  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 500  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = 0.  # SNR at which encoder is trained\n",
    "dec_train_snr = -2.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.001  # Decoder Learning rate\n",
    "enc_lr = 0.001  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}__{enc_train_snr}_Encoder_{decoder_type}_{dec_train_snr}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/500] At -2.0 dB, Train Loss: 8.294940948486328 Train BER 0.477708101272583,                  \n",
      " [1/500] At 0.0 dB, Train Loss: 8.03927993774414 Train BER 0.47341081500053406\n",
      "Time for one full iteration is 9.1491 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[2/500] At -2.0 dB, Train Loss: 6.443418025970459 Train BER 0.46587568521499634,                  \n",
      " [2/500] At 0.0 dB, Train Loss: 6.370804309844971 Train BER 0.4617675542831421\n",
      "Time for one full iteration is 9.1561 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[3/500] At -2.0 dB, Train Loss: 5.996586799621582 Train BER 0.45709729194641113,                  \n",
      " [3/500] At 0.0 dB, Train Loss: 5.954767227172852 Train BER 0.4460756778717041\n",
      "Time for one full iteration is 9.0673 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[4/500] At -2.0 dB, Train Loss: 5.570364475250244 Train BER 0.40436217188835144,                  \n",
      " [4/500] At 0.0 dB, Train Loss: 5.306490898132324 Train BER 0.37046486139297485\n",
      "Time for one full iteration is 9.0969 minutes\n",
      "encoder learning rate: 8.00e-05, decoder learning rate: 8.00e-05\n",
      "[5/500] At -2.0 dB, Train Loss: 2.8932437896728516 Train BER 0.16725406050682068,                  \n",
      " [5/500] At 0.0 dB, Train Loss: 1.9617384672164917 Train BER 0.10443243384361267\n",
      "Time for one full iteration is 8.9803 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[6/500] At -2.0 dB, Train Loss: 1.4915382862091064 Train BER 0.07358378171920776,                  \n",
      " [6/500] At 0.0 dB, Train Loss: 0.5208749175071716 Train BER 0.02401621639728546\n",
      "Time for one full iteration is 8.9720 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[7/500] At -2.0 dB, Train Loss: 0.8784626126289368 Train BER 0.04241621494293213,                  \n",
      " [7/500] At 0.0 dB, Train Loss: 0.2977377474308014 Train BER 0.015383783727884293\n",
      "Time for one full iteration is 8.9951 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[8/500] At -2.0 dB, Train Loss: 0.6220550537109375 Train BER 0.02918918989598751,                  \n",
      " [8/500] At 0.0 dB, Train Loss: 0.22868986427783966 Train BER 0.010335135273635387\n",
      "Time for one full iteration is 8.9773 minutes\n",
      "encoder learning rate: 1.60e-04, decoder learning rate: 1.60e-04\n",
      "[9/500] At -2.0 dB, Train Loss: 0.4468737244606018 Train BER 0.018432432785630226,                  \n",
      " [9/500] At 0.0 dB, Train Loss: 0.08090230822563171 Train BER 0.00227567576803267\n",
      "Time for one full iteration is 8.9828 minutes\n",
      "encoder learning rate: 1.80e-04, decoder learning rate: 1.80e-04\n",
      "[10/500] At -2.0 dB, Train Loss: 0.32193607091903687 Train BER 0.013016216456890106,                  \n",
      " [10/500] At 0.0 dB, Train Loss: 0.039073117077350616 Train BER 0.0008486486622132361\n",
      "Time for one full iteration is 9.0341 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[11/500] At -2.0 dB, Train Loss: 0.2571004033088684 Train BER 0.010189189575612545,                  \n",
      " [11/500] At 0.0 dB, Train Loss: 0.02568700909614563 Train BER 0.0005891891778446734\n",
      "Time for one full iteration is 8.9851 minutes\n",
      "encoder learning rate: 2.20e-04, decoder learning rate: 2.20e-04\n",
      "[12/500] At -2.0 dB, Train Loss: 0.2164522260427475 Train BER 0.008729729801416397,                  \n",
      " [12/500] At 0.0 dB, Train Loss: 0.018260682001709938 Train BER 0.00037837837589904666\n",
      "Time for one full iteration is 8.9530 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[13/500] At -2.0 dB, Train Loss: 0.17954373359680176 Train BER 0.007108108140528202,                  \n",
      " [13/500] At 0.0 dB, Train Loss: 0.012513396330177784 Train BER 0.0002918918908108026\n",
      "Time for one full iteration is 9.0554 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[14/500] At -2.0 dB, Train Loss: 0.15815889835357666 Train BER 0.006270270328968763,                  \n",
      " [14/500] At 0.0 dB, Train Loss: 0.010560194030404091 Train BER 0.00022702703427057713\n",
      "Time for one full iteration is 9.0671 minutes\n",
      "encoder learning rate: 2.80e-04, decoder learning rate: 2.80e-04\n",
      "[15/500] At -2.0 dB, Train Loss: 0.12098761647939682 Train BER 0.004843243397772312,                  \n",
      " [15/500] At 0.0 dB, Train Loss: 0.007563220337033272 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 9.0023 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[16/500] At -2.0 dB, Train Loss: 0.1424194723367691 Train BER 0.005902702920138836,                  \n",
      " [16/500] At 0.0 dB, Train Loss: 0.007348187267780304 Train BER 0.000156756752403453\n",
      "Time for one full iteration is 9.0214 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[17/500] At -2.0 dB, Train Loss: 0.12845683097839355 Train BER 0.0052756755612790585,                  \n",
      " [17/500] At 0.0 dB, Train Loss: 0.005114240106195211 Train BER 0.00012432433140929788\n",
      "Time for one full iteration is 8.9922 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[18/500] At -2.0 dB, Train Loss: 0.10253459960222244 Train BER 0.0041189189068973064,                  \n",
      " [18/500] At 0.0 dB, Train Loss: 0.006489878986030817 Train BER 0.00022702703427057713\n",
      "Time for one full iteration is 8.9574 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[19/500] At -2.0 dB, Train Loss: 0.09802644699811935 Train BER 0.004086486529558897,                  \n",
      " [19/500] At 0.0 dB, Train Loss: 0.003806914435699582 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 9.0749 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[20/500] At -2.0 dB, Train Loss: 0.10510466247797012 Train BER 0.004378378391265869,                  \n",
      " [20/500] At 0.0 dB, Train Loss: 0.0034261092077940702 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 9.0898 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[21/500] At -2.0 dB, Train Loss: 0.08016844838857651 Train BER 0.003183783730491996,                  \n",
      " [21/500] At 0.0 dB, Train Loss: 0.0029615715611726046 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 9.0556 minutes\n",
      "encoder learning rate: 4.20e-04, decoder learning rate: 4.20e-04\n",
      "[22/500] At -2.0 dB, Train Loss: 0.0870141088962555 Train BER 0.003454054007306695,                  \n",
      " [22/500] At 0.0 dB, Train Loss: 0.0042438022792339325 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 9.1072 minutes\n",
      "encoder learning rate: 4.40e-04, decoder learning rate: 4.40e-04\n",
      "[23/500] At -2.0 dB, Train Loss: 0.09235797822475433 Train BER 0.0039459457620978355,                  \n",
      " [23/500] At 0.0 dB, Train Loss: 0.002514153951779008 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 9.0612 minutes\n",
      "encoder learning rate: 4.60e-04, decoder learning rate: 4.60e-04\n",
      "[24/500] At -2.0 dB, Train Loss: 0.07433732599020004 Train BER 0.0030378377996385098,                  \n",
      " [24/500] At 0.0 dB, Train Loss: 0.0036635431461036205 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 9.0280 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[25/500] At -2.0 dB, Train Loss: 0.06797583401203156 Train BER 0.0028378379065543413,                  \n",
      " [25/500] At 0.0 dB, Train Loss: 0.0027110206428915262 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 9.0591 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[26/500] At -2.0 dB, Train Loss: 0.0856446623802185 Train BER 0.003481080988422036,                  \n",
      " [26/500] At 0.0 dB, Train Loss: 0.0026603834703564644 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 9.0183 minutes\n",
      "encoder learning rate: 5.20e-04, decoder learning rate: 5.20e-04\n",
      "[27/500] At -2.0 dB, Train Loss: 0.08053123950958252 Train BER 0.003594594541937113,                  \n",
      " [27/500] At 0.0 dB, Train Loss: 0.002304530469700694 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 9.0400 minutes\n",
      "encoder learning rate: 5.40e-04, decoder learning rate: 5.40e-04\n",
      "[28/500] At -2.0 dB, Train Loss: 0.07389181852340698 Train BER 0.0031027027871459723,                  \n",
      " [28/500] At 0.0 dB, Train Loss: 0.0022918106988072395 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 9.0319 minutes\n",
      "encoder learning rate: 5.60e-04, decoder learning rate: 5.60e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/500] At -2.0 dB, Train Loss: 0.0729181170463562 Train BER 0.002881081076338887,                  \n",
      " [29/500] At 0.0 dB, Train Loss: 0.002056569093838334 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 9.0600 minutes\n",
      "encoder learning rate: 5.80e-04, decoder learning rate: 5.80e-04\n",
      "[30/500] At -2.0 dB, Train Loss: 0.07224392145872116 Train BER 0.002994594629853964,                  \n",
      " [30/500] At 0.0 dB, Train Loss: 0.002536626299843192 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 9.0556 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[31/500] At -2.0 dB, Train Loss: 0.07338368892669678 Train BER 0.0032540541142225266,                  \n",
      " [31/500] At 0.0 dB, Train Loss: 0.0029891799204051495 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 9.1216 minutes\n",
      "encoder learning rate: 6.20e-04, decoder learning rate: 6.20e-04\n",
      "[32/500] At -2.0 dB, Train Loss: 0.0744866132736206 Train BER 0.0032324325293302536,                  \n",
      " [32/500] At 0.0 dB, Train Loss: 0.00158638262655586 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 9.0005 minutes\n",
      "encoder learning rate: 6.40e-04, decoder learning rate: 6.40e-04\n",
      "[33/500] At -2.0 dB, Train Loss: 0.05405690148472786 Train BER 0.002270270371809602,                  \n",
      " [33/500] At 0.0 dB, Train Loss: 0.0009624628000892699 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0812 minutes\n",
      "encoder learning rate: 6.60e-04, decoder learning rate: 6.60e-04\n",
      "[34/500] At -2.0 dB, Train Loss: 0.05928673595190048 Train BER 0.002686486579477787,                  \n",
      " [34/500] At 0.0 dB, Train Loss: 0.0007092025480233133 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0372 minutes\n",
      "encoder learning rate: 6.80e-04, decoder learning rate: 6.80e-04\n",
      "[35/500] At -2.0 dB, Train Loss: 0.059201374650001526 Train BER 0.002389189088717103,                  \n",
      " [35/500] At 0.0 dB, Train Loss: 0.000784315459895879 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0168 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n",
      "[36/500] At -2.0 dB, Train Loss: 0.05656679719686508 Train BER 0.0025783784221857786,                  \n",
      " [36/500] At 0.0 dB, Train Loss: 0.001656838576309383 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 9.0389 minutes\n",
      "encoder learning rate: 7.20e-04, decoder learning rate: 7.20e-04\n",
      "[37/500] At -2.0 dB, Train Loss: 0.06311186403036118 Train BER 0.002713513560593128,                  \n",
      " [37/500] At 0.0 dB, Train Loss: 0.0005916297668591142 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9778 minutes\n",
      "encoder learning rate: 7.40e-04, decoder learning rate: 7.40e-04\n",
      "[38/500] At -2.0 dB, Train Loss: 0.0506448931992054 Train BER 0.002102702623233199,                  \n",
      " [38/500] At 0.0 dB, Train Loss: 0.0012801630655303597 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 9.0727 minutes\n",
      "encoder learning rate: 7.60e-04, decoder learning rate: 7.60e-04\n",
      "[39/500] At -2.0 dB, Train Loss: 0.04336849972605705 Train BER 0.0017243243055418134,                  \n",
      " [39/500] At 0.0 dB, Train Loss: 0.001143025467172265 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 9.0704 minutes\n",
      "encoder learning rate: 7.80e-04, decoder learning rate: 7.80e-04\n",
      "[40/500] At -2.0 dB, Train Loss: 0.05501951277256012 Train BER 0.0024108109064400196,                  \n",
      " [40/500] At 0.0 dB, Train Loss: 0.0005804534303024411 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0778 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[41/500] At -2.0 dB, Train Loss: 0.06798143684864044 Train BER 0.003064864780753851,                  \n",
      " [41/500] At 0.0 dB, Train Loss: 0.0011142889270558953 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 9.0284 minutes\n",
      "encoder learning rate: 8.20e-04, decoder learning rate: 8.20e-04\n",
      "[42/500] At -2.0 dB, Train Loss: 0.05293065682053566 Train BER 0.002270270371809602,                  \n",
      " [42/500] At 0.0 dB, Train Loss: 0.0022808685898780823 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 9.0237 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n",
      "[43/500] At -2.0 dB, Train Loss: 0.05130267143249512 Train BER 0.0022918919567018747,                  \n",
      " [43/500] At 0.0 dB, Train Loss: 0.003063397714868188 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 9.0959 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[44/500] At -2.0 dB, Train Loss: 0.053953636437654495 Train BER 0.0022594593465328217,                  \n",
      " [44/500] At 0.0 dB, Train Loss: 0.0025623920373618603 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 9.0137 minutes\n",
      "encoder learning rate: 8.80e-04, decoder learning rate: 8.80e-04\n",
      "[45/500] At -2.0 dB, Train Loss: 0.048423394560813904 Train BER 0.0021405406296253204,                  \n",
      " [45/500] At 0.0 dB, Train Loss: 0.004701873753219843 Train BER 0.00018918918794952333\n",
      "Time for one full iteration is 9.0749 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[46/500] At -2.0 dB, Train Loss: 0.07316530495882034 Train BER 0.0032486487179994583,                  \n",
      " [46/500] At 0.0 dB, Train Loss: 0.00232570874504745 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 9.0550 minutes\n",
      "encoder learning rate: 9.20e-04, decoder learning rate: 9.20e-04\n",
      "[47/500] At -2.0 dB, Train Loss: 0.07445729523897171 Train BER 0.0033945946488529444,                  \n",
      " [47/500] At 0.0 dB, Train Loss: 0.008096275851130486 Train BER 0.0003189189301338047\n",
      "Time for one full iteration is 9.0517 minutes\n",
      "encoder learning rate: 9.40e-04, decoder learning rate: 9.40e-04\n",
      "[48/500] At -2.0 dB, Train Loss: 0.056302767246961594 Train BER 0.0021837837994098663,                  \n",
      " [48/500] At 0.0 dB, Train Loss: 0.004666221793740988 Train BER 0.00012432433140929788\n",
      "Time for one full iteration is 9.0312 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[49/500] At -2.0 dB, Train Loss: 0.12436769902706146 Train BER 0.00506486464291811,                  \n",
      " [49/500] At 0.0 dB, Train Loss: 0.07651430368423462 Train BER 0.0029081080574542284\n",
      "Time for one full iteration is 9.0183 minutes\n",
      "encoder learning rate: 9.80e-04, decoder learning rate: 9.80e-04\n",
      "[50/500] At -2.0 dB, Train Loss: 0.2807499170303345 Train BER 0.011929729953408241,                  \n",
      " [50/500] At 0.0 dB, Train Loss: 0.2957901656627655 Train BER 0.011691891588270664\n",
      "Time for one full iteration is 9.0580 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[51/500] At -2.0 dB, Train Loss: 0.45281001925468445 Train BER 0.01998378336429596,                  \n",
      " [51/500] At 0.0 dB, Train Loss: 0.26847028732299805 Train BER 0.010183784179389477\n",
      "Time for one full iteration is 9.0138 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[52/500] At -2.0 dB, Train Loss: 0.5241811275482178 Train BER 0.023978378623723984,                  \n",
      " [52/500] At 0.0 dB, Train Loss: 0.35319605469703674 Train BER 0.014913513325154781\n",
      "Time for one full iteration is 9.0142 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[53/500] At -2.0 dB, Train Loss: 0.5891839861869812 Train BER 0.027578378096222878,                  \n",
      " [53/500] At 0.0 dB, Train Loss: 0.3872271180152893 Train BER 0.015843242406845093\n",
      "Time for one full iteration is 9.0140 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[54/500] At -2.0 dB, Train Loss: 0.5318633317947388 Train BER 0.024713514372706413,                  \n",
      " [54/500] At 0.0 dB, Train Loss: 0.4414610266685486 Train BER 0.01952432468533516\n",
      "Time for one full iteration is 9.0382 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[55/500] At -2.0 dB, Train Loss: 0.512352705001831 Train BER 0.024037837982177734,                  \n",
      " [55/500] At 0.0 dB, Train Loss: 0.6938180923461914 Train BER 0.02854594588279724\n",
      "Time for one full iteration is 9.0390 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[56/500] At -2.0 dB, Train Loss: 0.551437258720398 Train BER 0.02701621688902378,                  \n",
      " [56/500] At 0.0 dB, Train Loss: 0.5242838859558105 Train BER 0.023243242874741554\n",
      "Time for one full iteration is 9.0621 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/500] At -2.0 dB, Train Loss: 0.619063675403595 Train BER 0.029308108612895012,                  \n",
      " [57/500] At 0.0 dB, Train Loss: 0.5406001806259155 Train BER 0.0221891887485981\n",
      "Time for one full iteration is 9.0108 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[58/500] At -2.0 dB, Train Loss: 0.4388923943042755 Train BER 0.020643243566155434,                  \n",
      " [58/500] At 0.0 dB, Train Loss: 0.23344144225120544 Train BER 0.009421621449291706\n",
      "Time for one full iteration is 9.1039 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[59/500] At -2.0 dB, Train Loss: 0.30022260546684265 Train BER 0.013578378595411777,                  \n",
      " [59/500] At 0.0 dB, Train Loss: 0.10221028327941895 Train BER 0.0038702702149748802\n",
      "Time for one full iteration is 9.0417 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[60/500] At -2.0 dB, Train Loss: 0.16380874812602997 Train BER 0.007470270153135061,                  \n",
      " [60/500] At 0.0 dB, Train Loss: 0.03329867124557495 Train BER 0.0011405405821278691\n",
      "Time for one full iteration is 9.0241 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[61/500] At -2.0 dB, Train Loss: 0.16175058484077454 Train BER 0.007983784191310406,                  \n",
      " [61/500] At 0.0 dB, Train Loss: 0.03993641585111618 Train BER 0.0015891891671344638\n",
      "Time for one full iteration is 9.0660 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[62/500] At -2.0 dB, Train Loss: 0.09572429209947586 Train BER 0.004135135095566511,                  \n",
      " [62/500] At 0.0 dB, Train Loss: 0.0029717516154050827 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 9.0607 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[63/500] At -2.0 dB, Train Loss: 0.11275370419025421 Train BER 0.005113513674587011,                  \n",
      " [63/500] At 0.0 dB, Train Loss: 0.009735294617712498 Train BER 0.00035675676190294325\n",
      "Time for one full iteration is 9.0512 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[64/500] At -2.0 dB, Train Loss: 0.08930855989456177 Train BER 0.003891891799867153,                  \n",
      " [64/500] At 0.0 dB, Train Loss: 0.002429599640890956 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 9.0790 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[65/500] At -2.0 dB, Train Loss: 0.07786347717046738 Train BER 0.0039675673469901085,                  \n",
      " [65/500] At 0.0 dB, Train Loss: 0.0013546807458624244 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 9.0776 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[66/500] At -2.0 dB, Train Loss: 0.053558606654405594 Train BER 0.0023945944849401712,                  \n",
      " [66/500] At 0.0 dB, Train Loss: 0.0006598953623324633 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0035 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[67/500] At -2.0 dB, Train Loss: 0.05218377336859703 Train BER 0.002270270371809602,                  \n",
      " [67/500] At 0.0 dB, Train Loss: 0.0004942244850099087 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0104 minutes\n",
      "encoder learning rate: 9.96e-04, decoder learning rate: 9.96e-04\n",
      "[68/500] At -2.0 dB, Train Loss: 0.04705152288079262 Train BER 0.0020918918307870626,                  \n",
      " [68/500] At 0.0 dB, Train Loss: 0.0017062192782759666 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 9.0928 minutes\n",
      "encoder learning rate: 9.96e-04, decoder learning rate: 9.96e-04\n",
      "[69/500] At -2.0 dB, Train Loss: 0.05162353441119194 Train BER 0.0023297297302633524,                  \n",
      " [69/500] At 0.0 dB, Train Loss: 0.0013077626936137676 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 9.0723 minutes\n",
      "encoder learning rate: 9.96e-04, decoder learning rate: 9.96e-04\n",
      "[70/500] At -2.0 dB, Train Loss: 0.05324126407504082 Train BER 0.0024594594724476337,                  \n",
      " [70/500] At 0.0 dB, Train Loss: 0.0004134455230087042 Train BER 0.0\n",
      "Time for one full iteration is 9.0543 minutes\n",
      "encoder learning rate: 9.95e-04, decoder learning rate: 9.95e-04\n",
      "[71/500] At -2.0 dB, Train Loss: 0.049675557762384415 Train BER 0.002421621698886156,                  \n",
      " [71/500] At 0.0 dB, Train Loss: 0.0003784500004258007 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0682 minutes\n",
      "encoder learning rate: 9.95e-04, decoder learning rate: 9.95e-04\n",
      "[72/500] At -2.0 dB, Train Loss: 0.054123084992170334 Train BER 0.0025243242271244526,                  \n",
      " [72/500] At 0.0 dB, Train Loss: 0.0006137859309092164 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0480 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[73/500] At -2.0 dB, Train Loss: 0.04509846866130829 Train BER 0.002118918811902404,                  \n",
      " [73/500] At 0.0 dB, Train Loss: 0.00040975690353661776 Train BER 0.0\n",
      "Time for one full iteration is 8.8255 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[74/500] At -2.0 dB, Train Loss: 0.05459556728601456 Train BER 0.0026702701579779387,                  \n",
      " [74/500] At 0.0 dB, Train Loss: 0.0005612800014205277 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9038 minutes\n",
      "encoder learning rate: 9.93e-04, decoder learning rate: 9.93e-04\n",
      "[75/500] At -2.0 dB, Train Loss: 0.05522763729095459 Train BER 0.002567567629739642,                  \n",
      " [75/500] At 0.0 dB, Train Loss: 0.0003712004399858415 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9426 minutes\n",
      "encoder learning rate: 9.92e-04, decoder learning rate: 9.92e-04\n",
      "[76/500] At -2.0 dB, Train Loss: 0.040077224373817444 Train BER 0.0018972973339259624,                  \n",
      " [76/500] At 0.0 dB, Train Loss: 0.0010679446859285235 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.9300 minutes\n",
      "encoder learning rate: 9.92e-04, decoder learning rate: 9.92e-04\n",
      "[77/500] At -2.0 dB, Train Loss: 0.060507882386446 Train BER 0.0027243243530392647,                  \n",
      " [77/500] At 0.0 dB, Train Loss: 0.000909475376829505 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0243 minutes\n",
      "encoder learning rate: 9.91e-04, decoder learning rate: 9.91e-04\n",
      "[78/500] At -2.0 dB, Train Loss: 0.050062146037817 Train BER 0.002102702623233199,                  \n",
      " [78/500] At 0.0 dB, Train Loss: 0.000571885728277266 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9809 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[79/500] At -2.0 dB, Train Loss: 0.04746871814131737 Train BER 0.0020702702458947897,                  \n",
      " [79/500] At 0.0 dB, Train Loss: 0.0003175096644554287 Train BER 0.0\n",
      "Time for one full iteration is 8.9810 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[80/500] At -2.0 dB, Train Loss: 0.04230942577123642 Train BER 0.0018324324628338218,                  \n",
      " [80/500] At 0.0 dB, Train Loss: 0.0007647582096979022 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0176 minutes\n",
      "encoder learning rate: 9.89e-04, decoder learning rate: 9.89e-04\n",
      "[81/500] At -2.0 dB, Train Loss: 0.04643160104751587 Train BER 0.002032432472333312,                  \n",
      " [81/500] At 0.0 dB, Train Loss: 0.0003214891767129302 Train BER 0.0\n",
      "Time for one full iteration is 8.9926 minutes\n",
      "encoder learning rate: 9.88e-04, decoder learning rate: 9.88e-04\n",
      "[82/500] At -2.0 dB, Train Loss: 0.04077282175421715 Train BER 0.001767567591741681,                  \n",
      " [82/500] At 0.0 dB, Train Loss: 0.00031228939769789577 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9377 minutes\n",
      "encoder learning rate: 9.88e-04, decoder learning rate: 9.88e-04\n",
      "[83/500] At -2.0 dB, Train Loss: 0.03764808550477028 Train BER 0.001643243245780468,                  \n",
      " [83/500] At 0.0 dB, Train Loss: 0.00019423912453930825 Train BER 0.0\n",
      "Time for one full iteration is 8.9982 minutes\n",
      "encoder learning rate: 9.87e-04, decoder learning rate: 9.87e-04\n",
      "[84/500] At -2.0 dB, Train Loss: 0.03895562142133713 Train BER 0.0016270270571112633,                  \n",
      " [84/500] At 0.0 dB, Train Loss: 0.0003988407552242279 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9790 minutes\n",
      "encoder learning rate: 9.86e-04, decoder learning rate: 9.86e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/500] At -2.0 dB, Train Loss: 0.035915642976760864 Train BER 0.0015837837709113955,                  \n",
      " [85/500] At 0.0 dB, Train Loss: 0.00030000857077538967 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9699 minutes\n",
      "encoder learning rate: 9.85e-04, decoder learning rate: 9.85e-04\n",
      "[86/500] At -2.0 dB, Train Loss: 0.04497091472148895 Train BER 0.0020000000949949026,                  \n",
      " [86/500] At 0.0 dB, Train Loss: 0.0003743644047062844 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0670 minutes\n",
      "encoder learning rate: 9.84e-04, decoder learning rate: 9.84e-04\n",
      "[87/500] At -2.0 dB, Train Loss: 0.0375279039144516 Train BER 0.001713513513095677,                  \n",
      " [87/500] At 0.0 dB, Train Loss: 0.0002523459552321583 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0083 minutes\n",
      "encoder learning rate: 9.83e-04, decoder learning rate: 9.83e-04\n",
      "[88/500] At -2.0 dB, Train Loss: 0.028495168313384056 Train BER 0.0011297296732664108,                  \n",
      " [88/500] At 0.0 dB, Train Loss: 0.0002577916893642396 Train BER 0.0\n",
      "Time for one full iteration is 8.9314 minutes\n",
      "encoder learning rate: 9.83e-04, decoder learning rate: 9.83e-04\n",
      "[89/500] At -2.0 dB, Train Loss: 0.04706758260726929 Train BER 0.0021837837994098663,                  \n",
      " [89/500] At 0.0 dB, Train Loss: 0.00022773505770601332 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.1106 minutes\n",
      "encoder learning rate: 9.82e-04, decoder learning rate: 9.82e-04\n",
      "[90/500] At -2.0 dB, Train Loss: 0.039726659655570984 Train BER 0.001691891928203404,                  \n",
      " [90/500] At 0.0 dB, Train Loss: 0.00029306893702596426 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0195 minutes\n",
      "encoder learning rate: 9.81e-04, decoder learning rate: 9.81e-04\n",
      "[91/500] At -2.0 dB, Train Loss: 0.04725547507405281 Train BER 0.0023837836924940348,                  \n",
      " [91/500] At 0.0 dB, Train Loss: 0.00027178763411939144 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.1353 minutes\n",
      "encoder learning rate: 9.80e-04, decoder learning rate: 9.80e-04\n",
      "[92/500] At -2.0 dB, Train Loss: 0.03557473048567772 Train BER 0.0016810811357572675,                  \n",
      " [92/500] At 0.0 dB, Train Loss: 0.0005044763092882931 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0546 minutes\n",
      "encoder learning rate: 9.79e-04, decoder learning rate: 9.79e-04\n",
      "[93/500] At -2.0 dB, Train Loss: 0.05211191624403 Train BER 0.0022918919567018747,                  \n",
      " [93/500] At 0.0 dB, Train Loss: 0.0004886185633949935 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0016 minutes\n",
      "encoder learning rate: 9.78e-04, decoder learning rate: 9.78e-04\n",
      "[94/500] At -2.0 dB, Train Loss: 0.038826268166303635 Train BER 0.0017081081168726087,                  \n",
      " [94/500] At 0.0 dB, Train Loss: 0.0002909702598117292 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0815 minutes\n",
      "encoder learning rate: 9.77e-04, decoder learning rate: 9.77e-04\n",
      "[95/500] At -2.0 dB, Train Loss: 0.038927607238292694 Train BER 0.0016864865319803357,                  \n",
      " [95/500] At 0.0 dB, Train Loss: 0.0005378558998927474 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 9.0520 minutes\n",
      "encoder learning rate: 9.76e-04, decoder learning rate: 9.76e-04\n",
      "[96/500] At -2.0 dB, Train Loss: 0.03232567012310028 Train BER 0.00139999995008111,                  \n",
      " [96/500] At 0.0 dB, Train Loss: 0.00010073552402900532 Train BER 0.0\n",
      "Time for one full iteration is 9.0532 minutes\n",
      "encoder learning rate: 9.74e-04, decoder learning rate: 9.74e-04\n",
      "[97/500] At -2.0 dB, Train Loss: 0.04536658525466919 Train BER 0.0021837837994098663,                  \n",
      " [97/500] At 0.0 dB, Train Loss: 0.00028256213408894837 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.1115 minutes\n",
      "encoder learning rate: 9.73e-04, decoder learning rate: 9.73e-04\n",
      "[98/500] At -2.0 dB, Train Loss: 0.04752349853515625 Train BER 0.0022162161767482758,                  \n",
      " [98/500] At 0.0 dB, Train Loss: 0.0004474020970519632 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0525 minutes\n",
      "encoder learning rate: 9.72e-04, decoder learning rate: 9.72e-04\n",
      "[99/500] At -2.0 dB, Train Loss: 0.040004145354032516 Train BER 0.0018972973339259624,                  \n",
      " [99/500] At 0.0 dB, Train Loss: 6.21525541646406e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0513 minutes\n",
      "encoder learning rate: 9.71e-04, decoder learning rate: 9.71e-04\n",
      "[100/500] At -2.0 dB, Train Loss: 0.04361634701490402 Train BER 0.002199999988079071,                  \n",
      " [100/500] At 0.0 dB, Train Loss: 7.902683137217537e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7194 minutes\n",
      "encoder learning rate: 9.70e-04, decoder learning rate: 9.70e-04\n",
      "[101/500] At -2.0 dB, Train Loss: 0.03441641107201576 Train BER 0.001572972978465259,                  \n",
      " [101/500] At 0.0 dB, Train Loss: 0.00036689016269519925 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.6039 minutes\n",
      "encoder learning rate: 9.69e-04, decoder learning rate: 9.69e-04\n",
      "[102/500] At -2.0 dB, Train Loss: 0.03760861977934837 Train BER 0.0017297297017648816,                  \n",
      " [102/500] At 0.0 dB, Train Loss: 0.00029019531211815774 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.7398 minutes\n",
      "encoder learning rate: 9.67e-04, decoder learning rate: 9.67e-04\n",
      "[103/500] At -2.0 dB, Train Loss: 0.029819311574101448 Train BER 0.0012756757205352187,                  \n",
      " [103/500] At 0.0 dB, Train Loss: 0.000164662953466177 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7000 minutes\n",
      "encoder learning rate: 9.66e-04, decoder learning rate: 9.66e-04\n",
      "[104/500] At -2.0 dB, Train Loss: 0.03667234256863594 Train BER 0.001621621660888195,                  \n",
      " [104/500] At 0.0 dB, Train Loss: 9.171655983664095e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7191 minutes\n",
      "encoder learning rate: 9.65e-04, decoder learning rate: 9.65e-04\n",
      "[105/500] At -2.0 dB, Train Loss: 0.046388428658246994 Train BER 0.002443243283778429,                  \n",
      " [105/500] At 0.0 dB, Train Loss: 0.00022707824246026576 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.6061 minutes\n",
      "encoder learning rate: 9.64e-04, decoder learning rate: 9.64e-04\n",
      "[106/500] At -2.0 dB, Train Loss: 0.033711448311805725 Train BER 0.0015243242960423231,                  \n",
      " [106/500] At 0.0 dB, Train Loss: 0.0001376024738419801 Train BER 0.0\n",
      "Time for one full iteration is 8.7187 minutes\n",
      "encoder learning rate: 9.62e-04, decoder learning rate: 9.62e-04\n",
      "[107/500] At -2.0 dB, Train Loss: 0.03980419039726257 Train BER 0.0018324324628338218,                  \n",
      " [107/500] At 0.0 dB, Train Loss: 0.00042964736348949373 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.6320 minutes\n",
      "encoder learning rate: 9.61e-04, decoder learning rate: 9.61e-04\n",
      "[108/500] At -2.0 dB, Train Loss: 0.03992844745516777 Train BER 0.0019675674848258495,                  \n",
      " [108/500] At 0.0 dB, Train Loss: 0.0002949205518234521 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6693 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[109/500] At -2.0 dB, Train Loss: 0.03645801171660423 Train BER 0.001789189176633954,                  \n",
      " [109/500] At 0.0 dB, Train Loss: 0.0002117510448442772 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7189 minutes\n",
      "encoder learning rate: 9.58e-04, decoder learning rate: 9.58e-04\n",
      "[110/500] At -2.0 dB, Train Loss: 0.033812835812568665 Train BER 0.0017297297017648816,                  \n",
      " [110/500] At 0.0 dB, Train Loss: 0.00017326809756923467 Train BER 0.0\n",
      "Time for one full iteration is 8.7063 minutes\n",
      "encoder learning rate: 9.57e-04, decoder learning rate: 9.57e-04\n",
      "[111/500] At -2.0 dB, Train Loss: 0.03921297937631607 Train BER 0.0017729729879647493,                  \n",
      " [111/500] At 0.0 dB, Train Loss: 0.00026108123711310327 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.6315 minutes\n",
      "encoder learning rate: 9.55e-04, decoder learning rate: 9.55e-04\n",
      "[112/500] At -2.0 dB, Train Loss: 0.03264092653989792 Train BER 0.0014648648211732507,                  \n",
      " [112/500] At 0.0 dB, Train Loss: 0.0002350555150769651 Train BER 0.0\n",
      "Time for one full iteration is 8.6299 minutes\n",
      "encoder learning rate: 9.54e-04, decoder learning rate: 9.54e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/500] At -2.0 dB, Train Loss: 0.04053599759936333 Train BER 0.0019027027301490307,                  \n",
      " [113/500] At 0.0 dB, Train Loss: 0.00048584621981717646 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.6912 minutes\n",
      "encoder learning rate: 9.52e-04, decoder learning rate: 9.52e-04\n",
      "[114/500] At -2.0 dB, Train Loss: 0.030959071591496468 Train BER 0.001378378365188837,                  \n",
      " [114/500] At 0.0 dB, Train Loss: 0.000845771050080657 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.6632 minutes\n",
      "encoder learning rate: 9.51e-04, decoder learning rate: 9.51e-04\n",
      "[115/500] At -2.0 dB, Train Loss: 0.03662770614027977 Train BER 0.0017189189093187451,                  \n",
      " [115/500] At 0.0 dB, Train Loss: 0.0001867402606876567 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6507 minutes\n",
      "encoder learning rate: 9.49e-04, decoder learning rate: 9.49e-04\n",
      "[116/500] At -2.0 dB, Train Loss: 0.03270968422293663 Train BER 0.0014432432362809777,                  \n",
      " [116/500] At 0.0 dB, Train Loss: 0.0011177411070093513 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.7445 minutes\n",
      "encoder learning rate: 9.48e-04, decoder learning rate: 9.48e-04\n",
      "[117/500] At -2.0 dB, Train Loss: 0.03770378232002258 Train BER 0.001886486541479826,                  \n",
      " [117/500] At 0.0 dB, Train Loss: 0.00024153687991201878 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6394 minutes\n",
      "encoder learning rate: 9.46e-04, decoder learning rate: 9.46e-04\n",
      "[118/500] At -2.0 dB, Train Loss: 0.038619570434093475 Train BER 0.0017459458904340863,                  \n",
      " [118/500] At 0.0 dB, Train Loss: 0.00032811303390190005 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.6573 minutes\n",
      "encoder learning rate: 9.45e-04, decoder learning rate: 9.45e-04\n",
      "[119/500] At -2.0 dB, Train Loss: 0.03168390318751335 Train BER 0.0014540540287271142,                  \n",
      " [119/500] At 0.0 dB, Train Loss: 0.00018498647841624916 Train BER 0.0\n",
      "Time for one full iteration is 8.6679 minutes\n",
      "encoder learning rate: 9.43e-04, decoder learning rate: 9.43e-04\n",
      "[120/500] At -2.0 dB, Train Loss: 0.03638171777129173 Train BER 0.0017945945728570223,                  \n",
      " [120/500] At 0.0 dB, Train Loss: 0.0002619277802295983 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7027 minutes\n",
      "encoder learning rate: 9.42e-04, decoder learning rate: 9.42e-04\n",
      "[121/500] At -2.0 dB, Train Loss: 0.036596205085515976 Train BER 0.0018324324628338218,                  \n",
      " [121/500] At 0.0 dB, Train Loss: 0.00013238671817816794 Train BER 0.0\n",
      "Time for one full iteration is 8.6631 minutes\n",
      "encoder learning rate: 9.40e-04, decoder learning rate: 9.40e-04\n",
      "[122/500] At -2.0 dB, Train Loss: 0.030103018507361412 Train BER 0.0013891891576349735,                  \n",
      " [122/500] At 0.0 dB, Train Loss: 0.00012329044693615288 Train BER 0.0\n",
      "Time for one full iteration is 8.6442 minutes\n",
      "encoder learning rate: 9.38e-04, decoder learning rate: 9.38e-04\n",
      "[123/500] At -2.0 dB, Train Loss: 0.03658977895975113 Train BER 0.001881081028841436,                  \n",
      " [123/500] At 0.0 dB, Train Loss: 0.001165977562777698 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.6309 minutes\n",
      "encoder learning rate: 9.37e-04, decoder learning rate: 9.37e-04\n",
      "[124/500] At -2.0 dB, Train Loss: 0.036191754043102264 Train BER 0.0016540540382266045,                  \n",
      " [124/500] At 0.0 dB, Train Loss: 0.00021749705774709582 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.6421 minutes\n",
      "encoder learning rate: 9.35e-04, decoder learning rate: 9.35e-04\n",
      "[125/500] At -2.0 dB, Train Loss: 0.0383344404399395 Train BER 0.0017729729879647493,                  \n",
      " [125/500] At 0.0 dB, Train Loss: 0.00045633831177838147 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.6882 minutes\n",
      "encoder learning rate: 9.33e-04, decoder learning rate: 9.33e-04\n",
      "[126/500] At -2.0 dB, Train Loss: 0.036807749420404434 Train BER 0.0018216216703876853,                  \n",
      " [126/500] At 0.0 dB, Train Loss: 0.00034912952105514705 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.7263 minutes\n",
      "encoder learning rate: 9.31e-04, decoder learning rate: 9.31e-04\n",
      "[127/500] At -2.0 dB, Train Loss: 0.03441740199923515 Train BER 0.0018432432552799582,                  \n",
      " [127/500] At 0.0 dB, Train Loss: 0.00020762733765877783 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.6869 minutes\n",
      "encoder learning rate: 9.30e-04, decoder learning rate: 9.30e-04\n",
      "[128/500] At -2.0 dB, Train Loss: 0.045203790068626404 Train BER 0.002102702623233199,                  \n",
      " [128/500] At 0.0 dB, Train Loss: 7.050995918689296e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6244 minutes\n",
      "encoder learning rate: 9.28e-04, decoder learning rate: 9.28e-04\n",
      "[129/500] At -2.0 dB, Train Loss: 0.03899295628070831 Train BER 0.0020432432647794485,                  \n",
      " [129/500] At 0.0 dB, Train Loss: 7.735074177617207e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6046 minutes\n",
      "encoder learning rate: 9.26e-04, decoder learning rate: 9.26e-04\n",
      "[130/500] At -2.0 dB, Train Loss: 0.03693367913365364 Train BER 0.001664864830672741,                  \n",
      " [130/500] At 0.0 dB, Train Loss: 0.0008636616403236985 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.5707 minutes\n",
      "encoder learning rate: 9.24e-04, decoder learning rate: 9.24e-04\n",
      "[131/500] At -2.0 dB, Train Loss: 0.03431064262986183 Train BER 0.0017999999690800905,                  \n",
      " [131/500] At 0.0 dB, Train Loss: 6.044310066499747e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6699 minutes\n",
      "encoder learning rate: 9.22e-04, decoder learning rate: 9.22e-04\n",
      "[132/500] At -2.0 dB, Train Loss: 0.043154988437891006 Train BER 0.0021459460258483887,                  \n",
      " [132/500] At 0.0 dB, Train Loss: 0.00011539889965206385 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7098 minutes\n",
      "encoder learning rate: 9.20e-04, decoder learning rate: 9.20e-04\n",
      "[133/500] At -2.0 dB, Train Loss: 0.029129739850759506 Train BER 0.0012162162456661463,                  \n",
      " [133/500] At 0.0 dB, Train Loss: 0.0004032031283713877 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.6395 minutes\n",
      "encoder learning rate: 9.18e-04, decoder learning rate: 9.18e-04\n",
      "[134/500] At -2.0 dB, Train Loss: 0.034237004816532135 Train BER 0.0015783783746883273,                  \n",
      " [134/500] At 0.0 dB, Train Loss: 0.0001277469127671793 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6147 minutes\n",
      "encoder learning rate: 9.17e-04, decoder learning rate: 9.17e-04\n",
      "[135/500] At -2.0 dB, Train Loss: 0.03875213488936424 Train BER 0.001810810761526227,                  \n",
      " [135/500] At 0.0 dB, Train Loss: 0.0004331188683863729 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.6252 minutes\n",
      "encoder learning rate: 9.15e-04, decoder learning rate: 9.15e-04\n",
      "[136/500] At -2.0 dB, Train Loss: 0.031761471182107925 Train BER 0.0014864865224808455,                  \n",
      " [136/500] At 0.0 dB, Train Loss: 0.0004447439860086888 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.6606 minutes\n",
      "encoder learning rate: 9.13e-04, decoder learning rate: 9.13e-04\n",
      "[137/500] At -2.0 dB, Train Loss: 0.03806105628609657 Train BER 0.0018918919377028942,                  \n",
      " [137/500] At 0.0 dB, Train Loss: 0.0005886015715077519 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.5893 minutes\n",
      "encoder learning rate: 9.11e-04, decoder learning rate: 9.11e-04\n",
      "[138/500] At -2.0 dB, Train Loss: 0.040695395320653915 Train BER 0.0020864864345639944,                  \n",
      " [138/500] At 0.0 dB, Train Loss: 0.00038082373794168234 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.6701 minutes\n",
      "encoder learning rate: 9.09e-04, decoder learning rate: 9.09e-04\n",
      "[139/500] At -2.0 dB, Train Loss: 0.04231349378824234 Train BER 0.0020000000949949026,                  \n",
      " [139/500] At 0.0 dB, Train Loss: 0.000974318478256464 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.6683 minutes\n",
      "encoder learning rate: 9.07e-04, decoder learning rate: 9.07e-04\n",
      "[140/500] At -2.0 dB, Train Loss: 0.04043576866388321 Train BER 0.0017621621955186129,                  \n",
      " [140/500] At 0.0 dB, Train Loss: 0.0017535657389089465 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.5875 minutes\n",
      "encoder learning rate: 9.05e-04, decoder learning rate: 9.05e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/500] At -2.0 dB, Train Loss: 0.06098247319459915 Train BER 0.0028162163216620684,                  \n",
      " [141/500] At 0.0 dB, Train Loss: 0.0026327534578740597 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.6491 minutes\n",
      "encoder learning rate: 9.03e-04, decoder learning rate: 9.03e-04\n",
      "[142/500] At -2.0 dB, Train Loss: 0.06477758288383484 Train BER 0.0031567567493766546,                  \n",
      " [142/500] At 0.0 dB, Train Loss: 0.003813195275142789 Train BER 0.00014054053463041782\n",
      "Time for one full iteration is 8.6673 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[143/500] At -2.0 dB, Train Loss: 0.06528784334659576 Train BER 0.002924324246123433,                  \n",
      " [143/500] At 0.0 dB, Train Loss: 0.0014880377566441894 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.6776 minutes\n",
      "encoder learning rate: 8.98e-04, decoder learning rate: 8.98e-04\n",
      "[144/500] At -2.0 dB, Train Loss: 0.05326071381568909 Train BER 0.0026162161957472563,                  \n",
      " [144/500] At 0.0 dB, Train Loss: 0.0004792431427631527 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.6147 minutes\n",
      "encoder learning rate: 8.96e-04, decoder learning rate: 8.96e-04\n",
      "[145/500] At -2.0 dB, Train Loss: 0.03493797034025192 Train BER 0.0016108108684420586,                  \n",
      " [145/500] At 0.0 dB, Train Loss: 5.181556480238214e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5774 minutes\n",
      "encoder learning rate: 8.94e-04, decoder learning rate: 8.94e-04\n",
      "[146/500] At -2.0 dB, Train Loss: 0.03250889852643013 Train BER 0.001572972978465259,                  \n",
      " [146/500] At 0.0 dB, Train Loss: 0.00010761323210317641 Train BER 0.0\n",
      "Time for one full iteration is 8.6107 minutes\n",
      "encoder learning rate: 8.92e-04, decoder learning rate: 8.92e-04\n",
      "[147/500] At -2.0 dB, Train Loss: 0.035608742386102676 Train BER 0.0017081081168726087,                  \n",
      " [147/500] At 0.0 dB, Train Loss: 0.00020348455291241407 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6477 minutes\n",
      "encoder learning rate: 8.90e-04, decoder learning rate: 8.90e-04\n",
      "[148/500] At -2.0 dB, Train Loss: 0.033852994441986084 Train BER 0.0018054053653031588,                  \n",
      " [148/500] At 0.0 dB, Train Loss: 0.0001302634773310274 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.4204 minutes\n",
      "encoder learning rate: 8.88e-04, decoder learning rate: 8.88e-04\n",
      "[149/500] At -2.0 dB, Train Loss: 0.02729131281375885 Train BER 0.0012918919092044234,                  \n",
      " [149/500] At 0.0 dB, Train Loss: 3.874471804010682e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4503 minutes\n",
      "encoder learning rate: 8.85e-04, decoder learning rate: 8.85e-04\n",
      "[150/500] At -2.0 dB, Train Loss: 0.039000675082206726 Train BER 0.001978378277271986,                  \n",
      " [150/500] At 0.0 dB, Train Loss: 0.0009252859745174646 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.3677 minutes\n",
      "encoder learning rate: 8.83e-04, decoder learning rate: 8.83e-04\n",
      "[151/500] At -2.0 dB, Train Loss: 0.03684690222144127 Train BER 0.0017459458904340863,                  \n",
      " [151/500] At 0.0 dB, Train Loss: 7.415876461891457e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3926 minutes\n",
      "encoder learning rate: 8.81e-04, decoder learning rate: 8.81e-04\n",
      "[152/500] At -2.0 dB, Train Loss: 0.030956700444221497 Train BER 0.0015135135035961866,                  \n",
      " [152/500] At 0.0 dB, Train Loss: 0.00038028068956919014 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.4213 minutes\n",
      "encoder learning rate: 8.79e-04, decoder learning rate: 8.79e-04\n",
      "[153/500] At -2.0 dB, Train Loss: 0.02901558391749859 Train BER 0.0013621621765196323,                  \n",
      " [153/500] At 0.0 dB, Train Loss: 0.00011733804421965033 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.3676 minutes\n",
      "encoder learning rate: 8.76e-04, decoder learning rate: 8.76e-04\n",
      "[154/500] At -2.0 dB, Train Loss: 0.025313569232821465 Train BER 0.000994594651274383,                  \n",
      " [154/500] At 0.0 dB, Train Loss: 0.00015893216186668724 Train BER 0.0\n",
      "Time for one full iteration is 8.3559 minutes\n",
      "encoder learning rate: 8.74e-04, decoder learning rate: 8.74e-04\n",
      "[155/500] At -2.0 dB, Train Loss: 0.024257853627204895 Train BER 0.0010810811072587967,                  \n",
      " [155/500] At 0.0 dB, Train Loss: 0.0001312240638071671 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.3969 minutes\n",
      "encoder learning rate: 8.72e-04, decoder learning rate: 8.72e-04\n",
      "[156/500] At -2.0 dB, Train Loss: 0.0345751978456974 Train BER 0.0017297297017648816,                  \n",
      " [156/500] At 0.0 dB, Train Loss: 0.00017956897499971092 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7198 minutes\n",
      "encoder learning rate: 8.69e-04, decoder learning rate: 8.69e-04\n",
      "[157/500] At -2.0 dB, Train Loss: 0.029680656269192696 Train BER 0.0013891891576349735,                  \n",
      " [157/500] At 0.0 dB, Train Loss: 0.00020804119412787259 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.8990 minutes\n",
      "encoder learning rate: 8.67e-04, decoder learning rate: 8.67e-04\n",
      "[158/500] At -2.0 dB, Train Loss: 0.03832637146115303 Train BER 0.0020378378685563803,                  \n",
      " [158/500] At 0.0 dB, Train Loss: 7.150691089918837e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5790 minutes\n",
      "encoder learning rate: 8.65e-04, decoder learning rate: 8.65e-04\n",
      "[159/500] At -2.0 dB, Train Loss: 0.03246169909834862 Train BER 0.001475675730034709,                  \n",
      " [159/500] At 0.0 dB, Train Loss: 5.3412499255500734e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9637 minutes\n",
      "encoder learning rate: 8.62e-04, decoder learning rate: 8.62e-04\n",
      "[160/500] At -2.0 dB, Train Loss: 0.028814144432544708 Train BER 0.0014378378400579095,                  \n",
      " [160/500] At 0.0 dB, Train Loss: 0.0002443551493342966 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0246 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[161/500] At -2.0 dB, Train Loss: 0.0308682881295681 Train BER 0.0014324324438348413,                  \n",
      " [161/500] At 0.0 dB, Train Loss: 0.00037177192280068994 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0837 minutes\n",
      "encoder learning rate: 8.57e-04, decoder learning rate: 8.57e-04\n",
      "[162/500] At -2.0 dB, Train Loss: 0.035152617841959 Train BER 0.0016864865319803357,                  \n",
      " [162/500] At 0.0 dB, Train Loss: 9.6903633675538e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0607 minutes\n",
      "encoder learning rate: 8.55e-04, decoder learning rate: 8.55e-04\n",
      "[163/500] At -2.0 dB, Train Loss: 0.035563502460718155 Train BER 0.0017459458904340863,                  \n",
      " [163/500] At 0.0 dB, Train Loss: 0.0002269229298690334 Train BER 0.0\n",
      "Time for one full iteration is 9.0155 minutes\n",
      "encoder learning rate: 8.52e-04, decoder learning rate: 8.52e-04\n",
      "[164/500] At -2.0 dB, Train Loss: 0.03240736573934555 Train BER 0.0016162162646651268,                  \n",
      " [164/500] At 0.0 dB, Train Loss: 0.00024642710923217237 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.1108 minutes\n",
      "encoder learning rate: 8.50e-04, decoder learning rate: 8.50e-04\n",
      "[165/500] At -2.0 dB, Train Loss: 0.035152096301317215 Train BER 0.001789189176633954,                  \n",
      " [165/500] At 0.0 dB, Train Loss: 0.0002382825332460925 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0281 minutes\n",
      "encoder learning rate: 8.47e-04, decoder learning rate: 8.47e-04\n",
      "[166/500] At -2.0 dB, Train Loss: 0.03925895690917969 Train BER 0.0020162162836641073,                  \n",
      " [166/500] At 0.0 dB, Train Loss: 0.0005278901080600917 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0192 minutes\n",
      "encoder learning rate: 8.45e-04, decoder learning rate: 8.45e-04\n",
      "[167/500] At -2.0 dB, Train Loss: 0.04596022889018059 Train BER 0.0022324323654174805,                  \n",
      " [167/500] At 0.0 dB, Train Loss: 0.0003026213962584734 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0020 minutes\n",
      "encoder learning rate: 8.42e-04, decoder learning rate: 8.42e-04\n",
      "[168/500] At -2.0 dB, Train Loss: 0.039584096521139145 Train BER 0.001810810761526227,                  \n",
      " [168/500] At 0.0 dB, Train Loss: 0.000740753544960171 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 9.1457 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169/500] At -2.0 dB, Train Loss: 0.040380243211984634 Train BER 0.002005405491217971,                  \n",
      " [169/500] At 0.0 dB, Train Loss: 0.0003002540033776313 Train BER 0.0\n",
      "Time for one full iteration is 9.0461 minutes\n",
      "encoder learning rate: 8.37e-04, decoder learning rate: 8.37e-04\n",
      "[170/500] At -2.0 dB, Train Loss: 0.05009594187140465 Train BER 0.002513513434678316,                  \n",
      " [170/500] At 0.0 dB, Train Loss: 0.0010665880981832743 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9978 minutes\n",
      "encoder learning rate: 8.35e-04, decoder learning rate: 8.35e-04\n",
      "[171/500] At -2.0 dB, Train Loss: 0.05201005935668945 Train BER 0.0024594594724476337,                  \n",
      " [171/500] At 0.0 dB, Train Loss: 0.0020856717601418495 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.9927 minutes\n",
      "encoder learning rate: 8.32e-04, decoder learning rate: 8.32e-04\n",
      "[172/500] At -2.0 dB, Train Loss: 0.08050832152366638 Train BER 0.003978378605097532,                  \n",
      " [172/500] At 0.0 dB, Train Loss: 0.025589127093553543 Train BER 0.0010972972959280014\n",
      "Time for one full iteration is 8.7457 minutes\n",
      "encoder learning rate: 8.30e-04, decoder learning rate: 8.30e-04\n",
      "[173/500] At -2.0 dB, Train Loss: 0.2193491905927658 Train BER 0.011497297324240208,                  \n",
      " [173/500] At 0.0 dB, Train Loss: 0.16242298483848572 Train BER 0.007816215977072716\n",
      "Time for one full iteration is 8.7943 minutes\n",
      "encoder learning rate: 8.27e-04, decoder learning rate: 8.27e-04\n",
      "[174/500] At -2.0 dB, Train Loss: 0.4195314943790436 Train BER 0.02178378403186798,                  \n",
      " [174/500] At 0.0 dB, Train Loss: 0.28554269671440125 Train BER 0.013297297060489655\n",
      "Time for one full iteration is 8.6269 minutes\n",
      "encoder learning rate: 8.24e-04, decoder learning rate: 8.24e-04\n",
      "[175/500] At -2.0 dB, Train Loss: 0.45234426856040955 Train BER 0.021827027201652527,                  \n",
      " [175/500] At 0.0 dB, Train Loss: 0.2879358232021332 Train BER 0.012740540318191051\n",
      "Time for one full iteration is 8.8638 minutes\n",
      "encoder learning rate: 8.22e-04, decoder learning rate: 8.22e-04\n",
      "[176/500] At -2.0 dB, Train Loss: 0.3747612535953522 Train BER 0.018562162294983864,                  \n",
      " [176/500] At 0.0 dB, Train Loss: 0.5601538419723511 Train BER 0.02353513427078724\n",
      "Time for one full iteration is 8.6400 minutes\n",
      "encoder learning rate: 8.19e-04, decoder learning rate: 8.19e-04\n",
      "[177/500] At -2.0 dB, Train Loss: 0.4646241068840027 Train BER 0.02417837828397751,                  \n",
      " [177/500] At 0.0 dB, Train Loss: 0.3468204736709595 Train BER 0.015875674784183502\n",
      "Time for one full iteration is 8.6421 minutes\n",
      "encoder learning rate: 8.16e-04, decoder learning rate: 8.16e-04\n",
      "[178/500] At -2.0 dB, Train Loss: 0.3827165961265564 Train BER 0.018686486408114433,                  \n",
      " [178/500] At 0.0 dB, Train Loss: 0.26020628213882446 Train BER 0.011054053902626038\n",
      "Time for one full iteration is 8.6671 minutes\n",
      "encoder learning rate: 8.13e-04, decoder learning rate: 8.13e-04\n",
      "[179/500] At -2.0 dB, Train Loss: 0.32950660586357117 Train BER 0.015978379175066948,                  \n",
      " [179/500] At 0.0 dB, Train Loss: 0.1707032471895218 Train BER 0.006962161976844072\n",
      "Time for one full iteration is 8.6571 minutes\n",
      "encoder learning rate: 8.11e-04, decoder learning rate: 8.11e-04\n",
      "[180/500] At -2.0 dB, Train Loss: 0.29847532510757446 Train BER 0.014821621589362621,                  \n",
      " [180/500] At 0.0 dB, Train Loss: 0.3035571575164795 Train BER 0.009610811248421669\n",
      "Time for one full iteration is 8.7044 minutes\n",
      "encoder learning rate: 8.08e-04, decoder learning rate: 8.08e-04\n",
      "[181/500] At -2.0 dB, Train Loss: 0.17541484534740448 Train BER 0.00839999970048666,                  \n",
      " [181/500] At 0.0 dB, Train Loss: 0.04424203559756279 Train BER 0.0017945945728570223\n",
      "Time for one full iteration is 8.6298 minutes\n",
      "encoder learning rate: 8.05e-04, decoder learning rate: 8.05e-04\n",
      "[182/500] At -2.0 dB, Train Loss: 0.18013018369674683 Train BER 0.009167567826807499,                  \n",
      " [182/500] At 0.0 dB, Train Loss: 0.19571411609649658 Train BER 0.0069189188070595264\n",
      "Time for one full iteration is 8.7619 minutes\n",
      "encoder learning rate: 8.02e-04, decoder learning rate: 8.02e-04\n",
      "[183/500] At -2.0 dB, Train Loss: 0.22195816040039062 Train BER 0.011437837965786457,                  \n",
      " [183/500] At 0.0 dB, Train Loss: 0.06128301844000816 Train BER 0.0014864865224808455\n",
      "Time for one full iteration is 8.7345 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[184/500] At -2.0 dB, Train Loss: 0.08708878606557846 Train BER 0.003762162057682872,                  \n",
      " [184/500] At 0.0 dB, Train Loss: 0.004055750090628862 Train BER 0.00022162162349559367\n",
      "Time for one full iteration is 8.9117 minutes\n",
      "encoder learning rate: 7.97e-04, decoder learning rate: 7.97e-04\n",
      "[185/500] At -2.0 dB, Train Loss: 0.12138669192790985 Train BER 0.006691891700029373,                  \n",
      " [185/500] At 0.0 dB, Train Loss: 0.028371913358569145 Train BER 0.001016216236166656\n",
      "Time for one full iteration is 8.9003 minutes\n",
      "encoder learning rate: 7.94e-04, decoder learning rate: 7.94e-04\n",
      "[186/500] At -2.0 dB, Train Loss: 0.06518485397100449 Train BER 0.0029891892336308956,                  \n",
      " [186/500] At 0.0 dB, Train Loss: 0.0003968972887378186 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7845 minutes\n",
      "encoder learning rate: 7.91e-04, decoder learning rate: 7.91e-04\n",
      "[187/500] At -2.0 dB, Train Loss: 0.047609683126211166 Train BER 0.0024054055102169514,                  \n",
      " [187/500] At 0.0 dB, Train Loss: 0.0004806705692317337 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.6349 minutes\n",
      "encoder learning rate: 7.88e-04, decoder learning rate: 7.88e-04\n",
      "[188/500] At -2.0 dB, Train Loss: 0.039872169494628906 Train BER 0.001810810761526227,                  \n",
      " [188/500] At 0.0 dB, Train Loss: 0.0002552195801399648 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7675 minutes\n",
      "encoder learning rate: 7.86e-04, decoder learning rate: 7.86e-04\n",
      "[189/500] At -2.0 dB, Train Loss: 0.041879843920469284 Train BER 0.002081081038340926,                  \n",
      " [189/500] At 0.0 dB, Train Loss: 0.00030711834551766515 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9316 minutes\n",
      "encoder learning rate: 7.83e-04, decoder learning rate: 7.83e-04\n",
      "[190/500] At -2.0 dB, Train Loss: 0.03831208497285843 Train BER 0.001956756692379713,                  \n",
      " [190/500] At 0.0 dB, Train Loss: 0.00020949315512552857 Train BER 0.0\n",
      "Time for one full iteration is 8.8763 minutes\n",
      "encoder learning rate: 7.80e-04, decoder learning rate: 7.80e-04\n",
      "[191/500] At -2.0 dB, Train Loss: 0.04341554269194603 Train BER 0.0022540539503097534,                  \n",
      " [191/500] At 0.0 dB, Train Loss: 0.0008764975937083364 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.8629 minutes\n",
      "encoder learning rate: 7.77e-04, decoder learning rate: 7.77e-04\n",
      "[192/500] At -2.0 dB, Train Loss: 0.041409317404031754 Train BER 0.0020216216798871756,                  \n",
      " [192/500] At 0.0 dB, Train Loss: 0.0005003956030122936 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9200 minutes\n",
      "encoder learning rate: 7.74e-04, decoder learning rate: 7.74e-04\n",
      "[193/500] At -2.0 dB, Train Loss: 0.031220098957419395 Train BER 0.0012864865129813552,                  \n",
      " [193/500] At 0.0 dB, Train Loss: 0.0003012597153428942 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9113 minutes\n",
      "encoder learning rate: 7.71e-04, decoder learning rate: 7.71e-04\n",
      "[194/500] At -2.0 dB, Train Loss: 0.047320716083049774 Train BER 0.002540540648624301,                  \n",
      " [194/500] At 0.0 dB, Train Loss: 0.0003086647775489837 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9280 minutes\n",
      "encoder learning rate: 7.68e-04, decoder learning rate: 7.68e-04\n",
      "[195/500] At -2.0 dB, Train Loss: 0.0388902872800827 Train BER 0.0019027027301490307,                  \n",
      " [195/500] At 0.0 dB, Train Loss: 0.00016613460320513695 Train BER 0.0\n",
      "Time for one full iteration is 8.9853 minutes\n",
      "encoder learning rate: 7.65e-04, decoder learning rate: 7.65e-04\n",
      "[196/500] At -2.0 dB, Train Loss: 0.04044404625892639 Train BER 0.0019351351074874401,                  \n",
      " [196/500] At 0.0 dB, Train Loss: 0.0002928765898104757 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8961 minutes\n",
      "encoder learning rate: 7.62e-04, decoder learning rate: 7.62e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/500] At -2.0 dB, Train Loss: 0.03319062292575836 Train BER 0.0015837837709113955,                  \n",
      " [197/500] At 0.0 dB, Train Loss: 0.00030747876735404134 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9272 minutes\n",
      "encoder learning rate: 7.59e-04, decoder learning rate: 7.59e-04\n",
      "[198/500] At -2.0 dB, Train Loss: 0.03518558666110039 Train BER 0.0017243243055418134,                  \n",
      " [198/500] At 0.0 dB, Train Loss: 0.0003538820019457489 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9313 minutes\n",
      "encoder learning rate: 7.56e-04, decoder learning rate: 7.56e-04\n",
      "[199/500] At -2.0 dB, Train Loss: 0.029043754562735558 Train BER 0.0014108108589425683,                  \n",
      " [199/500] At 0.0 dB, Train Loss: 0.00019270673510618508 Train BER 0.0\n",
      "Time for one full iteration is 8.9707 minutes\n",
      "encoder learning rate: 7.53e-04, decoder learning rate: 7.53e-04\n",
      "[200/500] At -2.0 dB, Train Loss: 0.02619548887014389 Train BER 0.0011891891481354833,                  \n",
      " [200/500] At 0.0 dB, Train Loss: 0.0002740272902883589 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9448 minutes\n",
      "encoder learning rate: 7.50e-04, decoder learning rate: 7.50e-04\n",
      "[201/500] At -2.0 dB, Train Loss: 0.03252611309289932 Train BER 0.0016540540382266045,                  \n",
      " [201/500] At 0.0 dB, Train Loss: 0.0001970702433027327 Train BER 0.0\n",
      "Time for one full iteration is 8.9550 minutes\n",
      "encoder learning rate: 7.47e-04, decoder learning rate: 7.47e-04\n",
      "[202/500] At -2.0 dB, Train Loss: 0.030871106311678886 Train BER 0.0015891891671344638,                  \n",
      " [202/500] At 0.0 dB, Train Loss: 8.630509546492249e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9305 minutes\n",
      "encoder learning rate: 7.44e-04, decoder learning rate: 7.44e-04\n",
      "[203/500] At -2.0 dB, Train Loss: 0.03557216376066208 Train BER 0.0018540540477260947,                  \n",
      " [203/500] At 0.0 dB, Train Loss: 0.00011207050556549802 Train BER 0.0\n",
      "Time for one full iteration is 8.9408 minutes\n",
      "encoder learning rate: 7.41e-04, decoder learning rate: 7.41e-04\n",
      "[204/500] At -2.0 dB, Train Loss: 0.03217138722538948 Train BER 0.0015513513935729861,                  \n",
      " [204/500] At 0.0 dB, Train Loss: 0.00013996318739373237 Train BER 0.0\n",
      "Time for one full iteration is 8.9282 minutes\n",
      "encoder learning rate: 7.38e-04, decoder learning rate: 7.38e-04\n",
      "[205/500] At -2.0 dB, Train Loss: 0.045467644929885864 Train BER 0.0024324324913322926,                  \n",
      " [205/500] At 0.0 dB, Train Loss: 0.00019764111493714154 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9332 minutes\n",
      "encoder learning rate: 7.35e-04, decoder learning rate: 7.35e-04\n",
      "[206/500] At -2.0 dB, Train Loss: 0.025940323248505592 Train BER 0.001254054019227624,                  \n",
      " [206/500] At 0.0 dB, Train Loss: 0.0006888091447763145 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.9527 minutes\n",
      "encoder learning rate: 7.32e-04, decoder learning rate: 7.32e-04\n",
      "[207/500] At -2.0 dB, Train Loss: 0.027581563219428062 Train BER 0.00139999995008111,                  \n",
      " [207/500] At 0.0 dB, Train Loss: 0.00013471663987729698 Train BER 0.0\n",
      "Time for one full iteration is 8.9307 minutes\n",
      "encoder learning rate: 7.29e-04, decoder learning rate: 7.29e-04\n",
      "[208/500] At -2.0 dB, Train Loss: 0.02509724721312523 Train BER 0.0010972972959280014,                  \n",
      " [208/500] At 0.0 dB, Train Loss: 0.0009646273101679981 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 9.0087 minutes\n",
      "encoder learning rate: 7.26e-04, decoder learning rate: 7.26e-04\n",
      "[209/500] At -2.0 dB, Train Loss: 0.036245979368686676 Train BER 0.0015297296922653913,                  \n",
      " [209/500] At 0.0 dB, Train Loss: 0.00022003021149430424 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9625 minutes\n",
      "encoder learning rate: 7.23e-04, decoder learning rate: 7.23e-04\n",
      "[210/500] At -2.0 dB, Train Loss: 0.03286588937044144 Train BER 0.0015783783746883273,                  \n",
      " [210/500] At 0.0 dB, Train Loss: 0.00021293888858053833 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9799 minutes\n",
      "encoder learning rate: 7.19e-04, decoder learning rate: 7.19e-04\n",
      "[211/500] At -2.0 dB, Train Loss: 0.023276429623365402 Train BER 0.0010702703148126602,                  \n",
      " [211/500] At 0.0 dB, Train Loss: 0.00020972322090528905 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9760 minutes\n",
      "encoder learning rate: 7.16e-04, decoder learning rate: 7.16e-04\n",
      "[212/500] At -2.0 dB, Train Loss: 0.03202460706233978 Train BER 0.0014648648211732507,                  \n",
      " [212/500] At 0.0 dB, Train Loss: 7.888959953561425e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9006 minutes\n",
      "encoder learning rate: 7.13e-04, decoder learning rate: 7.13e-04\n",
      "[213/500] At -2.0 dB, Train Loss: 0.025262480601668358 Train BER 0.0012270270381122828,                  \n",
      " [213/500] At 0.0 dB, Train Loss: 4.296156112104654e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9459 minutes\n",
      "encoder learning rate: 7.10e-04, decoder learning rate: 7.10e-04\n",
      "[214/500] At -2.0 dB, Train Loss: 0.02715487964451313 Train BER 0.0012702703243121505,                  \n",
      " [214/500] At 0.0 dB, Train Loss: 0.000186787816346623 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9347 minutes\n",
      "encoder learning rate: 7.07e-04, decoder learning rate: 7.07e-04\n",
      "[215/500] At -2.0 dB, Train Loss: 0.029956836253404617 Train BER 0.0016594594344496727,                  \n",
      " [215/500] At 0.0 dB, Train Loss: 0.00028527204995043576 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9113 minutes\n",
      "encoder learning rate: 7.04e-04, decoder learning rate: 7.04e-04\n",
      "[216/500] At -2.0 dB, Train Loss: 0.028243664652109146 Train BER 0.0012216216418892145,                  \n",
      " [216/500] At 0.0 dB, Train Loss: 9.274332842323929e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9138 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n",
      "[217/500] At -2.0 dB, Train Loss: 0.028606656938791275 Train BER 0.0013459459878504276,                  \n",
      " [217/500] At 0.0 dB, Train Loss: 0.00014409681898541749 Train BER 0.0\n",
      "Time for one full iteration is 8.9048 minutes\n",
      "encoder learning rate: 6.97e-04, decoder learning rate: 6.97e-04\n",
      "[218/500] At -2.0 dB, Train Loss: 0.020265595987439156 Train BER 0.0009837837424129248,                  \n",
      " [218/500] At 0.0 dB, Train Loss: 6.248602585401386e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9886 minutes\n",
      "encoder learning rate: 6.94e-04, decoder learning rate: 6.94e-04\n",
      "[219/500] At -2.0 dB, Train Loss: 0.033576600253582 Train BER 0.0016864865319803357,                  \n",
      " [219/500] At 0.0 dB, Train Loss: 5.806816625408828e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9487 minutes\n",
      "encoder learning rate: 6.91e-04, decoder learning rate: 6.91e-04\n",
      "[220/500] At -2.0 dB, Train Loss: 0.026423582807183266 Train BER 0.0012216216418892145,                  \n",
      " [220/500] At 0.0 dB, Train Loss: 5.950483318883926e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9273 minutes\n",
      "encoder learning rate: 6.88e-04, decoder learning rate: 6.88e-04\n",
      "[221/500] At -2.0 dB, Train Loss: 0.03287031129002571 Train BER 0.0016162162646651268,                  \n",
      " [221/500] At 0.0 dB, Train Loss: 0.0002081913553411141 Train BER 0.0\n",
      "Time for one full iteration is 8.9446 minutes\n",
      "encoder learning rate: 6.84e-04, decoder learning rate: 6.84e-04\n",
      "[222/500] At -2.0 dB, Train Loss: 0.03170354291796684 Train BER 0.0015567567897960544,                  \n",
      " [222/500] At 0.0 dB, Train Loss: 0.0002130602515535429 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9392 minutes\n",
      "encoder learning rate: 6.81e-04, decoder learning rate: 6.81e-04\n",
      "[223/500] At -2.0 dB, Train Loss: 0.021797677502036095 Train BER 0.0011459459783509374,                  \n",
      " [223/500] At 0.0 dB, Train Loss: 7.271915819728747e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9625 minutes\n",
      "encoder learning rate: 6.78e-04, decoder learning rate: 6.78e-04\n",
      "[224/500] At -2.0 dB, Train Loss: 0.02344980090856552 Train BER 0.0011243242770433426,                  \n",
      " [224/500] At 0.0 dB, Train Loss: 6.543836207129061e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8934 minutes\n",
      "encoder learning rate: 6.75e-04, decoder learning rate: 6.75e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[225/500] At -2.0 dB, Train Loss: 0.02982260286808014 Train BER 0.0014324324438348413,                  \n",
      " [225/500] At 0.0 dB, Train Loss: 0.00010067070979857817 Train BER 0.0\n",
      "Time for one full iteration is 8.9418 minutes\n",
      "encoder learning rate: 6.71e-04, decoder learning rate: 6.71e-04\n",
      "[226/500] At -2.0 dB, Train Loss: 0.03437909483909607 Train BER 0.0015459459973499179,                  \n",
      " [226/500] At 0.0 dB, Train Loss: 0.0001353562402073294 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7844 minutes\n",
      "encoder learning rate: 6.68e-04, decoder learning rate: 6.68e-04\n",
      "[227/500] At -2.0 dB, Train Loss: 0.03137366846203804 Train BER 0.0015513513935729861,                  \n",
      " [227/500] At 0.0 dB, Train Loss: 4.0357397665502504e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6148 minutes\n",
      "encoder learning rate: 6.65e-04, decoder learning rate: 6.65e-04\n",
      "[228/500] At -2.0 dB, Train Loss: 0.023691194131970406 Train BER 0.001162162167020142,                  \n",
      " [228/500] At 0.0 dB, Train Loss: 8.217334834625944e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6382 minutes\n",
      "encoder learning rate: 6.61e-04, decoder learning rate: 6.61e-04\n",
      "[229/500] At -2.0 dB, Train Loss: 0.02760280855000019 Train BER 0.001329729682765901,                  \n",
      " [229/500] At 0.0 dB, Train Loss: 4.6980127081042156e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6689 minutes\n",
      "encoder learning rate: 6.58e-04, decoder learning rate: 6.58e-04\n",
      "[230/500] At -2.0 dB, Train Loss: 0.0257246270775795 Train BER 0.0011945945443585515,                  \n",
      " [230/500] At 0.0 dB, Train Loss: 3.7957703170832247e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6619 minutes\n",
      "encoder learning rate: 6.55e-04, decoder learning rate: 6.55e-04\n",
      "[231/500] At -2.0 dB, Train Loss: 0.025678616017103195 Train BER 0.0012270270381122828,                  \n",
      " [231/500] At 0.0 dB, Train Loss: 9.125564974965528e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5986 minutes\n",
      "encoder learning rate: 6.52e-04, decoder learning rate: 6.52e-04\n",
      "[232/500] At -2.0 dB, Train Loss: 0.021123545244336128 Train BER 0.0010432432172819972,                  \n",
      " [232/500] At 0.0 dB, Train Loss: 0.0004695183306466788 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.6181 minutes\n",
      "encoder learning rate: 6.48e-04, decoder learning rate: 6.48e-04\n",
      "[233/500] At -2.0 dB, Train Loss: 0.03256415203213692 Train BER 0.001767567591741681,                  \n",
      " [233/500] At 0.0 dB, Train Loss: 0.00015749056183267385 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7296 minutes\n",
      "encoder learning rate: 6.45e-04, decoder learning rate: 6.45e-04\n",
      "[234/500] At -2.0 dB, Train Loss: 0.027710892260074615 Train BER 0.0013891891576349735,                  \n",
      " [234/500] At 0.0 dB, Train Loss: 5.1548180636018515e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7721 minutes\n",
      "encoder learning rate: 6.42e-04, decoder learning rate: 6.42e-04\n",
      "[235/500] At -2.0 dB, Train Loss: 0.03322834521532059 Train BER 0.0014918919187039137,                  \n",
      " [235/500] At 0.0 dB, Train Loss: 2.974310700665228e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7246 minutes\n",
      "encoder learning rate: 6.38e-04, decoder learning rate: 6.38e-04\n",
      "[236/500] At -2.0 dB, Train Loss: 0.029908297583460808 Train BER 0.001378378365188837,                  \n",
      " [236/500] At 0.0 dB, Train Loss: 0.0009464403265155852 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9005 minutes\n",
      "encoder learning rate: 6.35e-04, decoder learning rate: 6.35e-04\n",
      "[237/500] At -2.0 dB, Train Loss: 0.029910830780863762 Train BER 0.0014810811262577772,                  \n",
      " [237/500] At 0.0 dB, Train Loss: 8.912025805329904e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6720 minutes\n",
      "encoder learning rate: 6.31e-04, decoder learning rate: 6.31e-04\n",
      "[238/500] At -2.0 dB, Train Loss: 0.019590824842453003 Train BER 0.0008594594546593726,                  \n",
      " [238/500] At 0.0 dB, Train Loss: 4.901022111880593e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6505 minutes\n",
      "encoder learning rate: 6.28e-04, decoder learning rate: 6.28e-04\n",
      "[239/500] At -2.0 dB, Train Loss: 0.02408161386847496 Train BER 0.001183783751912415,                  \n",
      " [239/500] At 0.0 dB, Train Loss: 0.00016369928198400885 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.8951 minutes\n",
      "encoder learning rate: 6.25e-04, decoder learning rate: 6.25e-04\n",
      "[240/500] At -2.0 dB, Train Loss: 0.028936024755239487 Train BER 0.0014378378400579095,                  \n",
      " [240/500] At 0.0 dB, Train Loss: 2.8166505217086524e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8705 minutes\n",
      "encoder learning rate: 6.21e-04, decoder learning rate: 6.21e-04\n",
      "[241/500] At -2.0 dB, Train Loss: 0.026807324960827827 Train BER 0.0012054054532200098,                  \n",
      " [241/500] At 0.0 dB, Train Loss: 0.00016734236851334572 Train BER 0.0\n",
      "Time for one full iteration is 8.8275 minutes\n",
      "encoder learning rate: 6.18e-04, decoder learning rate: 6.18e-04\n",
      "[242/500] At -2.0 dB, Train Loss: 0.02790728397667408 Train BER 0.0013459459878504276,                  \n",
      " [242/500] At 0.0 dB, Train Loss: 0.0007042528013698757 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8820 minutes\n",
      "encoder learning rate: 6.15e-04, decoder learning rate: 6.15e-04\n",
      "[243/500] At -2.0 dB, Train Loss: 0.029446210712194443 Train BER 0.0013945945538580418,                  \n",
      " [243/500] At 0.0 dB, Train Loss: 5.065409277449362e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7308 minutes\n",
      "encoder learning rate: 6.11e-04, decoder learning rate: 6.11e-04\n",
      "[244/500] At -2.0 dB, Train Loss: 0.027403883635997772 Train BER 0.001497297314926982,                  \n",
      " [244/500] At 0.0 dB, Train Loss: 9.788609168026596e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8579 minutes\n",
      "encoder learning rate: 6.08e-04, decoder learning rate: 6.08e-04\n",
      "[245/500] At -2.0 dB, Train Loss: 0.02205616980791092 Train BER 0.0009081081370823085,                  \n",
      " [245/500] At 0.0 dB, Train Loss: 7.716463733231649e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7628 minutes\n",
      "encoder learning rate: 6.04e-04, decoder learning rate: 6.04e-04\n",
      "[246/500] At -2.0 dB, Train Loss: 0.03262270614504814 Train BER 0.0017189189093187451,                  \n",
      " [246/500] At 0.0 dB, Train Loss: 6.934050179552287e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8747 minutes\n",
      "encoder learning rate: 6.01e-04, decoder learning rate: 6.01e-04\n",
      "[247/500] At -2.0 dB, Train Loss: 0.028739260509610176 Train BER 0.0013513513840734959,                  \n",
      " [247/500] At 0.0 dB, Train Loss: 0.00015935090777929872 Train BER 0.0\n",
      "Time for one full iteration is 8.8518 minutes\n",
      "encoder learning rate: 5.98e-04, decoder learning rate: 5.98e-04\n",
      "[248/500] At -2.0 dB, Train Loss: 0.02181158773601055 Train BER 0.0012216216418892145,                  \n",
      " [248/500] At 0.0 dB, Train Loss: 2.709919135668315e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7794 minutes\n",
      "encoder learning rate: 5.94e-04, decoder learning rate: 5.94e-04\n",
      "[249/500] At -2.0 dB, Train Loss: 0.020496688783168793 Train BER 0.0009675675537437201,                  \n",
      " [249/500] At 0.0 dB, Train Loss: 7.439851469825953e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7680 minutes\n",
      "encoder learning rate: 5.91e-04, decoder learning rate: 5.91e-04\n",
      "[250/500] At -2.0 dB, Train Loss: 0.030060088261961937 Train BER 0.0016162162646651268,                  \n",
      " [250/500] At 0.0 dB, Train Loss: 2.706116902118083e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7385 minutes\n",
      "encoder learning rate: 5.87e-04, decoder learning rate: 5.87e-04\n",
      "[251/500] At -2.0 dB, Train Loss: 0.02180062048137188 Train BER 0.0010972972959280014,                  \n",
      " [251/500] At 0.0 dB, Train Loss: 3.925839700968936e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7345 minutes\n",
      "encoder learning rate: 5.84e-04, decoder learning rate: 5.84e-04\n",
      "[252/500] At -2.0 dB, Train Loss: 0.01882150210440159 Train BER 0.0008594594546593726,                  \n",
      " [252/500] At 0.0 dB, Train Loss: 5.739186963182874e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7147 minutes\n",
      "encoder learning rate: 5.80e-04, decoder learning rate: 5.80e-04\n",
      "[253/500] At -2.0 dB, Train Loss: 0.028551867231726646 Train BER 0.00130270270165056,                  \n",
      " [253/500] At 0.0 dB, Train Loss: 5.580151264439337e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7355 minutes\n",
      "encoder learning rate: 5.77e-04, decoder learning rate: 5.77e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[254/500] At -2.0 dB, Train Loss: 0.027457498013973236 Train BER 0.0012594594154506922,                  \n",
      " [254/500] At 0.0 dB, Train Loss: 0.0006520934402942657 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.7604 minutes\n",
      "encoder learning rate: 5.73e-04, decoder learning rate: 5.73e-04\n",
      "[255/500] At -2.0 dB, Train Loss: 0.02493523620069027 Train BER 0.0011945945443585515,                  \n",
      " [255/500] At 0.0 dB, Train Loss: 0.00018649420235306025 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8501 minutes\n",
      "encoder learning rate: 5.70e-04, decoder learning rate: 5.70e-04\n",
      "[256/500] At -2.0 dB, Train Loss: 0.019520744681358337 Train BER 0.001016216236166656,                  \n",
      " [256/500] At 0.0 dB, Train Loss: 0.0006329089519567788 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.7705 minutes\n",
      "encoder learning rate: 5.67e-04, decoder learning rate: 5.67e-04\n",
      "[257/500] At -2.0 dB, Train Loss: 0.030139461159706116 Train BER 0.0015621621860191226,                  \n",
      " [257/500] At 0.0 dB, Train Loss: 5.7629873481346294e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7102 minutes\n",
      "encoder learning rate: 5.63e-04, decoder learning rate: 5.63e-04\n",
      "[258/500] At -2.0 dB, Train Loss: 0.030041584745049477 Train BER 0.0013837837614119053,                  \n",
      " [258/500] At 0.0 dB, Train Loss: 0.00010802878387039527 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7181 minutes\n",
      "encoder learning rate: 5.60e-04, decoder learning rate: 5.60e-04\n",
      "[259/500] At -2.0 dB, Train Loss: 0.032268427312374115 Train BER 0.001881081028841436,                  \n",
      " [259/500] At 0.0 dB, Train Loss: 5.5657637858530506e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7216 minutes\n",
      "encoder learning rate: 5.56e-04, decoder learning rate: 5.56e-04\n",
      "[260/500] At -2.0 dB, Train Loss: 0.016831273213028908 Train BER 0.0007945945835672319,                  \n",
      " [260/500] At 0.0 dB, Train Loss: 5.7676144933793694e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7066 minutes\n",
      "encoder learning rate: 5.53e-04, decoder learning rate: 5.53e-04\n",
      "[261/500] At -2.0 dB, Train Loss: 0.03280004486441612 Train BER 0.0015891891671344638,                  \n",
      " [261/500] At 0.0 dB, Train Loss: 5.2266084821894765e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7672 minutes\n",
      "encoder learning rate: 5.49e-04, decoder learning rate: 5.49e-04\n",
      "[262/500] At -2.0 dB, Train Loss: 0.03113194927573204 Train BER 0.0015999999595806003,                  \n",
      " [262/500] At 0.0 dB, Train Loss: 2.8347263651085086e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7088 minutes\n",
      "encoder learning rate: 5.46e-04, decoder learning rate: 5.46e-04\n",
      "[263/500] At -2.0 dB, Train Loss: 0.03412464261054993 Train BER 0.0016702702268958092,                  \n",
      " [263/500] At 0.0 dB, Train Loss: 2.8102545911679044e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7209 minutes\n",
      "encoder learning rate: 5.42e-04, decoder learning rate: 5.42e-04\n",
      "[264/500] At -2.0 dB, Train Loss: 0.030144738033413887 Train BER 0.0014810811262577772,                  \n",
      " [264/500] At 0.0 dB, Train Loss: 2.9473809263436124e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7281 minutes\n",
      "encoder learning rate: 5.39e-04, decoder learning rate: 5.39e-04\n",
      "[265/500] At -2.0 dB, Train Loss: 0.025149475783109665 Train BER 0.0011027026921510696,                  \n",
      " [265/500] At 0.0 dB, Train Loss: 0.0001615368528291583 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7422 minutes\n",
      "encoder learning rate: 5.35e-04, decoder learning rate: 5.35e-04\n",
      "[266/500] At -2.0 dB, Train Loss: 0.02750932052731514 Train BER 0.001254054019227624,                  \n",
      " [266/500] At 0.0 dB, Train Loss: 2.7618314561550505e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6221 minutes\n",
      "encoder learning rate: 5.32e-04, decoder learning rate: 5.32e-04\n",
      "[267/500] At -2.0 dB, Train Loss: 0.021899787709116936 Train BER 0.0010540540097281337,                  \n",
      " [267/500] At 0.0 dB, Train Loss: 3.4849865187425166e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6805 minutes\n",
      "encoder learning rate: 5.28e-04, decoder learning rate: 5.28e-04\n",
      "[268/500] At -2.0 dB, Train Loss: 0.022994274273514748 Train BER 0.0011135134845972061,                  \n",
      " [268/500] At 0.0 dB, Train Loss: 3.152466524625197e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7654 minutes\n",
      "encoder learning rate: 5.25e-04, decoder learning rate: 5.25e-04\n",
      "[269/500] At -2.0 dB, Train Loss: 0.024922125041484833 Train BER 0.0012162162456661463,                  \n",
      " [269/500] At 0.0 dB, Train Loss: 2.6218544007861055e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7205 minutes\n",
      "encoder learning rate: 5.21e-04, decoder learning rate: 5.21e-04\n",
      "[270/500] At -2.0 dB, Train Loss: 0.024126870557665825 Train BER 0.0012270270381122828,                  \n",
      " [270/500] At 0.0 dB, Train Loss: 3.729348100023344e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6800 minutes\n",
      "encoder learning rate: 5.18e-04, decoder learning rate: 5.18e-04\n",
      "[271/500] At -2.0 dB, Train Loss: 0.024789756163954735 Train BER 0.0011297296732664108,                  \n",
      " [271/500] At 0.0 dB, Train Loss: 3.7123205402167514e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7462 minutes\n",
      "encoder learning rate: 5.14e-04, decoder learning rate: 5.14e-04\n",
      "[272/500] At -2.0 dB, Train Loss: 0.025334618985652924 Train BER 0.001254054019227624,                  \n",
      " [272/500] At 0.0 dB, Train Loss: 0.00031496546580456197 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.5961 minutes\n",
      "encoder learning rate: 5.11e-04, decoder learning rate: 5.11e-04\n",
      "[273/500] At -2.0 dB, Train Loss: 0.031397685408592224 Train BER 0.0017189189093187451,                  \n",
      " [273/500] At 0.0 dB, Train Loss: 4.201976116746664e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6102 minutes\n",
      "encoder learning rate: 5.07e-04, decoder learning rate: 5.07e-04\n",
      "[274/500] At -2.0 dB, Train Loss: 0.022365752607584 Train BER 0.001135135185904801,                  \n",
      " [274/500] At 0.0 dB, Train Loss: 3.196591205778532e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7814 minutes\n",
      "encoder learning rate: 5.04e-04, decoder learning rate: 5.04e-04\n",
      "[275/500] At -2.0 dB, Train Loss: 0.026612747460603714 Train BER 0.0012054054532200098,                  \n",
      " [275/500] At 0.0 dB, Train Loss: 4.7391309635713696e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7439 minutes\n",
      "encoder learning rate: 5.01e-04, decoder learning rate: 5.01e-04\n",
      "[276/500] At -2.0 dB, Train Loss: 0.026175914332270622 Train BER 0.0012648648116737604,                  \n",
      " [276/500] At 0.0 dB, Train Loss: 1.528661960037425e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6682 minutes\n",
      "encoder learning rate: 4.97e-04, decoder learning rate: 4.97e-04\n",
      "[277/500] At -2.0 dB, Train Loss: 0.02864884026348591 Train BER 0.001470270217396319,                  \n",
      " [277/500] At 0.0 dB, Train Loss: 1.3643849342770409e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6736 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[278/500] At -2.0 dB, Train Loss: 0.02947387658059597 Train BER 0.0015513513935729861,                  \n",
      " [278/500] At 0.0 dB, Train Loss: 3.5521563404472545e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7265 minutes\n",
      "encoder learning rate: 4.90e-04, decoder learning rate: 4.90e-04\n",
      "[279/500] At -2.0 dB, Train Loss: 0.024226808920502663 Train BER 0.0011567567707970738,                  \n",
      " [279/500] At 0.0 dB, Train Loss: 2.7373203920433298e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6958 minutes\n",
      "encoder learning rate: 4.87e-04, decoder learning rate: 4.87e-04\n",
      "[280/500] At -2.0 dB, Train Loss: 0.02510523796081543 Train BER 0.0013189188903197646,                  \n",
      " [280/500] At 0.0 dB, Train Loss: 2.22678245336283e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7068 minutes\n",
      "encoder learning rate: 4.83e-04, decoder learning rate: 4.83e-04\n",
      "[281/500] At -2.0 dB, Train Loss: 0.023780176416039467 Train BER 0.0010972972959280014,                  \n",
      " [281/500] At 0.0 dB, Train Loss: 2.9784107027808204e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5640 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[282/500] At -2.0 dB, Train Loss: 0.023952189832925797 Train BER 0.0011783783556893468,                  \n",
      " [282/500] At 0.0 dB, Train Loss: 1.425387199560646e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6605 minutes\n",
      "encoder learning rate: 4.76e-04, decoder learning rate: 4.76e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283/500] At -2.0 dB, Train Loss: 0.024736151099205017 Train BER 0.0011567567707970738,                  \n",
      " [283/500] At 0.0 dB, Train Loss: 3.285973070887849e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7166 minutes\n",
      "encoder learning rate: 4.73e-04, decoder learning rate: 4.73e-04\n",
      "[284/500] At -2.0 dB, Train Loss: 0.02305952087044716 Train BER 0.0013081080978736281,                  \n",
      " [284/500] At 0.0 dB, Train Loss: 0.0005976660177111626 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.6283 minutes\n",
      "encoder learning rate: 4.69e-04, decoder learning rate: 4.69e-04\n",
      "[285/500] At -2.0 dB, Train Loss: 0.03388185054063797 Train BER 0.0017243243055418134,                  \n",
      " [285/500] At 0.0 dB, Train Loss: 6.766035221517086e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7399 minutes\n",
      "encoder learning rate: 4.66e-04, decoder learning rate: 4.66e-04\n",
      "[286/500] At -2.0 dB, Train Loss: 0.019939830526709557 Train BER 0.0009729729499667883,                  \n",
      " [286/500] At 0.0 dB, Train Loss: 3.9728005504002795e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7688 minutes\n",
      "encoder learning rate: 4.62e-04, decoder learning rate: 4.62e-04\n",
      "[287/500] At -2.0 dB, Train Loss: 0.021309716627001762 Train BER 0.0010486486135050654,                  \n",
      " [287/500] At 0.0 dB, Train Loss: 0.0012643466470763087 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.7607 minutes\n",
      "encoder learning rate: 4.59e-04, decoder learning rate: 4.59e-04\n",
      "[288/500] At -2.0 dB, Train Loss: 0.029682762920856476 Train BER 0.0014162162551656365,                  \n",
      " [288/500] At 0.0 dB, Train Loss: 0.0003854876267723739 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.7270 minutes\n",
      "encoder learning rate: 4.55e-04, decoder learning rate: 4.55e-04\n",
      "[289/500] At -2.0 dB, Train Loss: 0.023403137922286987 Train BER 0.0011513513745740056,                  \n",
      " [289/500] At 0.0 dB, Train Loss: 1.6548270650673658e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6910 minutes\n",
      "encoder learning rate: 4.52e-04, decoder learning rate: 4.52e-04\n",
      "[290/500] At -2.0 dB, Train Loss: 0.02671179547905922 Train BER 0.001378378365188837,                  \n",
      " [290/500] At 0.0 dB, Train Loss: 3.87480940844398e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5389 minutes\n",
      "encoder learning rate: 4.48e-04, decoder learning rate: 4.48e-04\n",
      "[291/500] At -2.0 dB, Train Loss: 0.028047969564795494 Train BER 0.001470270217396319,                  \n",
      " [291/500] At 0.0 dB, Train Loss: 3.4943146602017805e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6288 minutes\n",
      "encoder learning rate: 4.45e-04, decoder learning rate: 4.45e-04\n",
      "[292/500] At -2.0 dB, Train Loss: 0.022888951003551483 Train BER 0.001254054019227624,                  \n",
      " [292/500] At 0.0 dB, Train Loss: 1.9559040083549917e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7143 minutes\n",
      "encoder learning rate: 4.41e-04, decoder learning rate: 4.41e-04\n",
      "[293/500] At -2.0 dB, Train Loss: 0.02962971106171608 Train BER 0.0015675675822421908,                  \n",
      " [293/500] At 0.0 dB, Train Loss: 9.5874775070115e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5975 minutes\n",
      "encoder learning rate: 4.38e-04, decoder learning rate: 4.38e-04\n",
      "[294/500] At -2.0 dB, Train Loss: 0.02217288874089718 Train BER 0.0011081080883741379,                  \n",
      " [294/500] At 0.0 dB, Train Loss: 0.00010334968101233244 Train BER 0.0\n",
      "Time for one full iteration is 8.5832 minutes\n",
      "encoder learning rate: 4.34e-04, decoder learning rate: 4.34e-04\n",
      "[295/500] At -2.0 dB, Train Loss: 0.026263263076543808 Train BER 0.0012486486230045557,                  \n",
      " [295/500] At 0.0 dB, Train Loss: 3.341526826261543e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3274 minutes\n",
      "encoder learning rate: 4.31e-04, decoder learning rate: 4.31e-04\n",
      "[296/500] At -2.0 dB, Train Loss: 0.01905912160873413 Train BER 0.0009351351181976497,                  \n",
      " [296/500] At 0.0 dB, Train Loss: 0.00011564903252292424 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.3108 minutes\n",
      "encoder learning rate: 4.28e-04, decoder learning rate: 4.28e-04\n",
      "[297/500] At -2.0 dB, Train Loss: 0.022896412760019302 Train BER 0.0011513513745740056,                  \n",
      " [297/500] At 0.0 dB, Train Loss: 4.289327625883743e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.3261 minutes\n",
      "encoder learning rate: 4.24e-04, decoder learning rate: 4.24e-04\n",
      "[298/500] At -2.0 dB, Train Loss: 0.02688330039381981 Train BER 0.0012486486230045557,                  \n",
      " [298/500] At 0.0 dB, Train Loss: 7.695896056247875e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4962 minutes\n",
      "encoder learning rate: 4.21e-04, decoder learning rate: 4.21e-04\n",
      "[299/500] At -2.0 dB, Train Loss: 0.02878452092409134 Train BER 0.0014864865224808455,                  \n",
      " [299/500] At 0.0 dB, Train Loss: 4.607702430803329e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5768 minutes\n",
      "encoder learning rate: 4.17e-04, decoder learning rate: 4.17e-04\n",
      "[300/500] At -2.0 dB, Train Loss: 0.0234017763286829 Train BER 0.0010810811072587967,                  \n",
      " [300/500] At 0.0 dB, Train Loss: 5.595802213065326e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6260 minutes\n",
      "encoder learning rate: 4.14e-04, decoder learning rate: 4.14e-04\n",
      "[301/500] At -2.0 dB, Train Loss: 0.02650533616542816 Train BER 0.0012972973054274917,                  \n",
      " [301/500] At 0.0 dB, Train Loss: 2.2252628696151078e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5421 minutes\n",
      "encoder learning rate: 4.10e-04, decoder learning rate: 4.10e-04\n",
      "[302/500] At -2.0 dB, Train Loss: 0.01497521623969078 Train BER 0.00070270273135975,                  \n",
      " [302/500] At 0.0 dB, Train Loss: 0.0001318215945502743 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7477 minutes\n",
      "encoder learning rate: 4.07e-04, decoder learning rate: 4.07e-04\n",
      "[303/500] At -2.0 dB, Train Loss: 0.023738795891404152 Train BER 0.0010540540097281337,                  \n",
      " [303/500] At 0.0 dB, Train Loss: 9.63248749030754e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8137 minutes\n",
      "encoder learning rate: 4.03e-04, decoder learning rate: 4.03e-04\n",
      "[304/500] At -2.0 dB, Train Loss: 0.027763303369283676 Train BER 0.0011945945443585515,                  \n",
      " [304/500] At 0.0 dB, Train Loss: 5.36597945028916e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6877 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[305/500] At -2.0 dB, Train Loss: 0.026812724769115448 Train BER 0.0012216216418892145,                  \n",
      " [305/500] At 0.0 dB, Train Loss: 1.1766407624236308e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6939 minutes\n",
      "encoder learning rate: 3.97e-04, decoder learning rate: 3.97e-04\n",
      "[306/500] At -2.0 dB, Train Loss: 0.018450746312737465 Train BER 0.0008918918902054429,                  \n",
      " [306/500] At 0.0 dB, Train Loss: 2.4765047783148475e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8036 minutes\n",
      "encoder learning rate: 3.93e-04, decoder learning rate: 3.93e-04\n",
      "[307/500] At -2.0 dB, Train Loss: 0.02639003098011017 Train BER 0.0015135135035961866,                  \n",
      " [307/500] At 0.0 dB, Train Loss: 5.008029256714508e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8944 minutes\n",
      "encoder learning rate: 3.90e-04, decoder learning rate: 3.90e-04\n",
      "[308/500] At -2.0 dB, Train Loss: 0.02639440819621086 Train BER 0.0013243242865428329,                  \n",
      " [308/500] At 0.0 dB, Train Loss: 2.5960556740756147e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7784 minutes\n",
      "encoder learning rate: 3.86e-04, decoder learning rate: 3.86e-04\n",
      "[309/500] At -2.0 dB, Train Loss: 0.02745921164751053 Train BER 0.00130270270165056,                  \n",
      " [309/500] At 0.0 dB, Train Loss: 2.9171900678193197e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8885 minutes\n",
      "encoder learning rate: 3.83e-04, decoder learning rate: 3.83e-04\n",
      "[310/500] At -2.0 dB, Train Loss: 0.020934270694851875 Train BER 0.0009081081370823085,                  \n",
      " [310/500] At 0.0 dB, Train Loss: 4.8870966566028073e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9064 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[311/500] At -2.0 dB, Train Loss: 0.025986475870013237 Train BER 0.0013513513840734959,                  \n",
      " [311/500] At 0.0 dB, Train Loss: 8.335778693435714e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8916 minutes\n",
      "encoder learning rate: 3.76e-04, decoder learning rate: 3.76e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312/500] At -2.0 dB, Train Loss: 0.027884595096111298 Train BER 0.0013513513840734959,                  \n",
      " [312/500] At 0.0 dB, Train Loss: 1.0707673027354758e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9118 minutes\n",
      "encoder learning rate: 3.73e-04, decoder learning rate: 3.73e-04\n",
      "[313/500] At -2.0 dB, Train Loss: 0.018079848960042 Train BER 0.0007945945835672319,                  \n",
      " [313/500] At 0.0 dB, Train Loss: 2.1213892978266813e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8769 minutes\n",
      "encoder learning rate: 3.70e-04, decoder learning rate: 3.70e-04\n",
      "[314/500] At -2.0 dB, Train Loss: 0.01988224871456623 Train BER 0.0009621621575206518,                  \n",
      " [314/500] At 0.0 dB, Train Loss: 4.347607682575472e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9338 minutes\n",
      "encoder learning rate: 3.66e-04, decoder learning rate: 3.66e-04\n",
      "[315/500] At -2.0 dB, Train Loss: 0.022573916241526604 Train BER 0.0010000000474974513,                  \n",
      " [315/500] At 0.0 dB, Train Loss: 2.7826366931549273e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8113 minutes\n",
      "encoder learning rate: 3.63e-04, decoder learning rate: 3.63e-04\n",
      "[316/500] At -2.0 dB, Train Loss: 0.019803348928689957 Train BER 0.001064864918589592,                  \n",
      " [316/500] At 0.0 dB, Train Loss: 6.319835665635765e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9304 minutes\n",
      "encoder learning rate: 3.59e-04, decoder learning rate: 3.59e-04\n",
      "[317/500] At -2.0 dB, Train Loss: 0.026776563376188278 Train BER 0.0012918919092044234,                  \n",
      " [317/500] At 0.0 dB, Train Loss: 0.00024152992409653962 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8865 minutes\n",
      "encoder learning rate: 3.56e-04, decoder learning rate: 3.56e-04\n",
      "[318/500] At -2.0 dB, Train Loss: 0.023712506517767906 Train BER 0.0012918919092044234,                  \n",
      " [318/500] At 0.0 dB, Train Loss: 1.907792830024846e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9701 minutes\n",
      "encoder learning rate: 3.53e-04, decoder learning rate: 3.53e-04\n",
      "[319/500] At -2.0 dB, Train Loss: 0.020669138059020042 Train BER 0.001016216236166656,                  \n",
      " [319/500] At 0.0 dB, Train Loss: 2.2107518816483207e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9263 minutes\n",
      "encoder learning rate: 3.49e-04, decoder learning rate: 3.49e-04\n",
      "[320/500] At -2.0 dB, Train Loss: 0.023350011557340622 Train BER 0.001162162167020142,                  \n",
      " [320/500] At 0.0 dB, Train Loss: 2.1747453502030112e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9184 minutes\n",
      "encoder learning rate: 3.46e-04, decoder learning rate: 3.46e-04\n",
      "[321/500] At -2.0 dB, Train Loss: 0.029529428109526634 Train BER 0.0015675675822421908,                  \n",
      " [321/500] At 0.0 dB, Train Loss: 0.00010257725807605311 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9533 minutes\n",
      "encoder learning rate: 3.43e-04, decoder learning rate: 3.43e-04\n",
      "[322/500] At -2.0 dB, Train Loss: 0.028234899044036865 Train BER 0.0015567567897960544,                  \n",
      " [322/500] At 0.0 dB, Train Loss: 4.6127202949719504e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8399 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[323/500] At -2.0 dB, Train Loss: 0.025272652506828308 Train BER 0.0011891891481354833,                  \n",
      " [323/500] At 0.0 dB, Train Loss: 1.643564428377431e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9408 minutes\n",
      "encoder learning rate: 3.36e-04, decoder learning rate: 3.36e-04\n",
      "[324/500] At -2.0 dB, Train Loss: 0.018943510949611664 Train BER 0.000994594651274383,                  \n",
      " [324/500] At 0.0 dB, Train Loss: 0.0002159241121262312 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8286 minutes\n",
      "encoder learning rate: 3.33e-04, decoder learning rate: 3.33e-04\n",
      "[325/500] At -2.0 dB, Train Loss: 0.018710896372795105 Train BER 0.0008540540584363043,                  \n",
      " [325/500] At 0.0 dB, Train Loss: 1.8336075299885124e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8391 minutes\n",
      "encoder learning rate: 3.30e-04, decoder learning rate: 3.30e-04\n",
      "[326/500] At -2.0 dB, Train Loss: 0.020793471485376358 Train BER 0.0008486486622132361,                  \n",
      " [326/500] At 0.0 dB, Train Loss: 0.0005176009144634008 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.7388 minutes\n",
      "encoder learning rate: 3.26e-04, decoder learning rate: 3.26e-04\n",
      "[327/500] At -2.0 dB, Train Loss: 0.020760279148817062 Train BER 0.0010918918997049332,                  \n",
      " [327/500] At 0.0 dB, Train Loss: 1.7004393157549202e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7464 minutes\n",
      "encoder learning rate: 3.23e-04, decoder learning rate: 3.23e-04\n",
      "[328/500] At -2.0 dB, Train Loss: 0.019712921231985092 Train BER 0.0008648648508824408,                  \n",
      " [328/500] At 0.0 dB, Train Loss: 1.5905821783235297e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7135 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[329/500] At -2.0 dB, Train Loss: 0.025471284985542297 Train BER 0.0011405405821278691,                  \n",
      " [329/500] At 0.0 dB, Train Loss: 7.962306699482724e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8272 minutes\n",
      "encoder learning rate: 3.17e-04, decoder learning rate: 3.17e-04\n",
      "[330/500] At -2.0 dB, Train Loss: 0.018811769783496857 Train BER 0.0008594594546593726,                  \n",
      " [330/500] At 0.0 dB, Train Loss: 2.6332463676226325e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8605 minutes\n",
      "encoder learning rate: 3.13e-04, decoder learning rate: 3.13e-04\n",
      "[331/500] At -2.0 dB, Train Loss: 0.026674555614590645 Train BER 0.0014054054627195,                  \n",
      " [331/500] At 0.0 dB, Train Loss: 1.0647260751284193e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8477 minutes\n",
      "encoder learning rate: 3.10e-04, decoder learning rate: 3.10e-04\n",
      "[332/500] At -2.0 dB, Train Loss: 0.0220696609467268 Train BER 0.0010432432172819972,                  \n",
      " [332/500] At 0.0 dB, Train Loss: 2.1706826373701915e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7609 minutes\n",
      "encoder learning rate: 3.07e-04, decoder learning rate: 3.07e-04\n",
      "[333/500] At -2.0 dB, Train Loss: 0.017742423340678215 Train BER 0.0008054054342210293,                  \n",
      " [333/500] At 0.0 dB, Train Loss: 1.6368425349355675e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7698 minutes\n",
      "encoder learning rate: 3.04e-04, decoder learning rate: 3.04e-04\n",
      "[334/500] At -2.0 dB, Train Loss: 0.018870089203119278 Train BER 0.0009729729499667883,                  \n",
      " [334/500] At 0.0 dB, Train Loss: 2.5301060304627754e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8072 minutes\n",
      "encoder learning rate: 3.01e-04, decoder learning rate: 3.01e-04\n",
      "[335/500] At -2.0 dB, Train Loss: 0.03295258432626724 Train BER 0.001691891928203404,                  \n",
      " [335/500] At 0.0 dB, Train Loss: 0.0001631195336813107 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9168 minutes\n",
      "encoder learning rate: 2.97e-04, decoder learning rate: 2.97e-04\n",
      "[336/500] At -2.0 dB, Train Loss: 0.02377774752676487 Train BER 0.001064864918589592,                  \n",
      " [336/500] At 0.0 dB, Train Loss: 0.00010079437197418883 Train BER 0.0\n",
      "Time for one full iteration is 8.9154 minutes\n",
      "encoder learning rate: 2.94e-04, decoder learning rate: 2.94e-04\n",
      "[337/500] At -2.0 dB, Train Loss: 0.029675737023353577 Train BER 0.0017027027206495404,                  \n",
      " [337/500] At 0.0 dB, Train Loss: 2.9349881515372545e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8933 minutes\n",
      "encoder learning rate: 2.91e-04, decoder learning rate: 2.91e-04\n",
      "[338/500] At -2.0 dB, Train Loss: 0.021649761125445366 Train BER 0.0010702703148126602,                  \n",
      " [338/500] At 0.0 dB, Train Loss: 2.2668309611617588e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7993 minutes\n",
      "encoder learning rate: 2.88e-04, decoder learning rate: 2.88e-04\n",
      "[339/500] At -2.0 dB, Train Loss: 0.021734144538640976 Train BER 0.0009675675537437201,                  \n",
      " [339/500] At 0.0 dB, Train Loss: 5.062218406237662e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8224 minutes\n",
      "encoder learning rate: 2.85e-04, decoder learning rate: 2.85e-04\n",
      "[340/500] At -2.0 dB, Train Loss: 0.017394574359059334 Train BER 0.0008594594546593726,                  \n",
      " [340/500] At 0.0 dB, Train Loss: 2.9480841476470232e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8812 minutes\n",
      "encoder learning rate: 2.82e-04, decoder learning rate: 2.82e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[341/500] At -2.0 dB, Train Loss: 0.02786720171570778 Train BER 0.0013081080978736281,                  \n",
      " [341/500] At 0.0 dB, Train Loss: 2.158458846679423e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7847 minutes\n",
      "encoder learning rate: 2.78e-04, decoder learning rate: 2.78e-04\n",
      "[342/500] At -2.0 dB, Train Loss: 0.026579588651657104 Train BER 0.0011675675632432103,                  \n",
      " [342/500] At 0.0 dB, Train Loss: 5.229053203947842e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8721 minutes\n",
      "encoder learning rate: 2.75e-04, decoder learning rate: 2.75e-04\n",
      "[343/500] At -2.0 dB, Train Loss: 0.029937785118818283 Train BER 0.001448648632504046,                  \n",
      " [343/500] At 0.0 dB, Train Loss: 0.00030316750053316355 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8599 minutes\n",
      "encoder learning rate: 2.72e-04, decoder learning rate: 2.72e-04\n",
      "[344/500] At -2.0 dB, Train Loss: 0.022630108520388603 Train BER 0.0011891891481354833,                  \n",
      " [344/500] At 0.0 dB, Train Loss: 2.54428741754964e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8687 minutes\n",
      "encoder learning rate: 2.69e-04, decoder learning rate: 2.69e-04\n",
      "[345/500] At -2.0 dB, Train Loss: 0.021350130438804626 Train BER 0.0010756757110357285,                  \n",
      " [345/500] At 0.0 dB, Train Loss: 9.834695811150596e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9192 minutes\n",
      "encoder learning rate: 2.66e-04, decoder learning rate: 2.66e-04\n",
      "[346/500] At -2.0 dB, Train Loss: 0.02042277343571186 Train BER 0.0010432432172819972,                  \n",
      " [346/500] At 0.0 dB, Train Loss: 3.0112227250356227e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9740 minutes\n",
      "encoder learning rate: 2.63e-04, decoder learning rate: 2.63e-04\n",
      "[347/500] At -2.0 dB, Train Loss: 0.02652265690267086 Train BER 0.0014540540287271142,                  \n",
      " [347/500] At 0.0 dB, Train Loss: 5.614351903204806e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8613 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[348/500] At -2.0 dB, Train Loss: 0.018151767551898956 Train BER 0.0008162162266671658,                  \n",
      " [348/500] At 0.0 dB, Train Loss: 8.464547136100009e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7492 minutes\n",
      "encoder learning rate: 2.57e-04, decoder learning rate: 2.57e-04\n",
      "[349/500] At -2.0 dB, Train Loss: 0.01562783494591713 Train BER 0.0008756757015362382,                  \n",
      " [349/500] At 0.0 dB, Train Loss: 3.99488490074873e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8349 minutes\n",
      "encoder learning rate: 2.54e-04, decoder learning rate: 2.54e-04\n",
      "[350/500] At -2.0 dB, Train Loss: 0.01972527615725994 Train BER 0.0009783783461898565,                  \n",
      " [350/500] At 0.0 dB, Train Loss: 1.1664104022202082e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9206 minutes\n",
      "encoder learning rate: 2.51e-04, decoder learning rate: 2.51e-04\n",
      "[351/500] At -2.0 dB, Train Loss: 0.01992812752723694 Train BER 0.0009243243257515132,                  \n",
      " [351/500] At 0.0 dB, Train Loss: 1.0697437573981006e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9276 minutes\n",
      "encoder learning rate: 2.48e-04, decoder learning rate: 2.48e-04\n",
      "[352/500] At -2.0 dB, Train Loss: 0.019324712455272675 Train BER 0.0010108108399435878,                  \n",
      " [352/500] At 0.0 dB, Train Loss: 8.63877994561335e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8554 minutes\n",
      "encoder learning rate: 2.45e-04, decoder learning rate: 2.45e-04\n",
      "[353/500] At -2.0 dB, Train Loss: 0.023904332891106606 Train BER 0.001281081116758287,                  \n",
      " [353/500] At 0.0 dB, Train Loss: 0.0001017458489513956 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8529 minutes\n",
      "encoder learning rate: 2.42e-04, decoder learning rate: 2.42e-04\n",
      "[354/500] At -2.0 dB, Train Loss: 0.018607929348945618 Train BER 0.0008864864939823747,                  \n",
      " [354/500] At 0.0 dB, Train Loss: 2.1487483536475338e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8723 minutes\n",
      "encoder learning rate: 2.39e-04, decoder learning rate: 2.39e-04\n",
      "[355/500] At -2.0 dB, Train Loss: 0.018356531858444214 Train BER 0.0009783783461898565,                  \n",
      " [355/500] At 0.0 dB, Train Loss: 2.5012754122144543e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9127 minutes\n",
      "encoder learning rate: 2.36e-04, decoder learning rate: 2.36e-04\n",
      "[356/500] At -2.0 dB, Train Loss: 0.02234547957777977 Train BER 0.0012000000569969416,                  \n",
      " [356/500] At 0.0 dB, Train Loss: 1.6257055904134177e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8072 minutes\n",
      "encoder learning rate: 2.33e-04, decoder learning rate: 2.33e-04\n",
      "[357/500] At -2.0 dB, Train Loss: 0.017202047631144524 Train BER 0.0007891891873441637,                  \n",
      " [357/500] At 0.0 dB, Train Loss: 2.0051400497322902e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8562 minutes\n",
      "encoder learning rate: 2.30e-04, decoder learning rate: 2.30e-04\n",
      "[358/500] At -2.0 dB, Train Loss: 0.02891385927796364 Train BER 0.0015621621860191226,                  \n",
      " [358/500] At 0.0 dB, Train Loss: 0.00010801981261465698 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8367 minutes\n",
      "encoder learning rate: 2.27e-04, decoder learning rate: 2.27e-04\n",
      "[359/500] At -2.0 dB, Train Loss: 0.02098791114985943 Train BER 0.000994594651274383,                  \n",
      " [359/500] At 0.0 dB, Train Loss: 3.2001404179027304e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8634 minutes\n",
      "encoder learning rate: 2.24e-04, decoder learning rate: 2.24e-04\n",
      "[360/500] At -2.0 dB, Train Loss: 0.019450576975941658 Train BER 0.000821621622890234,                  \n",
      " [360/500] At 0.0 dB, Train Loss: 7.735844519629609e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7178 minutes\n",
      "encoder learning rate: 2.21e-04, decoder learning rate: 2.21e-04\n",
      "[361/500] At -2.0 dB, Train Loss: 0.016354376450181007 Train BER 0.0007405405631288886,                  \n",
      " [361/500] At 0.0 dB, Train Loss: 4.8661502660252154e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7889 minutes\n",
      "encoder learning rate: 2.18e-04, decoder learning rate: 2.18e-04\n",
      "[362/500] At -2.0 dB, Train Loss: 0.027387667447328568 Train BER 0.0015405404847115278,                  \n",
      " [362/500] At 0.0 dB, Train Loss: 7.718569577264134e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7421 minutes\n",
      "encoder learning rate: 2.15e-04, decoder learning rate: 2.15e-04\n",
      "[363/500] At -2.0 dB, Train Loss: 0.013488898985087872 Train BER 0.0005783783853985369,                  \n",
      " [363/500] At 0.0 dB, Train Loss: 0.0001612029445823282 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8390 minutes\n",
      "encoder learning rate: 2.13e-04, decoder learning rate: 2.13e-04\n",
      "[364/500] At -2.0 dB, Train Loss: 0.0217218566685915 Train BER 0.0011081080883741379,                  \n",
      " [364/500] At 0.0 dB, Train Loss: 1.1951031410717405e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8121 minutes\n",
      "encoder learning rate: 2.10e-04, decoder learning rate: 2.10e-04\n",
      "[365/500] At -2.0 dB, Train Loss: 0.01978130266070366 Train BER 0.001059459405951202,                  \n",
      " [365/500] At 0.0 dB, Train Loss: 1.5717221685918048e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8268 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[366/500] At -2.0 dB, Train Loss: 0.017584700137376785 Train BER 0.000940540514420718,                  \n",
      " [366/500] At 0.0 dB, Train Loss: 0.0009187671821564436 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.8275 minutes\n",
      "encoder learning rate: 2.04e-04, decoder learning rate: 2.04e-04\n",
      "[367/500] At -2.0 dB, Train Loss: 0.022055355831980705 Train BER 0.0010540540097281337,                  \n",
      " [367/500] At 0.0 dB, Train Loss: 9.260038495995104e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8677 minutes\n",
      "encoder learning rate: 2.01e-04, decoder learning rate: 2.01e-04\n",
      "[368/500] At -2.0 dB, Train Loss: 0.019555171951651573 Train BER 0.0009729729499667883,                  \n",
      " [368/500] At 0.0 dB, Train Loss: 9.820005288929678e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8202 minutes\n",
      "encoder learning rate: 1.99e-04, decoder learning rate: 1.99e-04\n",
      "[369/500] At -2.0 dB, Train Loss: 0.018890032544732094 Train BER 0.0009297297219745815,                  \n",
      " [369/500] At 0.0 dB, Train Loss: 0.0003964646894019097 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.8659 minutes\n",
      "encoder learning rate: 1.96e-04, decoder learning rate: 1.96e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370/500] At -2.0 dB, Train Loss: 0.025374457240104675 Train BER 0.0014864865224808455,                  \n",
      " [370/500] At 0.0 dB, Train Loss: 0.0006059925653971732 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.7766 minutes\n",
      "encoder learning rate: 1.93e-04, decoder learning rate: 1.93e-04\n",
      "[371/500] At -2.0 dB, Train Loss: 0.026120871305465698 Train BER 0.001232432434335351,                  \n",
      " [371/500] At 0.0 dB, Train Loss: 1.5532816178165376e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9002 minutes\n",
      "encoder learning rate: 1.90e-04, decoder learning rate: 1.90e-04\n",
      "[372/500] At -2.0 dB, Train Loss: 0.018257809802889824 Train BER 0.0008918918902054429,                  \n",
      " [372/500] At 0.0 dB, Train Loss: 1.0219930118182674e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8755 minutes\n",
      "encoder learning rate: 1.88e-04, decoder learning rate: 1.88e-04\n",
      "[373/500] At -2.0 dB, Train Loss: 0.02109287492930889 Train BER 0.0010324324248358607,                  \n",
      " [373/500] At 0.0 dB, Train Loss: 2.7688685804605484e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8243 minutes\n",
      "encoder learning rate: 1.85e-04, decoder learning rate: 1.85e-04\n",
      "[374/500] At -2.0 dB, Train Loss: 0.020843276754021645 Train BER 0.001059459405951202,                  \n",
      " [374/500] At 0.0 dB, Train Loss: 3.742089029401541e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8071 minutes\n",
      "encoder learning rate: 1.82e-04, decoder learning rate: 1.82e-04\n",
      "[375/500] At -2.0 dB, Train Loss: 0.022118033841252327 Train BER 0.0009513513650745153,                  \n",
      " [375/500] At 0.0 dB, Train Loss: 4.55179724667687e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8724 minutes\n",
      "encoder learning rate: 1.79e-04, decoder learning rate: 1.79e-04\n",
      "[376/500] At -2.0 dB, Train Loss: 0.020328443497419357 Train BER 0.000989189138635993,                  \n",
      " [376/500] At 0.0 dB, Train Loss: 1.1304085091978777e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9099 minutes\n",
      "encoder learning rate: 1.77e-04, decoder learning rate: 1.77e-04\n",
      "[377/500] At -2.0 dB, Train Loss: 0.02775542251765728 Train BER 0.0012756757205352187,                  \n",
      " [377/500] At 0.0 dB, Train Loss: 2.5841685783234425e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8423 minutes\n",
      "encoder learning rate: 1.74e-04, decoder learning rate: 1.74e-04\n",
      "[378/500] At -2.0 dB, Train Loss: 0.01765899546444416 Train BER 0.0009351351181976497,                  \n",
      " [378/500] At 0.0 dB, Train Loss: 3.741915861610323e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7797 minutes\n",
      "encoder learning rate: 1.71e-04, decoder learning rate: 1.71e-04\n",
      "[379/500] At -2.0 dB, Train Loss: 0.016584979370236397 Train BER 0.0008432432659901679,                  \n",
      " [379/500] At 0.0 dB, Train Loss: 4.0453465771861374e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8148 minutes\n",
      "encoder learning rate: 1.69e-04, decoder learning rate: 1.69e-04\n",
      "[380/500] At -2.0 dB, Train Loss: 0.01526249572634697 Train BER 0.0006918918807059526,                  \n",
      " [380/500] At 0.0 dB, Train Loss: 9.69038865150651e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8652 minutes\n",
      "encoder learning rate: 1.66e-04, decoder learning rate: 1.66e-04\n",
      "[381/500] At -2.0 dB, Train Loss: 0.02141623944044113 Train BER 0.000994594651274383,                  \n",
      " [381/500] At 0.0 dB, Train Loss: 1.383647759212181e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9130 minutes\n",
      "encoder learning rate: 1.64e-04, decoder learning rate: 1.64e-04\n",
      "[382/500] At -2.0 dB, Train Loss: 0.017822207883000374 Train BER 0.000940540514420718,                  \n",
      " [382/500] At 0.0 dB, Train Loss: 2.5204444682458416e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8699 minutes\n",
      "encoder learning rate: 1.61e-04, decoder learning rate: 1.61e-04\n",
      "[383/500] At -2.0 dB, Train Loss: 0.01872164197266102 Train BER 0.0009243243257515132,                  \n",
      " [383/500] At 0.0 dB, Train Loss: 6.376757937687216e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8765 minutes\n",
      "encoder learning rate: 1.59e-04, decoder learning rate: 1.59e-04\n",
      "[384/500] At -2.0 dB, Train Loss: 0.027395887300372124 Train BER 0.001356756780296564,                  \n",
      " [384/500] At 0.0 dB, Train Loss: 1.0897542779275682e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8422 minutes\n",
      "encoder learning rate: 1.56e-04, decoder learning rate: 1.56e-04\n",
      "[385/500] At -2.0 dB, Train Loss: 0.023886030539870262 Train BER 0.001210810849443078,                  \n",
      " [385/500] At 0.0 dB, Train Loss: 8.314076694659889e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.9214 minutes\n",
      "encoder learning rate: 1.54e-04, decoder learning rate: 1.54e-04\n",
      "[386/500] At -2.0 dB, Train Loss: 0.024543264880776405 Train BER 0.0011729729594662786,                  \n",
      " [386/500] At 0.0 dB, Train Loss: 6.819120608270168e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8478 minutes\n",
      "encoder learning rate: 1.51e-04, decoder learning rate: 1.51e-04\n",
      "[387/500] At -2.0 dB, Train Loss: 0.01459611114114523 Train BER 0.0006486486527137458,                  \n",
      " [387/500] At 0.0 dB, Train Loss: 4.0591214201413095e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8540 minutes\n",
      "encoder learning rate: 1.49e-04, decoder learning rate: 1.49e-04\n",
      "[388/500] At -2.0 dB, Train Loss: 0.020759692415595055 Train BER 0.001064864918589592,                  \n",
      " [388/500] At 0.0 dB, Train Loss: 2.5411516617168672e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9260 minutes\n",
      "encoder learning rate: 1.46e-04, decoder learning rate: 1.46e-04\n",
      "[389/500] At -2.0 dB, Train Loss: 0.01986340805888176 Train BER 0.0009567567612975836,                  \n",
      " [389/500] At 0.0 dB, Train Loss: 1.8574755813460797e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7244 minutes\n",
      "encoder learning rate: 1.44e-04, decoder learning rate: 1.44e-04\n",
      "[390/500] At -2.0 dB, Train Loss: 0.021835889667272568 Train BER 0.001037837821058929,                  \n",
      " [390/500] At 0.0 dB, Train Loss: 1.0169826055062003e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9220 minutes\n",
      "encoder learning rate: 1.41e-04, decoder learning rate: 1.41e-04\n",
      "[391/500] At -2.0 dB, Train Loss: 0.018682405352592468 Train BER 0.0010000000474974513,                  \n",
      " [391/500] At 0.0 dB, Train Loss: 5.970173788227839e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8410 minutes\n",
      "encoder learning rate: 1.39e-04, decoder learning rate: 1.39e-04\n",
      "[392/500] At -2.0 dB, Train Loss: 0.019907372072339058 Train BER 0.0009081081370823085,                  \n",
      " [392/500] At 0.0 dB, Train Loss: 7.302737230929779e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8293 minutes\n",
      "encoder learning rate: 1.36e-04, decoder learning rate: 1.36e-04\n",
      "[393/500] At -2.0 dB, Train Loss: 0.02133478969335556 Train BER 0.0010270270286127925,                  \n",
      " [393/500] At 0.0 dB, Train Loss: 2.1875534002901986e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8401 minutes\n",
      "encoder learning rate: 1.34e-04, decoder learning rate: 1.34e-04\n",
      "[394/500] At -2.0 dB, Train Loss: 0.026036178693175316 Train BER 0.001281081116758287,                  \n",
      " [394/500] At 0.0 dB, Train Loss: 6.839462730567902e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7663 minutes\n",
      "encoder learning rate: 1.32e-04, decoder learning rate: 1.32e-04\n",
      "[395/500] At -2.0 dB, Train Loss: 0.02334330976009369 Train BER 0.0012162162456661463,                  \n",
      " [395/500] At 0.0 dB, Train Loss: 3.402974107302725e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7168 minutes\n",
      "encoder learning rate: 1.29e-04, decoder learning rate: 1.29e-04\n",
      "[396/500] At -2.0 dB, Train Loss: 0.012640408240258694 Train BER 0.0005891891778446734,                  \n",
      " [396/500] At 0.0 dB, Train Loss: 1.3797351130051538e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7264 minutes\n",
      "encoder learning rate: 1.27e-04, decoder learning rate: 1.27e-04\n",
      "[397/500] At -2.0 dB, Train Loss: 0.016601799055933952 Train BER 0.0007567567517980933,                  \n",
      " [397/500] At 0.0 dB, Train Loss: 2.286595008627046e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9241 minutes\n",
      "encoder learning rate: 1.25e-04, decoder learning rate: 1.25e-04\n",
      "[398/500] At -2.0 dB, Train Loss: 0.021022846922278404 Train BER 0.0009513513650745153,                  \n",
      " [398/500] At 0.0 dB, Train Loss: 7.53454805817455e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8828 minutes\n",
      "encoder learning rate: 1.22e-04, decoder learning rate: 1.22e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/500] At -2.0 dB, Train Loss: 0.021560275927186012 Train BER 0.0010432432172819972,                  \n",
      " [399/500] At 0.0 dB, Train Loss: 1.331386374658905e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7982 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[400/500] At -2.0 dB, Train Loss: 0.02490381710231304 Train BER 0.0011405405821278691,                  \n",
      " [400/500] At 0.0 dB, Train Loss: 7.452341833413811e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8781 minutes\n",
      "encoder learning rate: 1.18e-04, decoder learning rate: 1.18e-04\n",
      "[401/500] At -2.0 dB, Train Loss: 0.019634082913398743 Train BER 0.0009675675537437201,                  \n",
      " [401/500] At 0.0 dB, Train Loss: 9.706855053082108e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8295 minutes\n",
      "encoder learning rate: 1.16e-04, decoder learning rate: 1.16e-04\n",
      "[402/500] At -2.0 dB, Train Loss: 0.019646119326353073 Train BER 0.0009621621575206518,                  \n",
      " [402/500] At 0.0 dB, Train Loss: 2.4675782697158866e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8605 minutes\n",
      "encoder learning rate: 1.13e-04, decoder learning rate: 1.13e-04\n",
      "[403/500] At -2.0 dB, Train Loss: 0.023798223584890366 Train BER 0.0012054054532200098,                  \n",
      " [403/500] At 0.0 dB, Train Loss: 0.00010181512334384024 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7863 minutes\n",
      "encoder learning rate: 1.11e-04, decoder learning rate: 1.11e-04\n",
      "[404/500] At -2.0 dB, Train Loss: 0.015905573964118958 Train BER 0.0006918918807059526,                  \n",
      " [404/500] At 0.0 dB, Train Loss: 1.917478584800847e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9758 minutes\n",
      "encoder learning rate: 1.09e-04, decoder learning rate: 1.09e-04\n",
      "[405/500] At -2.0 dB, Train Loss: 0.018211089074611664 Train BER 0.0008918918902054429,                  \n",
      " [405/500] At 0.0 dB, Train Loss: 4.557494685286656e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7973 minutes\n",
      "encoder learning rate: 1.07e-04, decoder learning rate: 1.07e-04\n",
      "[406/500] At -2.0 dB, Train Loss: 0.01672525890171528 Train BER 0.0009297297219745815,                  \n",
      " [406/500] At 0.0 dB, Train Loss: 1.0738518540165387e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8264 minutes\n",
      "encoder learning rate: 1.05e-04, decoder learning rate: 1.05e-04\n",
      "[407/500] At -2.0 dB, Train Loss: 0.017778977751731873 Train BER 0.0008432432659901679,                  \n",
      " [407/500] At 0.0 dB, Train Loss: 1.67565667652525e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8266 minutes\n",
      "encoder learning rate: 1.03e-04, decoder learning rate: 1.03e-04\n",
      "[408/500] At -2.0 dB, Train Loss: 0.017643282189965248 Train BER 0.0007945945835672319,                  \n",
      " [408/500] At 0.0 dB, Train Loss: 4.7919206735969055e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.6638 minutes\n",
      "encoder learning rate: 1.01e-04, decoder learning rate: 1.01e-04\n",
      "[409/500] At -2.0 dB, Train Loss: 0.016813771799206734 Train BER 0.0007729729986749589,                  \n",
      " [409/500] At 0.0 dB, Train Loss: 2.263064197904896e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7758 minutes\n",
      "encoder learning rate: 9.85e-05, decoder learning rate: 9.85e-05\n",
      "[410/500] At -2.0 dB, Train Loss: 0.025595255196094513 Train BER 0.0013405405916273594,                  \n",
      " [410/500] At 0.0 dB, Train Loss: 4.9165690143126994e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7078 minutes\n",
      "encoder learning rate: 9.64e-05, decoder learning rate: 9.64e-05\n",
      "[411/500] At -2.0 dB, Train Loss: 0.016917414963245392 Train BER 0.0007999999797903001,                  \n",
      " [411/500] At 0.0 dB, Train Loss: 1.126676670537563e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6602 minutes\n",
      "encoder learning rate: 9.44e-05, decoder learning rate: 9.44e-05\n",
      "[412/500] At -2.0 dB, Train Loss: 0.016763679683208466 Train BER 0.0007459459593519568,                  \n",
      " [412/500] At 0.0 dB, Train Loss: 5.5147124840004835e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8949 minutes\n",
      "encoder learning rate: 9.23e-05, decoder learning rate: 9.23e-05\n",
      "[413/500] At -2.0 dB, Train Loss: 0.015419427305459976 Train BER 0.0007189189200289547,                  \n",
      " [413/500] At 0.0 dB, Train Loss: 1.0597179425531067e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7622 minutes\n",
      "encoder learning rate: 9.03e-05, decoder learning rate: 9.03e-05\n",
      "[414/500] At -2.0 dB, Train Loss: 0.020608840510249138 Train BER 0.0010270270286127925,                  \n",
      " [414/500] At 0.0 dB, Train Loss: 7.906725841166917e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7695 minutes\n",
      "encoder learning rate: 8.84e-05, decoder learning rate: 8.84e-05\n",
      "[415/500] At -2.0 dB, Train Loss: 0.017431704327464104 Train BER 0.0007999999797903001,                  \n",
      " [415/500] At 0.0 dB, Train Loss: 1.4392358025361318e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6754 minutes\n",
      "encoder learning rate: 8.64e-05, decoder learning rate: 8.64e-05\n",
      "[416/500] At -2.0 dB, Train Loss: 0.019373571500182152 Train BER 0.0010054054437205195,                  \n",
      " [416/500] At 0.0 dB, Train Loss: 4.546240234049037e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8567 minutes\n",
      "encoder learning rate: 8.45e-05, decoder learning rate: 8.45e-05\n",
      "[417/500] At -2.0 dB, Train Loss: 0.019517026841640472 Train BER 0.0009081081370823085,                  \n",
      " [417/500] At 0.0 dB, Train Loss: 2.362094892305322e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9267 minutes\n",
      "encoder learning rate: 8.25e-05, decoder learning rate: 8.25e-05\n",
      "[418/500] At -2.0 dB, Train Loss: 0.019564172253012657 Train BER 0.001037837821058929,                  \n",
      " [418/500] At 0.0 dB, Train Loss: 0.00020456695347093046 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8671 minutes\n",
      "encoder learning rate: 8.06e-05, decoder learning rate: 8.06e-05\n",
      "[419/500] At -2.0 dB, Train Loss: 0.01466100849211216 Train BER 0.000724324316252023,                  \n",
      " [419/500] At 0.0 dB, Train Loss: 2.0584833691827953e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9060 minutes\n",
      "encoder learning rate: 7.88e-05, decoder learning rate: 7.88e-05\n",
      "[420/500] At -2.0 dB, Train Loss: 0.02103337273001671 Train BER 0.001016216236166656,                  \n",
      " [420/500] At 0.0 dB, Train Loss: 3.365285010659136e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8840 minutes\n",
      "encoder learning rate: 7.69e-05, decoder learning rate: 7.69e-05\n",
      "[421/500] At -2.0 dB, Train Loss: 0.015014712698757648 Train BER 0.0007783783948980272,                  \n",
      " [421/500] At 0.0 dB, Train Loss: 7.626349542988464e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.9209 minutes\n",
      "encoder learning rate: 7.51e-05, decoder learning rate: 7.51e-05\n",
      "[422/500] At -2.0 dB, Train Loss: 0.02332884632050991 Train BER 0.0011567567707970738,                  \n",
      " [422/500] At 0.0 dB, Train Loss: 4.997855285182595e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.9083 minutes\n",
      "encoder learning rate: 7.32e-05, decoder learning rate: 7.32e-05\n",
      "[423/500] At -2.0 dB, Train Loss: 0.015806740149855614 Train BER 0.0007837837911210954,                  \n",
      " [423/500] At 0.0 dB, Train Loss: 1.9834094928228296e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7911 minutes\n",
      "encoder learning rate: 7.14e-05, decoder learning rate: 7.14e-05\n",
      "[424/500] At -2.0 dB, Train Loss: 0.018707046285271645 Train BER 0.0008378378115594387,                  \n",
      " [424/500] At 0.0 dB, Train Loss: 1.6386034985771403e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8414 minutes\n",
      "encoder learning rate: 6.97e-05, decoder learning rate: 6.97e-05\n",
      "[425/500] At -2.0 dB, Train Loss: 0.019332803785800934 Train BER 0.000870270247105509,                  \n",
      " [425/500] At 0.0 dB, Train Loss: 5.9236917877569795e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7457 minutes\n",
      "encoder learning rate: 6.79e-05, decoder learning rate: 6.79e-05\n",
      "[426/500] At -2.0 dB, Train Loss: 0.00935483630746603 Train BER 0.000351351365679875,                  \n",
      " [426/500] At 0.0 dB, Train Loss: 8.254306158050895e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8593 minutes\n",
      "encoder learning rate: 6.62e-05, decoder learning rate: 6.62e-05\n",
      "[427/500] At -2.0 dB, Train Loss: 0.018488924950361252 Train BER 0.0008756757015362382,                  \n",
      " [427/500] At 0.0 dB, Train Loss: 2.617180325614754e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9342 minutes\n",
      "encoder learning rate: 6.45e-05, decoder learning rate: 6.45e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[428/500] At -2.0 dB, Train Loss: 0.016668813303112984 Train BER 0.0007729729986749589,                  \n",
      " [428/500] At 0.0 dB, Train Loss: 5.429033990367316e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8829 minutes\n",
      "encoder learning rate: 6.28e-05, decoder learning rate: 6.28e-05\n",
      "[429/500] At -2.0 dB, Train Loss: 0.02901601418852806 Train BER 0.0012594594154506922,                  \n",
      " [429/500] At 0.0 dB, Train Loss: 3.9792463212506846e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7599 minutes\n",
      "encoder learning rate: 6.11e-05, decoder learning rate: 6.11e-05\n",
      "[430/500] At -2.0 dB, Train Loss: 0.029980286955833435 Train BER 0.0015621621860191226,                  \n",
      " [430/500] At 0.0 dB, Train Loss: 1.9554248865460977e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8368 minutes\n",
      "encoder learning rate: 5.95e-05, decoder learning rate: 5.95e-05\n",
      "[431/500] At -2.0 dB, Train Loss: 0.02150692231953144 Train BER 0.000994594651274383,                  \n",
      " [431/500] At 0.0 dB, Train Loss: 1.0547861165832728e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7704 minutes\n",
      "encoder learning rate: 5.78e-05, decoder learning rate: 5.78e-05\n",
      "[432/500] At -2.0 dB, Train Loss: 0.019192863255739212 Train BER 0.0009081081370823085,                  \n",
      " [432/500] At 0.0 dB, Train Loss: 1.801628059183713e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8864 minutes\n",
      "encoder learning rate: 5.62e-05, decoder learning rate: 5.62e-05\n",
      "[433/500] At -2.0 dB, Train Loss: 0.015553904697299004 Train BER 0.0008648648508824408,                  \n",
      " [433/500] At 0.0 dB, Train Loss: 3.469425564617268e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8979 minutes\n",
      "encoder learning rate: 5.47e-05, decoder learning rate: 5.47e-05\n",
      "[434/500] At -2.0 dB, Train Loss: 0.020865067839622498 Train BER 0.0010270270286127925,                  \n",
      " [434/500] At 0.0 dB, Train Loss: 2.104432314808946e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9713 minutes\n",
      "encoder learning rate: 5.31e-05, decoder learning rate: 5.31e-05\n",
      "[435/500] At -2.0 dB, Train Loss: 0.01980653591454029 Train BER 0.0009675675537437201,                  \n",
      " [435/500] At 0.0 dB, Train Loss: 1.0944996574835386e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8497 minutes\n",
      "encoder learning rate: 5.16e-05, decoder learning rate: 5.16e-05\n",
      "[436/500] At -2.0 dB, Train Loss: 0.022435547783970833 Train BER 0.0011783783556893468,                  \n",
      " [436/500] At 0.0 dB, Train Loss: 1.844835060182959e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8650 minutes\n",
      "encoder learning rate: 5.00e-05, decoder learning rate: 5.00e-05\n",
      "[437/500] At -2.0 dB, Train Loss: 0.02300727553665638 Train BER 0.001254054019227624,                  \n",
      " [437/500] At 0.0 dB, Train Loss: 2.2579355572815984e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7233 minutes\n",
      "encoder learning rate: 4.85e-05, decoder learning rate: 4.85e-05\n",
      "[438/500] At -2.0 dB, Train Loss: 0.015450863167643547 Train BER 0.0007459459593519568,                  \n",
      " [438/500] At 0.0 dB, Train Loss: 5.500251427292824e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8768 minutes\n",
      "encoder learning rate: 4.71e-05, decoder learning rate: 4.71e-05\n",
      "[439/500] At -2.0 dB, Train Loss: 0.014732145704329014 Train BER 0.0006378378602676094,                  \n",
      " [439/500] At 0.0 dB, Train Loss: 2.346243491047062e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7734 minutes\n",
      "encoder learning rate: 4.56e-05, decoder learning rate: 4.56e-05\n",
      "[440/500] At -2.0 dB, Train Loss: 0.016336999833583832 Train BER 0.0007405405631288886,                  \n",
      " [440/500] At 0.0 dB, Train Loss: 1.259545751963742e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8854 minutes\n",
      "encoder learning rate: 4.42e-05, decoder learning rate: 4.42e-05\n",
      "[441/500] At -2.0 dB, Train Loss: 0.01262708567082882 Train BER 0.0005675675929524004,                  \n",
      " [441/500] At 0.0 dB, Train Loss: 0.0001425314403604716 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7555 minutes\n",
      "encoder learning rate: 4.28e-05, decoder learning rate: 4.28e-05\n",
      "[442/500] At -2.0 dB, Train Loss: 0.019066082313656807 Train BER 0.0009135135333053768,                  \n",
      " [442/500] At 0.0 dB, Train Loss: 5.973312454443658e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7809 minutes\n",
      "encoder learning rate: 4.14e-05, decoder learning rate: 4.14e-05\n",
      "[443/500] At -2.0 dB, Train Loss: 0.027223721146583557 Train BER 0.0014864865224808455,                  \n",
      " [443/500] At 0.0 dB, Train Loss: 4.6651177399326116e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9717 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[444/500] At -2.0 dB, Train Loss: 0.017668085172772408 Train BER 0.0008486486622132361,                  \n",
      " [444/500] At 0.0 dB, Train Loss: 8.005448034964502e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8391 minutes\n",
      "encoder learning rate: 3.87e-05, decoder learning rate: 3.87e-05\n",
      "[445/500] At -2.0 dB, Train Loss: 0.02175256237387657 Train BER 0.0010972972959280014,                  \n",
      " [445/500] At 0.0 dB, Train Loss: 1.4483190170722082e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8950 minutes\n",
      "encoder learning rate: 3.74e-05, decoder learning rate: 3.74e-05\n",
      "[446/500] At -2.0 dB, Train Loss: 0.018552882596850395 Train BER 0.0009459459688514471,                  \n",
      " [446/500] At 0.0 dB, Train Loss: 1.0780753655126318e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8985 minutes\n",
      "encoder learning rate: 3.61e-05, decoder learning rate: 3.61e-05\n",
      "[447/500] At -2.0 dB, Train Loss: 0.024867184460163116 Train BER 0.0012270270381122828,                  \n",
      " [447/500] At 0.0 dB, Train Loss: 1.2372871424304321e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8355 minutes\n",
      "encoder learning rate: 3.48e-05, decoder learning rate: 3.48e-05\n",
      "[448/500] At -2.0 dB, Train Loss: 0.012873864732682705 Train BER 0.0004918918712064624,                  \n",
      " [448/500] At 0.0 dB, Train Loss: 4.027779596071923e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7566 minutes\n",
      "encoder learning rate: 3.36e-05, decoder learning rate: 3.36e-05\n",
      "[449/500] At -2.0 dB, Train Loss: 0.020598236471414566 Train BER 0.0010270270286127925,                  \n",
      " [449/500] At 0.0 dB, Train Loss: 9.438907000003383e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.8353 minutes\n",
      "encoder learning rate: 3.23e-05, decoder learning rate: 3.23e-05\n",
      "[450/500] At -2.0 dB, Train Loss: 0.01756909303367138 Train BER 0.000821621622890234,                  \n",
      " [450/500] At 0.0 dB, Train Loss: 1.9150187654304318e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8292 minutes\n",
      "encoder learning rate: 3.11e-05, decoder learning rate: 3.11e-05\n",
      "[451/500] At -2.0 dB, Train Loss: 0.02003437839448452 Train BER 0.0010702703148126602,                  \n",
      " [451/500] At 0.0 dB, Train Loss: 1.0418895726616029e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7942 minutes\n",
      "encoder learning rate: 2.99e-05, decoder learning rate: 2.99e-05\n",
      "[452/500] At -2.0 dB, Train Loss: 0.01675860956311226 Train BER 0.000940540514420718,                  \n",
      " [452/500] At 0.0 dB, Train Loss: 6.94349073455669e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9618 minutes\n",
      "encoder learning rate: 2.88e-05, decoder learning rate: 2.88e-05\n",
      "[453/500] At -2.0 dB, Train Loss: 0.017692286521196365 Train BER 0.000821621622890234,                  \n",
      " [453/500] At 0.0 dB, Train Loss: 5.217772468313342e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.7949 minutes\n",
      "encoder learning rate: 2.76e-05, decoder learning rate: 2.76e-05\n",
      "[454/500] At -2.0 dB, Train Loss: 0.018899038434028625 Train BER 0.0009243243257515132,                  \n",
      " [454/500] At 0.0 dB, Train Loss: 1.091856938728597e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9237 minutes\n",
      "encoder learning rate: 2.65e-05, decoder learning rate: 2.65e-05\n",
      "[455/500] At -2.0 dB, Train Loss: 0.018026314675807953 Train BER 0.000870270247105509,                  \n",
      " [455/500] At 0.0 dB, Train Loss: 3.201055733370595e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8054 minutes\n",
      "encoder learning rate: 2.54e-05, decoder learning rate: 2.54e-05\n",
      "[456/500] At -2.0 dB, Train Loss: 0.024906940758228302 Train BER 0.0012864865129813552,                  \n",
      " [456/500] At 0.0 dB, Train Loss: 3.466287307674065e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7906 minutes\n",
      "encoder learning rate: 2.44e-05, decoder learning rate: 2.44e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[457/500] At -2.0 dB, Train Loss: 0.015406022779643536 Train BER 0.0007135135238058865,                  \n",
      " [457/500] At 0.0 dB, Train Loss: 8.621768211014569e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7756 minutes\n",
      "encoder learning rate: 2.33e-05, decoder learning rate: 2.33e-05\n",
      "[458/500] At -2.0 dB, Train Loss: 0.016912009567022324 Train BER 0.0007999999797903001,                  \n",
      " [458/500] At 0.0 dB, Train Loss: 9.165580195258372e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5804 minutes\n",
      "encoder learning rate: 2.23e-05, decoder learning rate: 2.23e-05\n",
      "[459/500] At -2.0 dB, Train Loss: 0.01952887699007988 Train BER 0.0010432432172819972,                  \n",
      " [459/500] At 0.0 dB, Train Loss: 4.4305052142590284e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5401 minutes\n",
      "encoder learning rate: 2.13e-05, decoder learning rate: 2.13e-05\n",
      "[460/500] At -2.0 dB, Train Loss: 0.017295630648732185 Train BER 0.0008162162266671658,                  \n",
      " [460/500] At 0.0 dB, Train Loss: 9.969607344828546e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6323 minutes\n",
      "encoder learning rate: 2.03e-05, decoder learning rate: 2.03e-05\n",
      "[461/500] At -2.0 dB, Train Loss: 0.016256868839263916 Train BER 0.0007783783948980272,                  \n",
      " [461/500] At 0.0 dB, Train Loss: 5.7160257711075246e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6928 minutes\n",
      "encoder learning rate: 1.94e-05, decoder learning rate: 1.94e-05\n",
      "[462/500] At -2.0 dB, Train Loss: 0.01829996518790722 Train BER 0.0008270270191133022,                  \n",
      " [462/500] At 0.0 dB, Train Loss: 8.304154107463546e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5526 minutes\n",
      "encoder learning rate: 1.85e-05, decoder learning rate: 1.85e-05\n",
      "[463/500] At -2.0 dB, Train Loss: 0.024389833211898804 Train BER 0.0012864865129813552,                  \n",
      " [463/500] At 0.0 dB, Train Loss: 1.0362753528170288e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5449 minutes\n",
      "encoder learning rate: 1.76e-05, decoder learning rate: 1.76e-05\n",
      "[464/500] At -2.0 dB, Train Loss: 0.01803126372396946 Train BER 0.0010432432172819972,                  \n",
      " [464/500] At 0.0 dB, Train Loss: 1.7979784388444386e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5930 minutes\n",
      "encoder learning rate: 1.67e-05, decoder learning rate: 1.67e-05\n",
      "[465/500] At -2.0 dB, Train Loss: 0.01638319157063961 Train BER 0.0008108108304440975,                  \n",
      " [465/500] At 0.0 dB, Train Loss: 9.206275717588142e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6270 minutes\n",
      "encoder learning rate: 1.58e-05, decoder learning rate: 1.58e-05\n",
      "[466/500] At -2.0 dB, Train Loss: 0.017562992870807648 Train BER 0.0007405405631288886,                  \n",
      " [466/500] At 0.0 dB, Train Loss: 1.5234370039252099e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6170 minutes\n",
      "encoder learning rate: 1.50e-05, decoder learning rate: 1.50e-05\n",
      "[467/500] At -2.0 dB, Train Loss: 0.01833629235625267 Train BER 0.0008054054342210293,                  \n",
      " [467/500] At 0.0 dB, Train Loss: 6.8270428528194316e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5406 minutes\n",
      "encoder learning rate: 1.42e-05, decoder learning rate: 1.42e-05\n",
      "[468/500] At -2.0 dB, Train Loss: 0.019212357699871063 Train BER 0.0010054054437205195,                  \n",
      " [468/500] At 0.0 dB, Train Loss: 2.706381565076299e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6704 minutes\n",
      "encoder learning rate: 1.34e-05, decoder learning rate: 1.34e-05\n",
      "[469/500] At -2.0 dB, Train Loss: 0.027269138023257256 Train BER 0.0014594594249501824,                  \n",
      " [469/500] At 0.0 dB, Train Loss: 1.2751200301863719e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5827 minutes\n",
      "encoder learning rate: 1.27e-05, decoder learning rate: 1.27e-05\n",
      "[470/500] At -2.0 dB, Train Loss: 0.023385103791952133 Train BER 0.0011405405821278691,                  \n",
      " [470/500] At 0.0 dB, Train Loss: 1.75918685272336e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4164 minutes\n",
      "encoder learning rate: 1.19e-05, decoder learning rate: 1.19e-05\n",
      "[471/500] At -2.0 dB, Train Loss: 0.019083887338638306 Train BER 0.0010270270286127925,                  \n",
      " [471/500] At 0.0 dB, Train Loss: 7.660588380531408e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4623 minutes\n",
      "encoder learning rate: 1.12e-05, decoder learning rate: 1.12e-05\n",
      "[472/500] At -2.0 dB, Train Loss: 0.016639798879623413 Train BER 0.0008648648508824408,                  \n",
      " [472/500] At 0.0 dB, Train Loss: 1.2268891623534728e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4313 minutes\n",
      "encoder learning rate: 1.05e-05, decoder learning rate: 1.05e-05\n",
      "[473/500] At -2.0 dB, Train Loss: 0.015121701173484325 Train BER 0.0007837837911210954,                  \n",
      " [473/500] At 0.0 dB, Train Loss: 8.33450940262992e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4063 minutes\n",
      "encoder learning rate: 9.85e-06, decoder learning rate: 9.85e-06\n",
      "[474/500] At -2.0 dB, Train Loss: 0.016934709623456 Train BER 0.0007999999797903001,                  \n",
      " [474/500] At 0.0 dB, Train Loss: 4.3704530980903655e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.3625 minutes\n",
      "encoder learning rate: 9.21e-06, decoder learning rate: 9.21e-06\n",
      "[475/500] At -2.0 dB, Train Loss: 0.02194046601653099 Train BER 0.001086486503481865,                  \n",
      " [475/500] At 0.0 dB, Train Loss: 1.949107536347583e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4182 minutes\n",
      "encoder learning rate: 8.59e-06, decoder learning rate: 8.59e-06\n",
      "[476/500] At -2.0 dB, Train Loss: 0.022754287347197533 Train BER 0.0011189188808202744,                  \n",
      " [476/500] At 0.0 dB, Train Loss: 1.3551821211876813e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4123 minutes\n",
      "encoder learning rate: 7.99e-06, decoder learning rate: 7.99e-06\n",
      "[477/500] At -2.0 dB, Train Loss: 0.016197433695197105 Train BER 0.0007621621480211616,                  \n",
      " [477/500] At 0.0 dB, Train Loss: 9.107188816415146e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4663 minutes\n",
      "encoder learning rate: 7.43e-06, decoder learning rate: 7.43e-06\n",
      "[478/500] At -2.0 dB, Train Loss: 0.022881120443344116 Train BER 0.0011189188808202744,                  \n",
      " [478/500] At 0.0 dB, Train Loss: 1.9914279619115405e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6276 minutes\n",
      "encoder learning rate: 6.88e-06, decoder learning rate: 6.88e-06\n",
      "[479/500] At -2.0 dB, Train Loss: 0.02188766747713089 Train BER 0.0011675675632432103,                  \n",
      " [479/500] At 0.0 dB, Train Loss: 5.857858923263848e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6181 minutes\n",
      "encoder learning rate: 6.36e-06, decoder learning rate: 6.36e-06\n",
      "[480/500] At -2.0 dB, Train Loss: 0.013455328531563282 Train BER 0.0006540540489368141,                  \n",
      " [480/500] At 0.0 dB, Train Loss: 1.1747762073355261e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6143 minutes\n",
      "encoder learning rate: 5.86e-06, decoder learning rate: 5.86e-06\n",
      "[481/500] At -2.0 dB, Train Loss: 0.01800699345767498 Train BER 0.0008540540584363043,                  \n",
      " [481/500] At 0.0 dB, Train Loss: 2.0772939024027437e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6483 minutes\n",
      "encoder learning rate: 5.39e-06, decoder learning rate: 5.39e-06\n",
      "[482/500] At -2.0 dB, Train Loss: 0.01394551619887352 Train BER 0.0006486486527137458,                  \n",
      " [482/500] At 0.0 dB, Train Loss: 9.482405403105076e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5597 minutes\n",
      "encoder learning rate: 4.94e-06, decoder learning rate: 4.94e-06\n",
      "[483/500] At -2.0 dB, Train Loss: 0.019033050164580345 Train BER 0.0008756757015362382,                  \n",
      " [483/500] At 0.0 dB, Train Loss: 4.8703095671953633e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6079 minutes\n",
      "encoder learning rate: 4.51e-06, decoder learning rate: 4.51e-06\n",
      "[484/500] At -2.0 dB, Train Loss: 0.023653544485569 Train BER 0.001086486503481865,                  \n",
      " [484/500] At 0.0 dB, Train Loss: 1.7956785086425953e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4854 minutes\n",
      "encoder learning rate: 4.11e-06, decoder learning rate: 4.11e-06\n",
      "[485/500] At -2.0 dB, Train Loss: 0.022112255915999413 Train BER 0.001162162167020142,                  \n",
      " [485/500] At 0.0 dB, Train Loss: 1.0349348485760856e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4304 minutes\n",
      "encoder learning rate: 3.74e-06, decoder learning rate: 3.74e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[486/500] At -2.0 dB, Train Loss: 0.015666043385863304 Train BER 0.0007675675442442298,                  \n",
      " [486/500] At 0.0 dB, Train Loss: 1.1839146282000002e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4744 minutes\n",
      "encoder learning rate: 3.38e-06, decoder learning rate: 3.38e-06\n",
      "[487/500] At -2.0 dB, Train Loss: 0.017975473776459694 Train BER 0.000994594651274383,                  \n",
      " [487/500] At 0.0 dB, Train Loss: 1.4673090845462866e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4282 minutes\n",
      "encoder learning rate: 3.06e-06, decoder learning rate: 3.06e-06\n",
      "[488/500] At -2.0 dB, Train Loss: 0.01936698704957962 Train BER 0.0008972972864285111,                  \n",
      " [488/500] At 0.0 dB, Train Loss: 2.7522759410203435e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4630 minutes\n",
      "encoder learning rate: 2.75e-06, decoder learning rate: 2.75e-06\n",
      "[489/500] At -2.0 dB, Train Loss: 0.018850337713956833 Train BER 0.0009459459688514471,                  \n",
      " [489/500] At 0.0 dB, Train Loss: 1.8414209989714436e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.5108 minutes\n",
      "encoder learning rate: 2.47e-06, decoder learning rate: 2.47e-06\n",
      "[490/500] At -2.0 dB, Train Loss: 0.019221633672714233 Train BER 0.0010270270286127925,                  \n",
      " [490/500] At 0.0 dB, Train Loss: 1.9493398212944157e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4899 minutes\n",
      "encoder learning rate: 2.22e-06, decoder learning rate: 2.22e-06\n",
      "[491/500] At -2.0 dB, Train Loss: 0.01810428872704506 Train BER 0.0009081081370823085,                  \n",
      " [491/500] At 0.0 dB, Train Loss: 1.1793644262070302e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4493 minutes\n",
      "encoder learning rate: 1.99e-06, decoder learning rate: 1.99e-06\n",
      "[492/500] At -2.0 dB, Train Loss: 0.01526689063757658 Train BER 0.0007081081275828183,                  \n",
      " [492/500] At 0.0 dB, Train Loss: 2.7259733542450704e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4463 minutes\n",
      "encoder learning rate: 1.78e-06, decoder learning rate: 1.78e-06\n",
      "[493/500] At -2.0 dB, Train Loss: 0.02288486622273922 Train BER 0.0011513513745740056,                  \n",
      " [493/500] At 0.0 dB, Train Loss: 0.00011847414134535939 Train BER 0.0\n",
      "Time for one full iteration is 8.4387 minutes\n",
      "encoder learning rate: 1.60e-06, decoder learning rate: 1.60e-06\n",
      "[494/500] At -2.0 dB, Train Loss: 0.023210473358631134 Train BER 0.0011297296732664108,                  \n",
      " [494/500] At 0.0 dB, Train Loss: 3.184106026310474e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.4588 minutes\n",
      "encoder learning rate: 1.44e-06, decoder learning rate: 1.44e-06\n",
      "[495/500] At -2.0 dB, Train Loss: 0.018626181408762932 Train BER 0.0009675675537437201,                  \n",
      " [495/500] At 0.0 dB, Train Loss: 7.059158633637708e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.4971 minutes\n",
      "encoder learning rate: 1.30e-06, decoder learning rate: 1.30e-06\n",
      "[496/500] At -2.0 dB, Train Loss: 0.014083374291658401 Train BER 0.0006486486527137458,                  \n",
      " [496/500] At 0.0 dB, Train Loss: 6.0975462474743836e-06 Train BER 0.0\n",
      "Time for one full iteration is 8.5518 minutes\n",
      "encoder learning rate: 1.19e-06, decoder learning rate: 1.19e-06\n",
      "[497/500] At -2.0 dB, Train Loss: 0.02475305087864399 Train BER 0.0011135134845972061,                  \n",
      " [497/500] At 0.0 dB, Train Loss: 1.2578225323522929e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7716 minutes\n",
      "encoder learning rate: 1.11e-06, decoder learning rate: 1.11e-06\n",
      "[498/500] At -2.0 dB, Train Loss: 0.01792057417333126 Train BER 0.0007459459593519568,                  \n",
      " [498/500] At 0.0 dB, Train Loss: 2.815556035784539e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7849 minutes\n",
      "encoder learning rate: 1.05e-06, decoder learning rate: 1.05e-06\n",
      "[499/500] At -2.0 dB, Train Loss: 0.02628755383193493 Train BER 0.0012432432267814875,                  \n",
      " [499/500] At 0.0 dB, Train Loss: 0.00015371372865047306 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7759 minutes\n",
      "encoder learning rate: 1.01e-06, decoder learning rate: 1.01e-06\n",
      "[500/500] At -2.0 dB, Train Loss: 0.024181412532925606 Train BER 0.0010918918997049332,                  \n",
      " [500/500] At 0.0 dB, Train Loss: 3.221018050680868e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7777 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO__0.0_Encoder_KO_-2.0_Decoder/epochs_500_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166706, BLER: 0.435555\n",
      "Deep - BER: 0.125251, BLER: 0.463693\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072262, BLER: 0.196056\n",
      "Deep - BER: 0.043785, BLER: 0.191945\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.020103, BLER: 0.055856\n",
      "Deep - BER: 0.008898, BLER: 0.047700\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003025, BLER: 0.008546\n",
      "Deep - BER: 0.000902, BLER: 0.006658\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000200, BLER: 0.000573\n",
      "Deep - BER: 0.000043, BLER: 0.000507\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.125250918880105, 0.0437850269973278, 0.008898081081919373, 0.0009023783816301148, 4.256756744507584e-05]\n",
      "BERs of SC decoding: [0.16670613507926463, 0.07226191888749599, 0.02010270267445594, 0.0030245675667829345, 0.0001998378385014803]\n",
      "BLERs of DeepPolar: [0.4636930000000008, 0.1919449999999998, 0.04769999999999976, 0.006657999999999969, 0.0005070000000000003]\n",
      "BLERs of SC decoding: [0.4355549999999999, 0.19605599999999965, 0.05585599999999989, 0.008545999999999951, 0.0005730000000000004]\n",
      "time = 380.09973146915434 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
