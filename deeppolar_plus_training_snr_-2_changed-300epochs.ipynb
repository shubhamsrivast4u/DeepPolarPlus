{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = True  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 300  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = -2.  # SNR at which encoder is trained\n",
    "dec_train_snr = -4.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.001  # Decoder Learning rate\n",
    "enc_lr = 0.001  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}__{enc_train_snr}_Encoder_{decoder_type}_{dec_train_snr}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/300] At -4.0 dB, Train Loss: 8.56799602508545 Train BER 0.4838648736476898,                  \n",
      " [1/300] At -2.0 dB, Train Loss: 8.237852096557617 Train BER 0.4765189290046692\n",
      "Time for one full iteration is 7.3722 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[2/300] At -4.0 dB, Train Loss: 6.523617267608643 Train BER 0.47389188408851624,                  \n",
      " [2/300] At -2.0 dB, Train Loss: 6.454334259033203 Train BER 0.4706432521343231\n",
      "Time for one full iteration is 7.4387 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[3/300] At -4.0 dB, Train Loss: 6.030630111694336 Train BER 0.47462162375450134,                  \n",
      " [3/300] At -2.0 dB, Train Loss: 6.007735729217529 Train BER 0.4671081006526947\n",
      "Time for one full iteration is 7.4297 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[4/300] At -4.0 dB, Train Loss: 5.868807315826416 Train BER 0.4571243226528168,                  \n",
      " [4/300] At -2.0 dB, Train Loss: 5.816616535186768 Train BER 0.444356769323349\n",
      "Time for one full iteration is 7.3807 minutes\n",
      "encoder learning rate: 8.00e-05, decoder learning rate: 8.00e-05\n",
      "[5/300] At -4.0 dB, Train Loss: 5.706058502197266 Train BER 0.43208107352256775,                  \n",
      " [5/300] At -2.0 dB, Train Loss: 5.529763221740723 Train BER 0.40354594588279724\n",
      "Time for one full iteration is 7.3443 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[6/300] At -4.0 dB, Train Loss: 4.6914167404174805 Train BER 0.30954593420028687,                  \n",
      " [6/300] At -2.0 dB, Train Loss: 3.7494969367980957 Train BER 0.2345999926328659\n",
      "Time for one full iteration is 7.1532 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[7/300] At -4.0 dB, Train Loss: 3.2808663845062256 Train BER 0.18855135142803192,                  \n",
      " [7/300] At -2.0 dB, Train Loss: 1.635054111480713 Train BER 0.07855135202407837\n",
      "Time for one full iteration is 7.1865 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[8/300] At -4.0 dB, Train Loss: 2.414546489715576 Train BER 0.1305837780237198,                  \n",
      " [8/300] At -2.0 dB, Train Loss: 0.890199601650238 Train BER 0.038205403834581375\n",
      "Time for one full iteration is 7.2590 minutes\n",
      "encoder learning rate: 1.60e-04, decoder learning rate: 1.60e-04\n",
      "[9/300] At -4.0 dB, Train Loss: 1.985500693321228 Train BER 0.1052270233631134,                  \n",
      " [9/300] At -2.0 dB, Train Loss: 0.623534083366394 Train BER 0.027675675228238106\n",
      "Time for one full iteration is 7.2284 minutes\n",
      "encoder learning rate: 1.80e-04, decoder learning rate: 1.80e-04\n",
      "[10/300] At -4.0 dB, Train Loss: 1.6971261501312256 Train BER 0.08782702684402466,                  \n",
      " [10/300] At -2.0 dB, Train Loss: 0.4747409224510193 Train BER 0.021102702245116234\n",
      "Time for one full iteration is 7.2713 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[11/300] At -4.0 dB, Train Loss: 1.438582181930542 Train BER 0.07202161848545074,                  \n",
      " [11/300] At -2.0 dB, Train Loss: 0.39764708280563354 Train BER 0.01612432487308979\n",
      "Time for one full iteration is 7.3013 minutes\n",
      "encoder learning rate: 2.20e-04, decoder learning rate: 2.20e-04\n",
      "[12/300] At -4.0 dB, Train Loss: 1.2880134582519531 Train BER 0.062383782118558884,                  \n",
      " [12/300] At -2.0 dB, Train Loss: 0.24349957704544067 Train BER 0.007578378543257713\n",
      "Time for one full iteration is 7.5306 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[13/300] At -4.0 dB, Train Loss: 1.1664631366729736 Train BER 0.056578379124403,                  \n",
      " [13/300] At -2.0 dB, Train Loss: 0.2021971344947815 Train BER 0.006151351146399975\n",
      "Time for one full iteration is 7.5744 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[14/300] At -4.0 dB, Train Loss: 1.1345088481903076 Train BER 0.054205406457185745,                  \n",
      " [14/300] At -2.0 dB, Train Loss: 0.16605865955352783 Train BER 0.005059459246695042\n",
      "Time for one full iteration is 7.8226 minutes\n",
      "encoder learning rate: 2.80e-04, decoder learning rate: 2.80e-04\n",
      "[15/300] At -4.0 dB, Train Loss: 0.9997550249099731 Train BER 0.0478486493229866,                  \n",
      " [15/300] At -2.0 dB, Train Loss: 0.14234501123428345 Train BER 0.004308108240365982\n",
      "Time for one full iteration is 7.6144 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[16/300] At -4.0 dB, Train Loss: 1.048228144645691 Train BER 0.052183784544467926,                  \n",
      " [16/300] At -2.0 dB, Train Loss: 0.1283060610294342 Train BER 0.004156756680458784\n",
      "Time for one full iteration is 7.4928 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[17/300] At -4.0 dB, Train Loss: 1.0001070499420166 Train BER 0.04917297139763832,                  \n",
      " [17/300] At -2.0 dB, Train Loss: 0.13000355660915375 Train BER 0.0042540542781353\n",
      "Time for one full iteration is 7.3567 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[18/300] At -4.0 dB, Train Loss: 1.0516108274459839 Train BER 0.051940541714429855,                  \n",
      " [18/300] At -2.0 dB, Train Loss: 0.1339375078678131 Train BER 0.004540540743619204\n",
      "Time for one full iteration is 7.4523 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[19/300] At -4.0 dB, Train Loss: 1.0013530254364014 Train BER 0.05015675723552704,                  \n",
      " [19/300] At -2.0 dB, Train Loss: 0.12923988699913025 Train BER 0.004572973120957613\n",
      "Time for one full iteration is 7.4927 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[20/300] At -4.0 dB, Train Loss: 0.9613015055656433 Train BER 0.04731351509690285,                  \n",
      " [20/300] At -2.0 dB, Train Loss: 0.16030387580394745 Train BER 0.006140540353953838\n",
      "Time for one full iteration is 7.3499 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[21/300] At -4.0 dB, Train Loss: 0.9559729695320129 Train BER 0.04752432554960251,                  \n",
      " [21/300] At -2.0 dB, Train Loss: 0.1290673464536667 Train BER 0.0045621623285114765\n",
      "Time for one full iteration is 7.3846 minutes\n",
      "encoder learning rate: 4.20e-04, decoder learning rate: 4.20e-04\n",
      "[22/300] At -4.0 dB, Train Loss: 0.9124897122383118 Train BER 0.04504324495792389,                  \n",
      " [22/300] At -2.0 dB, Train Loss: 0.14253844320774078 Train BER 0.005378378555178642\n",
      "Time for one full iteration is 7.2778 minutes\n",
      "encoder learning rate: 4.40e-04, decoder learning rate: 4.40e-04\n",
      "[23/300] At -4.0 dB, Train Loss: 0.9468427896499634 Train BER 0.04723783954977989,                  \n",
      " [23/300] At -2.0 dB, Train Loss: 0.1381857693195343 Train BER 0.005524324253201485\n",
      "Time for one full iteration is 7.2479 minutes\n",
      "encoder learning rate: 4.60e-04, decoder learning rate: 4.60e-04\n",
      "[24/300] At -4.0 dB, Train Loss: 0.9522964954376221 Train BER 0.04858918860554695,                  \n",
      " [24/300] At -2.0 dB, Train Loss: 0.16002708673477173 Train BER 0.006102702580392361\n",
      "Time for one full iteration is 7.2769 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[25/300] At -4.0 dB, Train Loss: 0.9424296021461487 Train BER 0.04711891710758209,                  \n",
      " [25/300] At -2.0 dB, Train Loss: 0.16534583270549774 Train BER 0.006718919146806002\n",
      "Time for one full iteration is 7.2258 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[26/300] At -4.0 dB, Train Loss: 0.9694263339042664 Train BER 0.049340538680553436,                  \n",
      " [26/300] At -2.0 dB, Train Loss: 0.16621440649032593 Train BER 0.006351351272314787\n",
      "Time for one full iteration is 7.1596 minutes\n",
      "encoder learning rate: 5.20e-04, decoder learning rate: 5.20e-04\n",
      "[27/300] At -4.0 dB, Train Loss: 0.8976107835769653 Train BER 0.04463783651590347,                  \n",
      " [27/300] At -2.0 dB, Train Loss: 0.13891933858394623 Train BER 0.0052540539763867855\n",
      "Time for one full iteration is 7.1619 minutes\n",
      "encoder learning rate: 5.40e-04, decoder learning rate: 5.40e-04\n",
      "[28/300] At -4.0 dB, Train Loss: 0.912225067615509 Train BER 0.046286486089229584,                  \n",
      " [28/300] At -2.0 dB, Train Loss: 0.11378960311412811 Train BER 0.0038756756111979485\n",
      "Time for one full iteration is 7.1278 minutes\n",
      "encoder learning rate: 5.60e-04, decoder learning rate: 5.60e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/300] At -4.0 dB, Train Loss: 0.9123716354370117 Train BER 0.04455135017633438,                  \n",
      " [29/300] At -2.0 dB, Train Loss: 0.12191226333379745 Train BER 0.004600000102072954\n",
      "Time for one full iteration is 7.1359 minutes\n",
      "encoder learning rate: 5.80e-04, decoder learning rate: 5.80e-04\n",
      "[30/300] At -4.0 dB, Train Loss: 0.9006304144859314 Train BER 0.04520000144839287,                  \n",
      " [30/300] At -2.0 dB, Train Loss: 0.11894287168979645 Train BER 0.004167567472904921\n",
      "Time for one full iteration is 7.2509 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[31/300] At -4.0 dB, Train Loss: 0.8843364715576172 Train BER 0.04381081089377403,                  \n",
      " [31/300] At -2.0 dB, Train Loss: 0.129862979054451 Train BER 0.004600000102072954\n",
      "Time for one full iteration is 7.2413 minutes\n",
      "encoder learning rate: 6.20e-04, decoder learning rate: 6.20e-04\n",
      "[32/300] At -4.0 dB, Train Loss: 0.9437417387962341 Train BER 0.04828648641705513,                  \n",
      " [32/300] At -2.0 dB, Train Loss: 0.12252677977085114 Train BER 0.004572973120957613\n",
      "Time for one full iteration is 7.2942 minutes\n",
      "encoder learning rate: 6.40e-04, decoder learning rate: 6.40e-04\n",
      "[33/300] At -4.0 dB, Train Loss: 0.9422385692596436 Train BER 0.047935135662555695,                  \n",
      " [33/300] At -2.0 dB, Train Loss: 0.18338854610919952 Train BER 0.0073837838135659695\n",
      "Time for one full iteration is 7.2539 minutes\n",
      "encoder learning rate: 6.60e-04, decoder learning rate: 6.60e-04\n",
      "[34/300] At -4.0 dB, Train Loss: 0.9056642651557922 Train BER 0.04531892016530037,                  \n",
      " [34/300] At -2.0 dB, Train Loss: 0.1948557049036026 Train BER 0.008070270530879498\n",
      "Time for one full iteration is 7.2669 minutes\n",
      "encoder learning rate: 6.80e-04, decoder learning rate: 6.80e-04\n",
      "[35/300] At -4.0 dB, Train Loss: 1.021554946899414 Train BER 0.051729731261730194,                  \n",
      " [35/300] At -2.0 dB, Train Loss: 0.2887267470359802 Train BER 0.011772972531616688\n",
      "Time for one full iteration is 7.2387 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n",
      "[36/300] At -4.0 dB, Train Loss: 1.0787049531936646 Train BER 0.05508648604154587,                  \n",
      " [36/300] At -2.0 dB, Train Loss: 0.32712042331695557 Train BER 0.01441081054508686\n",
      "Time for one full iteration is 7.3015 minutes\n",
      "encoder learning rate: 7.20e-04, decoder learning rate: 7.20e-04\n",
      "[37/300] At -4.0 dB, Train Loss: 1.1530805826187134 Train BER 0.05908648669719696,                  \n",
      " [37/300] At -2.0 dB, Train Loss: 0.3720424175262451 Train BER 0.015902703627943993\n",
      "Time for one full iteration is 7.3198 minutes\n",
      "encoder learning rate: 7.40e-04, decoder learning rate: 7.40e-04\n",
      "[38/300] At -4.0 dB, Train Loss: 1.075362205505371 Train BER 0.05542702600359917,                  \n",
      " [38/300] At -2.0 dB, Train Loss: 0.27876412868499756 Train BER 0.012151351198554039\n",
      "Time for one full iteration is 7.2143 minutes\n",
      "encoder learning rate: 7.60e-04, decoder learning rate: 7.60e-04\n",
      "[39/300] At -4.0 dB, Train Loss: 1.1372156143188477 Train BER 0.0596756748855114,                  \n",
      " [39/300] At -2.0 dB, Train Loss: 0.28064078092575073 Train BER 0.011632432229816914\n",
      "Time for one full iteration is 7.1535 minutes\n",
      "encoder learning rate: 7.80e-04, decoder learning rate: 7.80e-04\n",
      "[40/300] At -4.0 dB, Train Loss: 1.0526061058044434 Train BER 0.05478378385305405,                  \n",
      " [40/300] At -2.0 dB, Train Loss: 0.1781618893146515 Train BER 0.006827027071267366\n",
      "Time for one full iteration is 7.2080 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[41/300] At -4.0 dB, Train Loss: 1.0064698457717896 Train BER 0.0506054051220417,                  \n",
      " [41/300] At -2.0 dB, Train Loss: 0.15876156091690063 Train BER 0.006589189171791077\n",
      "Time for one full iteration is 7.1966 minutes\n",
      "encoder learning rate: 8.20e-04, decoder learning rate: 8.20e-04\n",
      "[42/300] At -4.0 dB, Train Loss: 0.9261317849159241 Train BER 0.046745944768190384,                  \n",
      " [42/300] At -2.0 dB, Train Loss: 0.14000244438648224 Train BER 0.005464864894747734\n",
      "Time for one full iteration is 7.1719 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n",
      "[43/300] At -4.0 dB, Train Loss: 0.947975754737854 Train BER 0.048886485397815704,                  \n",
      " [43/300] At -2.0 dB, Train Loss: 0.2352580726146698 Train BER 0.009724324569106102\n",
      "Time for one full iteration is 7.1862 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[44/300] At -4.0 dB, Train Loss: 0.9645785093307495 Train BER 0.05000000074505806,                  \n",
      " [44/300] At -2.0 dB, Train Loss: 0.2019631415605545 Train BER 0.008145946078002453\n",
      "Time for one full iteration is 7.2194 minutes\n",
      "encoder learning rate: 8.80e-04, decoder learning rate: 8.80e-04\n",
      "[45/300] At -4.0 dB, Train Loss: 0.995036244392395 Train BER 0.049983784556388855,                  \n",
      " [45/300] At -2.0 dB, Train Loss: 0.1863933801651001 Train BER 0.00792432390153408\n",
      "Time for one full iteration is 7.3385 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[46/300] At -4.0 dB, Train Loss: 0.9580153226852417 Train BER 0.049010809510946274,                  \n",
      " [46/300] At -2.0 dB, Train Loss: 0.2788878083229065 Train BER 0.012005405500531197\n",
      "Time for one full iteration is 7.1344 minutes\n",
      "encoder learning rate: 9.20e-04, decoder learning rate: 9.20e-04\n",
      "[47/300] At -4.0 dB, Train Loss: 1.0139002799987793 Train BER 0.051724325865507126,                  \n",
      " [47/300] At -2.0 dB, Train Loss: 0.2765200734138489 Train BER 0.011718918569386005\n",
      "Time for one full iteration is 7.1219 minutes\n",
      "encoder learning rate: 9.40e-04, decoder learning rate: 9.40e-04\n",
      "[48/300] At -4.0 dB, Train Loss: 1.0260831117630005 Train BER 0.052772972732782364,                  \n",
      " [48/300] At -2.0 dB, Train Loss: 0.329462468624115 Train BER 0.014151351526379585\n",
      "Time for one full iteration is 7.1122 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[49/300] At -4.0 dB, Train Loss: 1.0926828384399414 Train BER 0.05602702870965004,                  \n",
      " [49/300] At -2.0 dB, Train Loss: 0.45647305250167847 Train BER 0.020275674760341644\n",
      "Time for one full iteration is 7.0954 minutes\n",
      "encoder learning rate: 9.80e-04, decoder learning rate: 9.80e-04\n",
      "[50/300] At -4.0 dB, Train Loss: 1.2798608541488647 Train BER 0.06594594568014145,                  \n",
      " [50/300] At -2.0 dB, Train Loss: 0.4868764281272888 Train BER 0.02065405435860157\n",
      "Time for one full iteration is 7.0949 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[51/300] At -4.0 dB, Train Loss: 1.1468161344528198 Train BER 0.05901621654629707,                  \n",
      " [51/300] At -2.0 dB, Train Loss: 0.6169089674949646 Train BER 0.02772432379424572\n",
      "Time for one full iteration is 7.1906 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[52/300] At -4.0 dB, Train Loss: 1.3877148628234863 Train BER 0.07192432135343552,                  \n",
      " [52/300] At -2.0 dB, Train Loss: 0.4836428761482239 Train BER 0.02111891843378544\n",
      "Time for one full iteration is 7.2146 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[53/300] At -4.0 dB, Train Loss: 1.1274844408035278 Train BER 0.05750270187854767,                  \n",
      " [53/300] At -2.0 dB, Train Loss: 0.5567748546600342 Train BER 0.025589188560843468\n",
      "Time for one full iteration is 7.2035 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[54/300] At -4.0 dB, Train Loss: 1.5199482440948486 Train BER 0.08097296953201294,                  \n",
      " [54/300] At -2.0 dB, Train Loss: 0.5143179297447205 Train BER 0.022162161767482758\n",
      "Time for one full iteration is 7.1434 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[55/300] At -4.0 dB, Train Loss: 1.0669567584991455 Train BER 0.05633513629436493,                  \n",
      " [55/300] At -2.0 dB, Train Loss: 0.497435063123703 Train BER 0.023518918082118034\n",
      "Time for one full iteration is 7.1741 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[56/300] At -4.0 dB, Train Loss: 1.4302181005477905 Train BER 0.07885945588350296,                  \n",
      " [56/300] At -2.0 dB, Train Loss: 0.4202416241168976 Train BER 0.019729729741811752\n",
      "Time for one full iteration is 7.1584 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/300] At -4.0 dB, Train Loss: 1.0102227926254272 Train BER 0.054237838834524155,                  \n",
      " [57/300] At -2.0 dB, Train Loss: 0.1672849804162979 Train BER 0.006313513498753309\n",
      "Time for one full iteration is 7.2211 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[58/300] At -4.0 dB, Train Loss: 1.104935646057129 Train BER 0.05972972884774208,                  \n",
      " [58/300] At -2.0 dB, Train Loss: 0.500330924987793 Train BER 0.015767566859722137\n",
      "Time for one full iteration is 7.2025 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[59/300] At -4.0 dB, Train Loss: 0.9315801858901978 Train BER 0.05018378421664238,                  \n",
      " [59/300] At -2.0 dB, Train Loss: 0.32956019043922424 Train BER 0.016729729250073433\n",
      "Time for one full iteration is 7.1835 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[60/300] At -4.0 dB, Train Loss: 1.4113481044769287 Train BER 0.07778378576040268,                  \n",
      " [60/300] At -2.0 dB, Train Loss: 0.7851618528366089 Train BER 0.023583784699440002\n",
      "Time for one full iteration is 7.0648 minutes\n",
      "encoder learning rate: 9.96e-04, decoder learning rate: 9.96e-04\n",
      "[61/300] At -4.0 dB, Train Loss: 0.9751571416854858 Train BER 0.0544000007212162,                  \n",
      " [61/300] At -2.0 dB, Train Loss: 0.3564942181110382 Train BER 0.017054054886102676\n",
      "Time for one full iteration is 7.0669 minutes\n",
      "encoder learning rate: 9.95e-04, decoder learning rate: 9.95e-04\n",
      "[62/300] At -4.0 dB, Train Loss: 1.0713064670562744 Train BER 0.05712432414293289,                  \n",
      " [62/300] At -2.0 dB, Train Loss: 0.301850825548172 Train BER 0.012616216205060482\n",
      "Time for one full iteration is 7.0418 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[63/300] At -4.0 dB, Train Loss: 0.854503870010376 Train BER 0.04510810971260071,                  \n",
      " [63/300] At -2.0 dB, Train Loss: 0.17765778303146362 Train BER 0.006978378165513277\n",
      "Time for one full iteration is 7.2240 minutes\n",
      "encoder learning rate: 9.93e-04, decoder learning rate: 9.93e-04\n",
      "[64/300] At -4.0 dB, Train Loss: 0.9814419150352478 Train BER 0.051362160593271255,                  \n",
      " [64/300] At -2.0 dB, Train Loss: 0.13692869246006012 Train BER 0.005556756630539894\n",
      "Time for one full iteration is 7.3891 minutes\n",
      "encoder learning rate: 9.92e-04, decoder learning rate: 9.92e-04\n",
      "[65/300] At -4.0 dB, Train Loss: 1.0115740299224854 Train BER 0.05376216396689415,                  \n",
      " [65/300] At -2.0 dB, Train Loss: 0.25458312034606934 Train BER 0.010627026669681072\n",
      "Time for one full iteration is 8.3208 minutes\n",
      "encoder learning rate: 9.91e-04, decoder learning rate: 9.91e-04\n",
      "[66/300] At -4.0 dB, Train Loss: 0.8612573742866516 Train BER 0.04583783820271492,                  \n",
      " [66/300] At -2.0 dB, Train Loss: 0.12780910730361938 Train BER 0.004610810894519091\n",
      "Time for one full iteration is 8.3542 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[67/300] At -4.0 dB, Train Loss: 1.0294843912124634 Train BER 0.055405404418706894,                  \n",
      " [67/300] At -2.0 dB, Train Loss: 0.48484548926353455 Train BER 0.012691891752183437\n",
      "Time for one full iteration is 8.3440 minutes\n",
      "encoder learning rate: 9.89e-04, decoder learning rate: 9.89e-04\n",
      "[68/300] At -4.0 dB, Train Loss: 0.8252716660499573 Train BER 0.045762162655591965,                  \n",
      " [68/300] At -2.0 dB, Train Loss: 0.21268413960933685 Train BER 0.009616216644644737\n",
      "Time for one full iteration is 8.3346 minutes\n",
      "encoder learning rate: 9.87e-04, decoder learning rate: 9.87e-04\n",
      "[69/300] At -4.0 dB, Train Loss: 0.7937453389167786 Train BER 0.04216216132044792,                  \n",
      " [69/300] At -2.0 dB, Train Loss: 0.17432455718517303 Train BER 0.0074756755493581295\n",
      "Time for one full iteration is 8.3673 minutes\n",
      "encoder learning rate: 9.86e-04, decoder learning rate: 9.86e-04\n",
      "[70/300] At -4.0 dB, Train Loss: 1.2092785835266113 Train BER 0.06611891835927963,                  \n",
      " [70/300] At -2.0 dB, Train Loss: 0.8762650489807129 Train BER 0.017464864999055862\n",
      "Time for one full iteration is 8.3437 minutes\n",
      "encoder learning rate: 9.84e-04, decoder learning rate: 9.84e-04\n",
      "[71/300] At -4.0 dB, Train Loss: 0.8460688591003418 Train BER 0.04763243347406387,                  \n",
      " [71/300] At -2.0 dB, Train Loss: 0.21193836629390717 Train BER 0.010902702808380127\n",
      "Time for one full iteration is 8.3835 minutes\n",
      "encoder learning rate: 9.83e-04, decoder learning rate: 9.83e-04\n",
      "[72/300] At -4.0 dB, Train Loss: 0.9395731091499329 Train BER 0.05291351303458214,                  \n",
      " [72/300] At -2.0 dB, Train Loss: 0.27888691425323486 Train BER 0.013259459286928177\n",
      "Time for one full iteration is 8.4655 minutes\n",
      "encoder learning rate: 9.81e-04, decoder learning rate: 9.81e-04\n",
      "[73/300] At -4.0 dB, Train Loss: 0.8112937808036804 Train BER 0.04430270195007324,                  \n",
      " [73/300] At -2.0 dB, Train Loss: 0.18087492883205414 Train BER 0.006664864718914032\n",
      "Time for one full iteration is 8.4608 minutes\n",
      "encoder learning rate: 9.79e-04, decoder learning rate: 9.79e-04\n",
      "[74/300] At -4.0 dB, Train Loss: 0.8078290224075317 Train BER 0.042821623384952545,                  \n",
      " [74/300] At -2.0 dB, Train Loss: 0.07808192074298859 Train BER 0.0022324323654174805\n",
      "Time for one full iteration is 8.4785 minutes\n",
      "encoder learning rate: 9.77e-04, decoder learning rate: 9.77e-04\n",
      "[75/300] At -4.0 dB, Train Loss: 0.8394570350646973 Train BER 0.044324323534965515,                  \n",
      " [75/300] At -2.0 dB, Train Loss: 0.25580716133117676 Train BER 0.010627026669681072\n",
      "Time for one full iteration is 8.4593 minutes\n",
      "encoder learning rate: 9.76e-04, decoder learning rate: 9.76e-04\n",
      "[76/300] At -4.0 dB, Train Loss: 0.7692053914070129 Train BER 0.040621623396873474,                  \n",
      " [76/300] At -2.0 dB, Train Loss: 0.1070626825094223 Train BER 0.003551351372152567\n",
      "Time for one full iteration is 8.4279 minutes\n",
      "encoder learning rate: 9.74e-04, decoder learning rate: 9.74e-04\n",
      "[77/300] At -4.0 dB, Train Loss: 0.8945431709289551 Train BER 0.04678918793797493,                  \n",
      " [77/300] At -2.0 dB, Train Loss: 0.31973519921302795 Train BER 0.011043243110179901\n",
      "Time for one full iteration is 8.5573 minutes\n",
      "encoder learning rate: 9.72e-04, decoder learning rate: 9.72e-04\n",
      "[78/300] At -4.0 dB, Train Loss: 0.8346565365791321 Train BER 0.04521621763706207,                  \n",
      " [78/300] At -2.0 dB, Train Loss: 0.17775532603263855 Train BER 0.008545946329832077\n",
      "Time for one full iteration is 8.4088 minutes\n",
      "encoder learning rate: 9.69e-04, decoder learning rate: 9.69e-04\n",
      "[79/300] At -4.0 dB, Train Loss: 0.7989526391029358 Train BER 0.04260540381073952,                  \n",
      " [79/300] At -2.0 dB, Train Loss: 0.13461169600486755 Train BER 0.0047891889698803425\n",
      "Time for one full iteration is 8.2163 minutes\n",
      "encoder learning rate: 9.67e-04, decoder learning rate: 9.67e-04\n",
      "[80/300] At -4.0 dB, Train Loss: 0.7687464356422424 Train BER 0.04039459303021431,                  \n",
      " [80/300] At -2.0 dB, Train Loss: 0.06471507251262665 Train BER 0.002248648554086685\n",
      "Time for one full iteration is 7.7534 minutes\n",
      "encoder learning rate: 9.65e-04, decoder learning rate: 9.65e-04\n",
      "[81/300] At -4.0 dB, Train Loss: 0.8454638719558716 Train BER 0.04489729553461075,                  \n",
      " [81/300] At -2.0 dB, Train Loss: 0.1289137750864029 Train BER 0.0053621623665094376\n",
      "Time for one full iteration is 7.3858 minutes\n",
      "encoder learning rate: 9.63e-04, decoder learning rate: 9.63e-04\n",
      "[82/300] At -4.0 dB, Train Loss: 0.7714396119117737 Train BER 0.03964864835143089,                  \n",
      " [82/300] At -2.0 dB, Train Loss: 0.07539397478103638 Train BER 0.002567567629739642\n",
      "Time for one full iteration is 7.5063 minutes\n",
      "encoder learning rate: 9.60e-04, decoder learning rate: 9.60e-04\n",
      "[83/300] At -4.0 dB, Train Loss: 0.89933180809021 Train BER 0.04822162166237831,                  \n",
      " [83/300] At -2.0 dB, Train Loss: 0.2303701788187027 Train BER 0.009399999864399433\n",
      "Time for one full iteration is 7.3882 minutes\n",
      "encoder learning rate: 9.58e-04, decoder learning rate: 9.58e-04\n",
      "[84/300] At -4.0 dB, Train Loss: 0.8573783040046692 Train BER 0.044508107006549835,                  \n",
      " [84/300] At -2.0 dB, Train Loss: 0.11803104728460312 Train BER 0.004243243020027876\n",
      "Time for one full iteration is 7.3703 minutes\n",
      "encoder learning rate: 9.55e-04, decoder learning rate: 9.55e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/300] At -4.0 dB, Train Loss: 0.8548495769500732 Train BER 0.0445135124027729,                  \n",
      " [85/300] At -2.0 dB, Train Loss: 0.144927978515625 Train BER 0.005864864680916071\n",
      "Time for one full iteration is 7.4546 minutes\n",
      "encoder learning rate: 9.52e-04, decoder learning rate: 9.52e-04\n",
      "[86/300] At -4.0 dB, Train Loss: 0.8015671968460083 Train BER 0.04105405509471893,                  \n",
      " [86/300] At -2.0 dB, Train Loss: 0.08505576848983765 Train BER 0.0030594593845307827\n",
      "Time for one full iteration is 7.4366 minutes\n",
      "encoder learning rate: 9.50e-04, decoder learning rate: 9.50e-04\n",
      "[87/300] At -4.0 dB, Train Loss: 0.8876160383224487 Train BER 0.04609189182519913,                  \n",
      " [87/300] At -2.0 dB, Train Loss: 0.10308369249105453 Train BER 0.0041891890577971935\n",
      "Time for one full iteration is 7.2929 minutes\n",
      "encoder learning rate: 9.47e-04, decoder learning rate: 9.47e-04\n",
      "[88/300] At -4.0 dB, Train Loss: 0.7839367389678955 Train BER 0.04156216233968735,                  \n",
      " [88/300] At -2.0 dB, Train Loss: 0.077874094247818 Train BER 0.0032486487179994583\n",
      "Time for one full iteration is 7.3609 minutes\n",
      "encoder learning rate: 9.44e-04, decoder learning rate: 9.44e-04\n",
      "[89/300] At -4.0 dB, Train Loss: 0.8733386993408203 Train BER 0.04586486518383026,                  \n",
      " [89/300] At -2.0 dB, Train Loss: 0.11258982121944427 Train BER 0.004686486441642046\n",
      "Time for one full iteration is 7.1377 minutes\n",
      "encoder learning rate: 9.41e-04, decoder learning rate: 9.41e-04\n",
      "[90/300] At -4.0 dB, Train Loss: 0.812732994556427 Train BER 0.041940540075302124,                  \n",
      " [90/300] At -2.0 dB, Train Loss: 0.06999281048774719 Train BER 0.0024972972460091114\n",
      "Time for one full iteration is 7.1142 minutes\n",
      "encoder learning rate: 9.38e-04, decoder learning rate: 9.38e-04\n",
      "[91/300] At -4.0 dB, Train Loss: 0.8612479567527771 Train BER 0.04651891812682152,                  \n",
      " [91/300] At -2.0 dB, Train Loss: 0.10343418270349503 Train BER 0.0041189189068973064\n",
      "Time for one full iteration is 7.2187 minutes\n",
      "encoder learning rate: 9.35e-04, decoder learning rate: 9.35e-04\n",
      "[92/300] At -4.0 dB, Train Loss: 0.7417556047439575 Train BER 0.03787567466497421,                  \n",
      " [92/300] At -2.0 dB, Train Loss: 0.056590281426906586 Train BER 0.0020432432647794485\n",
      "Time for one full iteration is 7.2579 minutes\n",
      "encoder learning rate: 9.32e-04, decoder learning rate: 9.32e-04\n",
      "[93/300] At -4.0 dB, Train Loss: 0.8085989952087402 Train BER 0.042091891169548035,                  \n",
      " [93/300] At -2.0 dB, Train Loss: 0.14154958724975586 Train BER 0.005756756756454706\n",
      "Time for one full iteration is 7.0983 minutes\n",
      "encoder learning rate: 9.29e-04, decoder learning rate: 9.29e-04\n",
      "[94/300] At -4.0 dB, Train Loss: 0.6947336196899414 Train BER 0.03570810705423355,                  \n",
      " [94/300] At -2.0 dB, Train Loss: 0.08413438498973846 Train BER 0.003383783856406808\n",
      "Time for one full iteration is 7.3928 minutes\n",
      "encoder learning rate: 9.26e-04, decoder learning rate: 9.26e-04\n",
      "[95/300] At -4.0 dB, Train Loss: 0.8542457222938538 Train BER 0.04513513669371605,                  \n",
      " [95/300] At -2.0 dB, Train Loss: 0.16337575018405914 Train BER 0.0063405404798686504\n",
      "Time for one full iteration is 7.2569 minutes\n",
      "encoder learning rate: 9.22e-04, decoder learning rate: 9.22e-04\n",
      "[96/300] At -4.0 dB, Train Loss: 0.7410658597946167 Train BER 0.03828107938170433,                  \n",
      " [96/300] At -2.0 dB, Train Loss: 0.11157844960689545 Train BER 0.004291892051696777\n",
      "Time for one full iteration is 7.1191 minutes\n",
      "encoder learning rate: 9.19e-04, decoder learning rate: 9.19e-04\n",
      "[97/300] At -4.0 dB, Train Loss: 0.9142802953720093 Train BER 0.0471351332962513,                  \n",
      " [97/300] At -2.0 dB, Train Loss: 0.15824003517627716 Train BER 0.006016216240823269\n",
      "Time for one full iteration is 7.2331 minutes\n",
      "encoder learning rate: 9.15e-04, decoder learning rate: 9.15e-04\n",
      "[98/300] At -4.0 dB, Train Loss: 0.7650887966156006 Train BER 0.04039999842643738,                  \n",
      " [98/300] At -2.0 dB, Train Loss: 0.09316593408584595 Train BER 0.00324324332177639\n",
      "Time for one full iteration is 7.1532 minutes\n",
      "encoder learning rate: 9.12e-04, decoder learning rate: 9.12e-04\n",
      "[99/300] At -4.0 dB, Train Loss: 0.824895977973938 Train BER 0.04198378324508667,                  \n",
      " [99/300] At -2.0 dB, Train Loss: 0.07569389045238495 Train BER 0.00290270266123116\n",
      "Time for one full iteration is 7.2343 minutes\n",
      "encoder learning rate: 9.08e-04, decoder learning rate: 9.08e-04\n",
      "[100/300] At -4.0 dB, Train Loss: 0.7635911107063293 Train BER 0.039400000125169754,                  \n",
      " [100/300] At -2.0 dB, Train Loss: 0.06397465616464615 Train BER 0.002632432384416461\n",
      "Time for one full iteration is 7.1545 minutes\n",
      "encoder learning rate: 9.05e-04, decoder learning rate: 9.05e-04\n",
      "[101/300] At -4.0 dB, Train Loss: 0.794326901435852 Train BER 0.041627027094364166,                  \n",
      " [101/300] At -2.0 dB, Train Loss: 0.129595547914505 Train BER 0.005054053850471973\n",
      "Time for one full iteration is 7.1983 minutes\n",
      "encoder learning rate: 9.01e-04, decoder learning rate: 9.01e-04\n",
      "[102/300] At -4.0 dB, Train Loss: 0.6999887824058533 Train BER 0.03607567399740219,                  \n",
      " [102/300] At -2.0 dB, Train Loss: 0.0689878910779953 Train BER 0.0025027026422321796\n",
      "Time for one full iteration is 7.2132 minutes\n",
      "encoder learning rate: 8.97e-04, decoder learning rate: 8.97e-04\n",
      "[103/300] At -4.0 dB, Train Loss: 0.8046753406524658 Train BER 0.04088108241558075,                  \n",
      " [103/300] At -2.0 dB, Train Loss: 0.11657317727804184 Train BER 0.00488108117133379\n",
      "Time for one full iteration is 7.2534 minutes\n",
      "encoder learning rate: 8.93e-04, decoder learning rate: 8.93e-04\n",
      "[104/300] At -4.0 dB, Train Loss: 0.7231956124305725 Train BER 0.03688108175992966,                  \n",
      " [104/300] At -2.0 dB, Train Loss: 0.06365127116441727 Train BER 0.0021675676107406616\n",
      "Time for one full iteration is 7.4146 minutes\n",
      "encoder learning rate: 8.89e-04, decoder learning rate: 8.89e-04\n",
      "[105/300] At -4.0 dB, Train Loss: 0.8446022272109985 Train BER 0.04356756806373596,                  \n",
      " [105/300] At -2.0 dB, Train Loss: 0.13027161359786987 Train BER 0.005578378215432167\n",
      "Time for one full iteration is 8.4837 minutes\n",
      "encoder learning rate: 8.85e-04, decoder learning rate: 8.85e-04\n",
      "[106/300] At -4.0 dB, Train Loss: 0.6975268125534058 Train BER 0.0357675664126873,                  \n",
      " [106/300] At -2.0 dB, Train Loss: 0.07601610571146011 Train BER 0.0026000000070780516\n",
      "Time for one full iteration is 8.4274 minutes\n",
      "encoder learning rate: 8.81e-04, decoder learning rate: 8.81e-04\n",
      "[107/300] At -4.0 dB, Train Loss: 0.7893628478050232 Train BER 0.04112432524561882,                  \n",
      " [107/300] At -2.0 dB, Train Loss: 0.08597815781831741 Train BER 0.003351351246237755\n",
      "Time for one full iteration is 8.4732 minutes\n",
      "encoder learning rate: 8.77e-04, decoder learning rate: 8.77e-04\n",
      "[108/300] At -4.0 dB, Train Loss: 0.7299452424049377 Train BER 0.038443244993686676,                  \n",
      " [108/300] At -2.0 dB, Train Loss: 0.05537666007876396 Train BER 0.0019729728810489178\n",
      "Time for one full iteration is 8.4698 minutes\n",
      "encoder learning rate: 8.73e-04, decoder learning rate: 8.73e-04\n",
      "[109/300] At -4.0 dB, Train Loss: 0.8213745951652527 Train BER 0.042583782225847244,                  \n",
      " [109/300] At -2.0 dB, Train Loss: 0.09383048862218857 Train BER 0.003940540365874767\n",
      "Time for one full iteration is 8.5001 minutes\n",
      "encoder learning rate: 8.69e-04, decoder learning rate: 8.69e-04\n",
      "[110/300] At -4.0 dB, Train Loss: 0.7058150768280029 Train BER 0.0363459475338459,                  \n",
      " [110/300] At -2.0 dB, Train Loss: 0.058643490076065063 Train BER 0.0020594594534486532\n",
      "Time for one full iteration is 8.4912 minutes\n",
      "encoder learning rate: 8.65e-04, decoder learning rate: 8.65e-04\n",
      "[111/300] At -4.0 dB, Train Loss: 0.8532353043556213 Train BER 0.04515675827860832,                  \n",
      " [111/300] At -2.0 dB, Train Loss: 0.10822058469057083 Train BER 0.004259459674358368\n",
      "Time for one full iteration is 8.5232 minutes\n",
      "encoder learning rate: 8.60e-04, decoder learning rate: 8.60e-04\n",
      "[112/300] At -4.0 dB, Train Loss: 0.7253188490867615 Train BER 0.03662702813744545,                  \n",
      " [112/300] At -2.0 dB, Train Loss: 0.07203083485364914 Train BER 0.0026162161957472563\n",
      "Time for one full iteration is 8.5546 minutes\n",
      "encoder learning rate: 8.56e-04, decoder learning rate: 8.56e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/300] At -4.0 dB, Train Loss: 0.8155164122581482 Train BER 0.042399998754262924,                  \n",
      " [113/300] At -2.0 dB, Train Loss: 0.1000690907239914 Train BER 0.004102702718228102\n",
      "Time for one full iteration is 8.5201 minutes\n",
      "encoder learning rate: 8.51e-04, decoder learning rate: 8.51e-04\n",
      "[114/300] At -4.0 dB, Train Loss: 0.6815280318260193 Train BER 0.03501081094145775,                  \n",
      " [114/300] At -2.0 dB, Train Loss: 0.05912125110626221 Train BER 0.002010810887441039\n",
      "Time for one full iteration is 8.4296 minutes\n",
      "encoder learning rate: 8.47e-04, decoder learning rate: 8.47e-04\n",
      "[115/300] At -4.0 dB, Train Loss: 0.7937519550323486 Train BER 0.04100540652871132,                  \n",
      " [115/300] At -2.0 dB, Train Loss: 0.08859280496835709 Train BER 0.0036648649256676435\n",
      "Time for one full iteration is 8.5284 minutes\n",
      "encoder learning rate: 8.42e-04, decoder learning rate: 8.42e-04\n",
      "[116/300] At -4.0 dB, Train Loss: 0.7225616574287415 Train BER 0.03757297247648239,                  \n",
      " [116/300] At -2.0 dB, Train Loss: 0.0648837462067604 Train BER 0.00227567576803267\n",
      "Time for one full iteration is 8.4200 minutes\n",
      "encoder learning rate: 8.38e-04, decoder learning rate: 8.38e-04\n",
      "[117/300] At -4.0 dB, Train Loss: 0.8229015469551086 Train BER 0.04263783618807793,                  \n",
      " [117/300] At -2.0 dB, Train Loss: 0.08327975124120712 Train BER 0.0034216216299682856\n",
      "Time for one full iteration is 8.4418 minutes\n",
      "encoder learning rate: 8.33e-04, decoder learning rate: 8.33e-04\n",
      "[118/300] At -4.0 dB, Train Loss: 0.7116217613220215 Train BER 0.03689729794859886,                  \n",
      " [118/300] At -2.0 dB, Train Loss: 0.055657513439655304 Train BER 0.0018432432552799582\n",
      "Time for one full iteration is 8.4231 minutes\n",
      "encoder learning rate: 8.28e-04, decoder learning rate: 8.28e-04\n",
      "[119/300] At -4.0 dB, Train Loss: 0.8364565968513489 Train BER 0.044075675308704376,                  \n",
      " [119/300] At -2.0 dB, Train Loss: 0.08401287347078323 Train BER 0.0034432432148605585\n",
      "Time for one full iteration is 8.3972 minutes\n",
      "encoder learning rate: 8.24e-04, decoder learning rate: 8.24e-04\n",
      "[120/300] At -4.0 dB, Train Loss: 0.682193398475647 Train BER 0.03457297384738922,                  \n",
      " [120/300] At -2.0 dB, Train Loss: 0.05690693110227585 Train BER 0.0020432432647794485\n",
      "Time for one full iteration is 7.4175 minutes\n",
      "encoder learning rate: 8.19e-04, decoder learning rate: 8.19e-04\n",
      "[121/300] At -4.0 dB, Train Loss: 0.7618698477745056 Train BER 0.038756757974624634,                  \n",
      " [121/300] At -2.0 dB, Train Loss: 0.08529113978147507 Train BER 0.0035189189948141575\n",
      "Time for one full iteration is 7.0887 minutes\n",
      "encoder learning rate: 8.14e-04, decoder learning rate: 8.14e-04\n",
      "[122/300] At -4.0 dB, Train Loss: 0.6977742910385132 Train BER 0.035481080412864685,                  \n",
      " [122/300] At -2.0 dB, Train Loss: 0.057679418474435806 Train BER 0.0020864864345639944\n",
      "Time for one full iteration is 7.1636 minutes\n",
      "encoder learning rate: 8.09e-04, decoder learning rate: 8.09e-04\n",
      "[123/300] At -4.0 dB, Train Loss: 0.763484537601471 Train BER 0.039394594728946686,                  \n",
      " [123/300] At -2.0 dB, Train Loss: 0.07745130360126495 Train BER 0.002978378441184759\n",
      "Time for one full iteration is 7.1512 minutes\n",
      "encoder learning rate: 8.04e-04, decoder learning rate: 8.04e-04\n",
      "[124/300] At -4.0 dB, Train Loss: 0.6807312369346619 Train BER 0.03383783623576164,                  \n",
      " [124/300] At -2.0 dB, Train Loss: 0.059992462396621704 Train BER 0.002118918811902404\n",
      "Time for one full iteration is 7.0928 minutes\n",
      "encoder learning rate: 7.99e-04, decoder learning rate: 7.99e-04\n",
      "[125/300] At -4.0 dB, Train Loss: 0.77666175365448 Train BER 0.04100000113248825,                  \n",
      " [125/300] At -2.0 dB, Train Loss: 0.07639824599027634 Train BER 0.003167567541822791\n",
      "Time for one full iteration is 7.1555 minutes\n",
      "encoder learning rate: 7.94e-04, decoder learning rate: 7.94e-04\n",
      "[126/300] At -4.0 dB, Train Loss: 0.6945050358772278 Train BER 0.03463243320584297,                  \n",
      " [126/300] At -2.0 dB, Train Loss: 0.06814169883728027 Train BER 0.0027675675228238106\n",
      "Time for one full iteration is 7.0804 minutes\n",
      "encoder learning rate: 7.89e-04, decoder learning rate: 7.89e-04\n",
      "[127/300] At -4.0 dB, Train Loss: 0.7990574836730957 Train BER 0.04130810871720314,                  \n",
      " [127/300] At -2.0 dB, Train Loss: 0.07924555987119675 Train BER 0.003124324372038245\n",
      "Time for one full iteration is 7.1622 minutes\n",
      "encoder learning rate: 7.84e-04, decoder learning rate: 7.84e-04\n",
      "[128/300] At -4.0 dB, Train Loss: 0.7020865678787231 Train BER 0.03504324331879616,                  \n",
      " [128/300] At -2.0 dB, Train Loss: 0.047969888895750046 Train BER 0.001713513513095677\n",
      "Time for one full iteration is 7.3066 minutes\n",
      "encoder learning rate: 7.79e-04, decoder learning rate: 7.79e-04\n",
      "[129/300] At -4.0 dB, Train Loss: 0.8003763556480408 Train BER 0.04159459471702576,                  \n",
      " [129/300] At -2.0 dB, Train Loss: 0.06714988499879837 Train BER 0.002486486453562975\n",
      "Time for one full iteration is 7.2006 minutes\n",
      "encoder learning rate: 7.73e-04, decoder learning rate: 7.73e-04\n",
      "[130/300] At -4.0 dB, Train Loss: 0.7055572271347046 Train BER 0.03619459643959999,                  \n",
      " [130/300] At -2.0 dB, Train Loss: 0.0586257204413414 Train BER 0.002075675642117858\n",
      "Time for one full iteration is 7.1071 minutes\n",
      "encoder learning rate: 7.68e-04, decoder learning rate: 7.68e-04\n",
      "[131/300] At -4.0 dB, Train Loss: 0.8059414625167847 Train BER 0.04121621698141098,                  \n",
      " [131/300] At -2.0 dB, Train Loss: 0.08330613374710083 Train BER 0.003362162271514535\n",
      "Time for one full iteration is 7.0931 minutes\n",
      "encoder learning rate: 7.63e-04, decoder learning rate: 7.63e-04\n",
      "[132/300] At -4.0 dB, Train Loss: 0.6948210597038269 Train BER 0.03589729592204094,                  \n",
      " [132/300] At -2.0 dB, Train Loss: 0.08032984286546707 Train BER 0.0031297297682613134\n",
      "Time for one full iteration is 7.0988 minutes\n",
      "encoder learning rate: 7.57e-04, decoder learning rate: 7.57e-04\n",
      "[133/300] At -4.0 dB, Train Loss: 0.8063762784004211 Train BER 0.04060540720820427,                  \n",
      " [133/300] At -2.0 dB, Train Loss: 0.12138695269823074 Train BER 0.005043243058025837\n",
      "Time for one full iteration is 7.2613 minutes\n",
      "encoder learning rate: 7.52e-04, decoder learning rate: 7.52e-04\n",
      "[134/300] At -4.0 dB, Train Loss: 0.7688487768173218 Train BER 0.03949729725718498,                  \n",
      " [134/300] At -2.0 dB, Train Loss: 0.12010147422552109 Train BER 0.004794594366103411\n",
      "Time for one full iteration is 7.2436 minutes\n",
      "encoder learning rate: 7.47e-04, decoder learning rate: 7.47e-04\n",
      "[135/300] At -4.0 dB, Train Loss: 0.8275790214538574 Train BER 0.042913515120744705,                  \n",
      " [135/300] At -2.0 dB, Train Loss: 0.1394401639699936 Train BER 0.0056486488319933414\n",
      "Time for one full iteration is 7.3306 minutes\n",
      "encoder learning rate: 7.41e-04, decoder learning rate: 7.41e-04\n",
      "[136/300] At -4.0 dB, Train Loss: 0.8379244208335876 Train BER 0.04295135289430618,                  \n",
      " [136/300] At -2.0 dB, Train Loss: 0.10031408071517944 Train BER 0.003891891799867153\n",
      "Time for one full iteration is 7.1730 minutes\n",
      "encoder learning rate: 7.36e-04, decoder learning rate: 7.36e-04\n",
      "[137/300] At -4.0 dB, Train Loss: 0.8351917266845703 Train BER 0.042702700942754745,                  \n",
      " [137/300] At -2.0 dB, Train Loss: 0.08756449073553085 Train BER 0.003427027026191354\n",
      "Time for one full iteration is 7.2274 minutes\n",
      "encoder learning rate: 7.30e-04, decoder learning rate: 7.30e-04\n",
      "[138/300] At -4.0 dB, Train Loss: 0.7241451740264893 Train BER 0.0362270288169384,                  \n",
      " [138/300] At -2.0 dB, Train Loss: 0.06011331453919411 Train BER 0.002178378403186798\n",
      "Time for one full iteration is 7.0993 minutes\n",
      "encoder learning rate: 7.24e-04, decoder learning rate: 7.24e-04\n",
      "[139/300] At -4.0 dB, Train Loss: 0.7727835774421692 Train BER 0.040075674653053284,                  \n",
      " [139/300] At -2.0 dB, Train Loss: 0.07384833693504333 Train BER 0.002978378441184759\n",
      "Time for one full iteration is 7.1256 minutes\n",
      "encoder learning rate: 7.19e-04, decoder learning rate: 7.19e-04\n",
      "[140/300] At -4.0 dB, Train Loss: 0.683757483959198 Train BER 0.03478918969631195,                  \n",
      " [140/300] At -2.0 dB, Train Loss: 0.04096149280667305 Train BER 0.0012270270381122828\n",
      "Time for one full iteration is 7.1328 minutes\n",
      "encoder learning rate: 7.13e-04, decoder learning rate: 7.13e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/300] At -4.0 dB, Train Loss: 0.7813335061073303 Train BER 0.040745947510004044,                  \n",
      " [141/300] At -2.0 dB, Train Loss: 0.07474099099636078 Train BER 0.002978378441184759\n",
      "Time for one full iteration is 7.3200 minutes\n",
      "encoder learning rate: 7.07e-04, decoder learning rate: 7.07e-04\n",
      "[142/300] At -4.0 dB, Train Loss: 0.6646397709846497 Train BER 0.03356216102838516,                  \n",
      " [142/300] At -2.0 dB, Train Loss: 0.05357290059328079 Train BER 0.0020648648496717215\n",
      "Time for one full iteration is 7.1184 minutes\n",
      "encoder learning rate: 7.02e-04, decoder learning rate: 7.02e-04\n",
      "[143/300] At -4.0 dB, Train Loss: 0.7743822336196899 Train BER 0.03984324261546135,                  \n",
      " [143/300] At -2.0 dB, Train Loss: 0.07016383856534958 Train BER 0.002751351334154606\n",
      "Time for one full iteration is 7.1456 minutes\n",
      "encoder learning rate: 6.96e-04, decoder learning rate: 6.96e-04\n",
      "[144/300] At -4.0 dB, Train Loss: 0.6522841453552246 Train BER 0.033470269292593,                  \n",
      " [144/300] At -2.0 dB, Train Loss: 0.05200711637735367 Train BER 0.0016594594344496727\n",
      "Time for one full iteration is 7.1011 minutes\n",
      "encoder learning rate: 6.90e-04, decoder learning rate: 6.90e-04\n",
      "[145/300] At -4.0 dB, Train Loss: 0.7395610809326172 Train BER 0.037286486476659775,                  \n",
      " [145/300] At -2.0 dB, Train Loss: 0.0714641660451889 Train BER 0.0029135134536772966\n",
      "Time for one full iteration is 7.1115 minutes\n",
      "encoder learning rate: 6.84e-04, decoder learning rate: 6.84e-04\n",
      "[146/300] At -4.0 dB, Train Loss: 0.6955872774124146 Train BER 0.03435135260224342,                  \n",
      " [146/300] At -2.0 dB, Train Loss: 0.041211578994989395 Train BER 0.0013135134940966964\n",
      "Time for one full iteration is 7.2546 minutes\n",
      "encoder learning rate: 6.79e-04, decoder learning rate: 6.79e-04\n",
      "[147/300] At -4.0 dB, Train Loss: 0.7663540244102478 Train BER 0.03819999843835831,                  \n",
      " [147/300] At -2.0 dB, Train Loss: 0.05821186676621437 Train BER 0.0022378377616405487\n",
      "Time for one full iteration is 7.2773 minutes\n",
      "encoder learning rate: 6.73e-04, decoder learning rate: 6.73e-04\n",
      "[148/300] At -4.0 dB, Train Loss: 0.6654641628265381 Train BER 0.033556755632162094,                  \n",
      " [148/300] At -2.0 dB, Train Loss: 0.03840399160981178 Train BER 0.0011027026921510696\n",
      "Time for one full iteration is 7.2700 minutes\n",
      "encoder learning rate: 6.67e-04, decoder learning rate: 6.67e-04\n",
      "[149/300] At -4.0 dB, Train Loss: 0.7690299153327942 Train BER 0.04088648781180382,                  \n",
      " [149/300] At -2.0 dB, Train Loss: 0.06441342085599899 Train BER 0.0025783784221857786\n",
      "Time for one full iteration is 7.0921 minutes\n",
      "encoder learning rate: 6.61e-04, decoder learning rate: 6.61e-04\n",
      "[150/300] At -4.0 dB, Train Loss: 0.6283351182937622 Train BER 0.03152432292699814,                  \n",
      " [150/300] At -2.0 dB, Train Loss: 0.04588275030255318 Train BER 0.0014864865224808455\n",
      "Time for one full iteration is 7.1609 minutes\n",
      "encoder learning rate: 6.55e-04, decoder learning rate: 6.55e-04\n",
      "[151/300] At -4.0 dB, Train Loss: 0.7777059674263 Train BER 0.041048649698495865,                  \n",
      " [151/300] At -2.0 dB, Train Loss: 0.054066114127635956 Train BER 0.0021081080194562674\n",
      "Time for one full iteration is 7.2321 minutes\n",
      "encoder learning rate: 6.49e-04, decoder learning rate: 6.49e-04\n",
      "[152/300] At -4.0 dB, Train Loss: 0.6811533570289612 Train BER 0.03455135226249695,                  \n",
      " [152/300] At -2.0 dB, Train Loss: 0.03837672993540764 Train BER 0.0012594594154506922\n",
      "Time for one full iteration is 7.3320 minutes\n",
      "encoder learning rate: 6.43e-04, decoder learning rate: 6.43e-04\n",
      "[153/300] At -4.0 dB, Train Loss: 0.7041633129119873 Train BER 0.03604324162006378,                  \n",
      " [153/300] At -2.0 dB, Train Loss: 0.06326114386320114 Train BER 0.002318918937817216\n",
      "Time for one full iteration is 7.3718 minutes\n",
      "encoder learning rate: 6.37e-04, decoder learning rate: 6.37e-04\n",
      "[154/300] At -4.0 dB, Train Loss: 0.6493675112724304 Train BER 0.03347567468881607,                  \n",
      " [154/300] At -2.0 dB, Train Loss: 0.04032973200082779 Train BER 0.0014918919187039137\n",
      "Time for one full iteration is 7.3199 minutes\n",
      "encoder learning rate: 6.31e-04, decoder learning rate: 6.31e-04\n",
      "[155/300] At -4.0 dB, Train Loss: 0.73728346824646 Train BER 0.0378270260989666,                  \n",
      " [155/300] At -2.0 dB, Train Loss: 0.05694214254617691 Train BER 0.0023567567113786936\n",
      "Time for one full iteration is 7.3686 minutes\n",
      "encoder learning rate: 6.25e-04, decoder learning rate: 6.25e-04\n",
      "[156/300] At -4.0 dB, Train Loss: 0.6406425833702087 Train BER 0.0327729731798172,                  \n",
      " [156/300] At -2.0 dB, Train Loss: 0.03868980333209038 Train BER 0.00130270270165056\n",
      "Time for one full iteration is 7.3181 minutes\n",
      "encoder learning rate: 6.19e-04, decoder learning rate: 6.19e-04\n",
      "[157/300] At -4.0 dB, Train Loss: 0.6872626543045044 Train BER 0.03460000082850456,                  \n",
      " [157/300] At -2.0 dB, Train Loss: 0.049978114664554596 Train BER 0.0019945946987718344\n",
      "Time for one full iteration is 7.3932 minutes\n",
      "encoder learning rate: 6.13e-04, decoder learning rate: 6.13e-04\n",
      "[158/300] At -4.0 dB, Train Loss: 0.6815645694732666 Train BER 0.03458378463983536,                  \n",
      " [158/300] At -2.0 dB, Train Loss: 0.042972128838300705 Train BER 0.0015243242960423231\n",
      "Time for one full iteration is 7.3724 minutes\n",
      "encoder learning rate: 6.06e-04, decoder learning rate: 6.06e-04\n",
      "[159/300] At -4.0 dB, Train Loss: 0.723169207572937 Train BER 0.03775675594806671,                  \n",
      " [159/300] At -2.0 dB, Train Loss: 0.05585136264562607 Train BER 0.0022162161767482758\n",
      "Time for one full iteration is 7.4014 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[160/300] At -4.0 dB, Train Loss: 0.6564183235168457 Train BER 0.03305405378341675,                  \n",
      " [160/300] At -2.0 dB, Train Loss: 0.04013724625110626 Train BER 0.0013729729689657688\n",
      "Time for one full iteration is 7.4875 minutes\n",
      "encoder learning rate: 5.94e-04, decoder learning rate: 5.94e-04\n",
      "[161/300] At -4.0 dB, Train Loss: 0.6747265458106995 Train BER 0.034810811281204224,                  \n",
      " [161/300] At -2.0 dB, Train Loss: 0.044328827410936356 Train BER 0.0015513513935729861\n",
      "Time for one full iteration is 7.5873 minutes\n",
      "encoder learning rate: 5.88e-04, decoder learning rate: 5.88e-04\n",
      "[162/300] At -4.0 dB, Train Loss: 0.6579911112785339 Train BER 0.033643241971731186,                  \n",
      " [162/300] At -2.0 dB, Train Loss: 0.04315069317817688 Train BER 0.0014054054627195\n",
      "Time for one full iteration is 7.3848 minutes\n",
      "encoder learning rate: 5.82e-04, decoder learning rate: 5.82e-04\n",
      "[163/300] At -4.0 dB, Train Loss: 0.6717727780342102 Train BER 0.03384324163198471,                  \n",
      " [163/300] At -2.0 dB, Train Loss: 0.05758053809404373 Train BER 0.002421621698886156\n",
      "Time for one full iteration is 7.2582 minutes\n",
      "encoder learning rate: 5.76e-04, decoder learning rate: 5.76e-04\n",
      "[164/300] At -4.0 dB, Train Loss: 0.6780456900596619 Train BER 0.034594595432281494,                  \n",
      " [164/300] At -2.0 dB, Train Loss: 0.03991370648145676 Train BER 0.0014432432362809777\n",
      "Time for one full iteration is 7.0984 minutes\n",
      "encoder learning rate: 5.69e-04, decoder learning rate: 5.69e-04\n",
      "[165/300] At -4.0 dB, Train Loss: 0.7221063375473022 Train BER 0.037043243646621704,                  \n",
      " [165/300] At -2.0 dB, Train Loss: 0.047639355063438416 Train BER 0.0017783783841878176\n",
      "Time for one full iteration is 7.2466 minutes\n",
      "encoder learning rate: 5.63e-04, decoder learning rate: 5.63e-04\n",
      "[166/300] At -4.0 dB, Train Loss: 0.652222216129303 Train BER 0.033427026122808456,                  \n",
      " [166/300] At -2.0 dB, Train Loss: 0.040282879024744034 Train BER 0.0014054054627195\n",
      "Time for one full iteration is 7.0878 minutes\n",
      "encoder learning rate: 5.57e-04, decoder learning rate: 5.57e-04\n",
      "[167/300] At -4.0 dB, Train Loss: 0.6993738412857056 Train BER 0.035448648035526276,                  \n",
      " [167/300] At -2.0 dB, Train Loss: 0.05665966123342514 Train BER 0.002221621572971344\n",
      "Time for one full iteration is 7.0588 minutes\n",
      "encoder learning rate: 5.51e-04, decoder learning rate: 5.51e-04\n",
      "[168/300] At -4.0 dB, Train Loss: 0.6533211469650269 Train BER 0.033399999141693115,                  \n",
      " [168/300] At -2.0 dB, Train Loss: 0.03923193737864494 Train BER 0.0013189188903197646\n",
      "Time for one full iteration is 7.1661 minutes\n",
      "encoder learning rate: 5.44e-04, decoder learning rate: 5.44e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169/300] At -4.0 dB, Train Loss: 0.7091526389122009 Train BER 0.036335136741399765,                  \n",
      " [169/300] At -2.0 dB, Train Loss: 0.04453843832015991 Train BER 0.0015459459973499179\n",
      "Time for one full iteration is 7.1624 minutes\n",
      "encoder learning rate: 5.38e-04, decoder learning rate: 5.38e-04\n",
      "[170/300] At -4.0 dB, Train Loss: 0.6322997212409973 Train BER 0.03182162344455719,                  \n",
      " [170/300] At -2.0 dB, Train Loss: 0.03896479681134224 Train BER 0.0012432432267814875\n",
      "Time for one full iteration is 7.4800 minutes\n",
      "encoder learning rate: 5.32e-04, decoder learning rate: 5.32e-04\n",
      "[171/300] At -4.0 dB, Train Loss: 0.7000524401664734 Train BER 0.0357675664126873,                  \n",
      " [171/300] At -2.0 dB, Train Loss: 0.03993763029575348 Train BER 0.0014324324438348413\n",
      "Time for one full iteration is 7.2058 minutes\n",
      "encoder learning rate: 5.26e-04, decoder learning rate: 5.26e-04\n",
      "[172/300] At -4.0 dB, Train Loss: 0.6202086210250854 Train BER 0.031275674700737,                  \n",
      " [172/300] At -2.0 dB, Train Loss: 0.033272117376327515 Train BER 0.0010108108399435878\n",
      "Time for one full iteration is 7.2800 minutes\n",
      "encoder learning rate: 5.19e-04, decoder learning rate: 5.19e-04\n",
      "[173/300] At -4.0 dB, Train Loss: 0.6398543119430542 Train BER 0.03189189359545708,                  \n",
      " [173/300] At -2.0 dB, Train Loss: 0.04751112684607506 Train BER 0.001810810761526227\n",
      "Time for one full iteration is 7.2415 minutes\n",
      "encoder learning rate: 5.13e-04, decoder learning rate: 5.13e-04\n",
      "[174/300] At -4.0 dB, Train Loss: 0.646050214767456 Train BER 0.03262702748179436,                  \n",
      " [174/300] At -2.0 dB, Train Loss: 0.04119477793574333 Train BER 0.0015081081073731184\n",
      "Time for one full iteration is 7.1738 minutes\n",
      "encoder learning rate: 5.07e-04, decoder learning rate: 5.07e-04\n",
      "[175/300] At -4.0 dB, Train Loss: 0.6853667497634888 Train BER 0.033956754952669144,                  \n",
      " [175/300] At -2.0 dB, Train Loss: 0.041422609239816666 Train BER 0.0014432432362809777\n",
      "Time for one full iteration is 7.2067 minutes\n",
      "encoder learning rate: 5.01e-04, decoder learning rate: 5.01e-04\n",
      "[176/300] At -4.0 dB, Train Loss: 0.6289324164390564 Train BER 0.03127026930451393,                  \n",
      " [176/300] At -2.0 dB, Train Loss: 0.03901413828134537 Train BER 0.0013675675727427006\n",
      "Time for one full iteration is 7.0513 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[177/300] At -4.0 dB, Train Loss: 0.6596074104309082 Train BER 0.032789189368486404,                  \n",
      " [177/300] At -2.0 dB, Train Loss: 0.055193278938531876 Train BER 0.0022648649755865335\n",
      "Time for one full iteration is 7.3134 minutes\n",
      "encoder learning rate: 4.88e-04, decoder learning rate: 4.88e-04\n",
      "[178/300] At -4.0 dB, Train Loss: 0.6506139636039734 Train BER 0.033497296273708344,                  \n",
      " [178/300] At -2.0 dB, Train Loss: 0.045777879655361176 Train BER 0.001713513513095677\n",
      "Time for one full iteration is 7.3300 minutes\n",
      "encoder learning rate: 4.82e-04, decoder learning rate: 4.82e-04\n",
      "[179/300] At -4.0 dB, Train Loss: 0.7017270922660828 Train BER 0.03568108007311821,                  \n",
      " [179/300] At -2.0 dB, Train Loss: 0.04996388778090477 Train BER 0.0019351351074874401\n",
      "Time for one full iteration is 7.1931 minutes\n",
      "encoder learning rate: 4.75e-04, decoder learning rate: 4.75e-04\n",
      "[180/300] At -4.0 dB, Train Loss: 0.646577775478363 Train BER 0.031902704387903214,                  \n",
      " [180/300] At -2.0 dB, Train Loss: 0.04508315771818161 Train BER 0.0017243243055418134\n",
      "Time for one full iteration is 7.3137 minutes\n",
      "encoder learning rate: 4.69e-04, decoder learning rate: 4.69e-04\n",
      "[181/300] At -4.0 dB, Train Loss: 0.6847158074378967 Train BER 0.03507567569613457,                  \n",
      " [181/300] At -2.0 dB, Train Loss: 0.04200822860002518 Train BER 0.0014810811262577772\n",
      "Time for one full iteration is 7.1682 minutes\n",
      "encoder learning rate: 4.63e-04, decoder learning rate: 4.63e-04\n",
      "[182/300] At -4.0 dB, Train Loss: 0.6001719236373901 Train BER 0.029297297820448875,                  \n",
      " [182/300] At -2.0 dB, Train Loss: 0.03780974820256233 Train BER 0.001329729682765901\n",
      "Time for one full iteration is 7.1660 minutes\n",
      "encoder learning rate: 4.57e-04, decoder learning rate: 4.57e-04\n",
      "[183/300] At -4.0 dB, Train Loss: 0.7014057636260986 Train BER 0.0354108102619648,                  \n",
      " [183/300] At -2.0 dB, Train Loss: 0.045062899589538574 Train BER 0.001664864830672741\n",
      "Time for one full iteration is 7.2099 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[184/300] At -4.0 dB, Train Loss: 0.6437881588935852 Train BER 0.03308108076453209,                  \n",
      " [184/300] At -2.0 dB, Train Loss: 0.040218304842710495 Train BER 0.0013729729689657688\n",
      "Time for one full iteration is 7.0982 minutes\n",
      "encoder learning rate: 4.44e-04, decoder learning rate: 4.44e-04\n",
      "[185/300] At -4.0 dB, Train Loss: 0.6757955551147461 Train BER 0.03494054079055786,                  \n",
      " [185/300] At -2.0 dB, Train Loss: 0.05341395363211632 Train BER 0.002156756818294525\n",
      "Time for one full iteration is 7.1433 minutes\n",
      "encoder learning rate: 4.38e-04, decoder learning rate: 4.38e-04\n",
      "[186/300] At -4.0 dB, Train Loss: 0.6317961812019348 Train BER 0.03188108280301094,                  \n",
      " [186/300] At -2.0 dB, Train Loss: 0.037013549357652664 Train BER 0.0012864865129813552\n",
      "Time for one full iteration is 7.1970 minutes\n",
      "encoder learning rate: 4.32e-04, decoder learning rate: 4.32e-04\n",
      "[187/300] At -4.0 dB, Train Loss: 0.6476356983184814 Train BER 0.03288648650050163,                  \n",
      " [187/300] At -2.0 dB, Train Loss: 0.04401671513915062 Train BER 0.0016054053558036685\n",
      "Time for one full iteration is 7.2222 minutes\n",
      "encoder learning rate: 4.25e-04, decoder learning rate: 4.25e-04\n",
      "[188/300] At -4.0 dB, Train Loss: 0.6363996267318726 Train BER 0.0319729745388031,                  \n",
      " [188/300] At -2.0 dB, Train Loss: 0.035029832273721695 Train BER 0.0012378378305584192\n",
      "Time for one full iteration is 7.1917 minutes\n",
      "encoder learning rate: 4.19e-04, decoder learning rate: 4.19e-04\n",
      "[189/300] At -4.0 dB, Train Loss: 0.6663544774055481 Train BER 0.03420000150799751,                  \n",
      " [189/300] At -2.0 dB, Train Loss: 0.0453898049890995 Train BER 0.0017243243055418134\n",
      "Time for one full iteration is 7.1185 minutes\n",
      "encoder learning rate: 4.13e-04, decoder learning rate: 4.13e-04\n",
      "[190/300] At -4.0 dB, Train Loss: 0.641832172870636 Train BER 0.03308648616075516,                  \n",
      " [190/300] At -2.0 dB, Train Loss: 0.03612757474184036 Train BER 0.0012000000569969416\n",
      "Time for one full iteration is 7.3039 minutes\n",
      "encoder learning rate: 4.07e-04, decoder learning rate: 4.07e-04\n",
      "[191/300] At -4.0 dB, Train Loss: 0.648247480392456 Train BER 0.03255675733089447,                  \n",
      " [191/300] At -2.0 dB, Train Loss: 0.04106350988149643 Train BER 0.0013459459878504276\n",
      "Time for one full iteration is 7.2036 minutes\n",
      "encoder learning rate: 4.01e-04, decoder learning rate: 4.01e-04\n",
      "[192/300] At -4.0 dB, Train Loss: 0.6134666204452515 Train BER 0.030805405229330063,                  \n",
      " [192/300] At -2.0 dB, Train Loss: 0.03880932554602623 Train BER 0.0013837837614119053\n",
      "Time for one full iteration is 7.0529 minutes\n",
      "encoder learning rate: 3.95e-04, decoder learning rate: 3.95e-04\n",
      "[193/300] At -4.0 dB, Train Loss: 0.6688889861106873 Train BER 0.034129731357097626,                  \n",
      " [193/300] At -2.0 dB, Train Loss: 0.0436817966401577 Train BER 0.0016054053558036685\n",
      "Time for one full iteration is 7.0788 minutes\n",
      "encoder learning rate: 3.88e-04, decoder learning rate: 3.88e-04\n",
      "[194/300] At -4.0 dB, Train Loss: 0.6433815360069275 Train BER 0.0320918932557106,                  \n",
      " [194/300] At -2.0 dB, Train Loss: 0.03942030668258667 Train BER 0.001378378365188837\n",
      "Time for one full iteration is 7.1744 minutes\n",
      "encoder learning rate: 3.82e-04, decoder learning rate: 3.82e-04\n",
      "[195/300] At -4.0 dB, Train Loss: 0.6313267350196838 Train BER 0.032113514840602875,                  \n",
      " [195/300] At -2.0 dB, Train Loss: 0.04357025399804115 Train BER 0.001664864830672741\n",
      "Time for one full iteration is 7.1297 minutes\n",
      "encoder learning rate: 3.76e-04, decoder learning rate: 3.76e-04\n",
      "[196/300] At -4.0 dB, Train Loss: 0.6063747406005859 Train BER 0.029400000348687172,                  \n",
      " [196/300] At -2.0 dB, Train Loss: 0.03462419658899307 Train BER 0.0010972972959280014\n",
      "Time for one full iteration is 7.2510 minutes\n",
      "encoder learning rate: 3.70e-04, decoder learning rate: 3.70e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/300] At -4.0 dB, Train Loss: 0.6425070762634277 Train BER 0.0325513519346714,                  \n",
      " [197/300] At -2.0 dB, Train Loss: 0.04014090448617935 Train BER 0.0013945945538580418\n",
      "Time for one full iteration is 7.2987 minutes\n",
      "encoder learning rate: 3.64e-04, decoder learning rate: 3.64e-04\n",
      "[198/300] At -4.0 dB, Train Loss: 0.6474236845970154 Train BER 0.0323135145008564,                  \n",
      " [198/300] At -2.0 dB, Train Loss: 0.03875211626291275 Train BER 0.001448648632504046\n",
      "Time for one full iteration is 7.4300 minutes\n",
      "encoder learning rate: 3.58e-04, decoder learning rate: 3.58e-04\n",
      "[199/300] At -4.0 dB, Train Loss: 0.6788954734802246 Train BER 0.035237837582826614,                  \n",
      " [199/300] At -2.0 dB, Train Loss: 0.0385640412569046 Train BER 0.0014810811262577772\n",
      "Time for one full iteration is 7.3347 minutes\n",
      "encoder learning rate: 3.52e-04, decoder learning rate: 3.52e-04\n",
      "[200/300] At -4.0 dB, Train Loss: 0.6395766735076904 Train BER 0.03241081163287163,                  \n",
      " [200/300] At -2.0 dB, Train Loss: 0.036075085401535034 Train BER 0.001232432434335351\n",
      "Time for one full iteration is 7.2451 minutes\n",
      "encoder learning rate: 3.46e-04, decoder learning rate: 3.46e-04\n",
      "[201/300] At -4.0 dB, Train Loss: 0.6622011661529541 Train BER 0.034010812640190125,                  \n",
      " [201/300] At -2.0 dB, Train Loss: 0.04450808838009834 Train BER 0.0016378378495573997\n",
      "Time for one full iteration is 7.2149 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[202/300] At -4.0 dB, Train Loss: 0.6079730987548828 Train BER 0.030064864084124565,                  \n",
      " [202/300] At -2.0 dB, Train Loss: 0.0417567603290081 Train BER 0.0014540540287271142\n",
      "Time for one full iteration is 7.4004 minutes\n",
      "encoder learning rate: 3.34e-04, decoder learning rate: 3.34e-04\n",
      "[203/300] At -4.0 dB, Train Loss: 0.6272351741790771 Train BER 0.03166486322879791,                  \n",
      " [203/300] At -2.0 dB, Train Loss: 0.040243156254291534 Train BER 0.0015783783746883273\n",
      "Time for one full iteration is 7.3705 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[204/300] At -4.0 dB, Train Loss: 0.6368840932846069 Train BER 0.03203783929347992,                  \n",
      " [204/300] At -2.0 dB, Train Loss: 0.03623794764280319 Train BER 0.0011891891481354833\n",
      "Time for one full iteration is 7.3403 minutes\n",
      "encoder learning rate: 3.22e-04, decoder learning rate: 3.22e-04\n",
      "[205/300] At -4.0 dB, Train Loss: 0.6560023427009583 Train BER 0.03316756710410118,                  \n",
      " [205/300] At -2.0 dB, Train Loss: 0.042590681463479996 Train BER 0.0015513513935729861\n",
      "Time for one full iteration is 7.1114 minutes\n",
      "encoder learning rate: 3.17e-04, decoder learning rate: 3.17e-04\n",
      "[206/300] At -4.0 dB, Train Loss: 0.6171444654464722 Train BER 0.030735135078430176,                  \n",
      " [206/300] At -2.0 dB, Train Loss: 0.0399949736893177 Train BER 0.001448648632504046\n",
      "Time for one full iteration is 7.3510 minutes\n",
      "encoder learning rate: 3.11e-04, decoder learning rate: 3.11e-04\n",
      "[207/300] At -4.0 dB, Train Loss: 0.654343843460083 Train BER 0.032637838274240494,                  \n",
      " [207/300] At -2.0 dB, Train Loss: 0.03895406052470207 Train BER 0.001470270217396319\n",
      "Time for one full iteration is 7.1616 minutes\n",
      "encoder learning rate: 3.05e-04, decoder learning rate: 3.05e-04\n",
      "[208/300] At -4.0 dB, Train Loss: 0.6605220437049866 Train BER 0.03331351280212402,                  \n",
      " [208/300] At -2.0 dB, Train Loss: 0.040118902921676636 Train BER 0.0014378378400579095\n",
      "Time for one full iteration is 7.1793 minutes\n",
      "encoder learning rate: 2.99e-04, decoder learning rate: 2.99e-04\n",
      "[209/300] At -4.0 dB, Train Loss: 0.6283901929855347 Train BER 0.03178918734192848,                  \n",
      " [209/300] At -2.0 dB, Train Loss: 0.03740294277667999 Train BER 0.0013513513840734959\n",
      "Time for one full iteration is 7.1969 minutes\n",
      "encoder learning rate: 2.94e-04, decoder learning rate: 2.94e-04\n",
      "[210/300] At -4.0 dB, Train Loss: 0.6348353624343872 Train BER 0.03268108144402504,                  \n",
      " [210/300] At -2.0 dB, Train Loss: 0.034860026091337204 Train BER 0.001183783751912415\n",
      "Time for one full iteration is 7.3167 minutes\n",
      "encoder learning rate: 2.88e-04, decoder learning rate: 2.88e-04\n",
      "[211/300] At -4.0 dB, Train Loss: 0.6438190937042236 Train BER 0.032848648726940155,                  \n",
      " [211/300] At -2.0 dB, Train Loss: 0.039238180965185165 Train BER 0.0014378378400579095\n",
      "Time for one full iteration is 7.2271 minutes\n",
      "encoder learning rate: 2.82e-04, decoder learning rate: 2.82e-04\n",
      "[212/300] At -4.0 dB, Train Loss: 0.66727215051651 Train BER 0.03319999948143959,                  \n",
      " [212/300] At -2.0 dB, Train Loss: 0.038729991763830185 Train BER 0.001470270217396319\n",
      "Time for one full iteration is 7.3690 minutes\n",
      "encoder learning rate: 2.77e-04, decoder learning rate: 2.77e-04\n",
      "[213/300] At -4.0 dB, Train Loss: 0.6196337938308716 Train BER 0.03157297149300575,                  \n",
      " [213/300] At -2.0 dB, Train Loss: 0.038899488747119904 Train BER 0.0012216216418892145\n",
      "Time for one full iteration is 7.2733 minutes\n",
      "encoder learning rate: 2.71e-04, decoder learning rate: 2.71e-04\n",
      "[214/300] At -4.0 dB, Train Loss: 0.6715474724769592 Train BER 0.03426486626267433,                  \n",
      " [214/300] At -2.0 dB, Train Loss: 0.03262639418244362 Train BER 0.0010540540097281337\n",
      "Time for one full iteration is 7.3799 minutes\n",
      "encoder learning rate: 2.65e-04, decoder learning rate: 2.65e-04\n",
      "[215/300] At -4.0 dB, Train Loss: 0.6617471575737 Train BER 0.03363783657550812,                  \n",
      " [215/300] At -2.0 dB, Train Loss: 0.03791443631052971 Train BER 0.00130270270165056\n",
      "Time for one full iteration is 7.3204 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[216/300] At -4.0 dB, Train Loss: 0.6168133616447449 Train BER 0.031140539795160294,                  \n",
      " [216/300] At -2.0 dB, Train Loss: 0.042384617030620575 Train BER 0.0016270270571112633\n",
      "Time for one full iteration is 7.2970 minutes\n",
      "encoder learning rate: 2.54e-04, decoder learning rate: 2.54e-04\n",
      "[217/300] At -4.0 dB, Train Loss: 0.6626607775688171 Train BER 0.032972972840070724,                  \n",
      " [217/300] At -2.0 dB, Train Loss: 0.03872663155198097 Train BER 0.0012486486230045557\n",
      "Time for one full iteration is 7.1628 minutes\n",
      "encoder learning rate: 2.49e-04, decoder learning rate: 2.49e-04\n",
      "[218/300] At -4.0 dB, Train Loss: 0.6119515895843506 Train BER 0.031410809606313705,                  \n",
      " [218/300] At -2.0 dB, Train Loss: 0.032417017966508865 Train BER 0.0010702703148126602\n",
      "Time for one full iteration is 7.3954 minutes\n",
      "encoder learning rate: 2.44e-04, decoder learning rate: 2.44e-04\n",
      "[219/300] At -4.0 dB, Train Loss: 0.6519441604614258 Train BER 0.03313513472676277,                  \n",
      " [219/300] At -2.0 dB, Train Loss: 0.03712325543165207 Train BER 0.00139999995008111\n",
      "Time for one full iteration is 7.2862 minutes\n",
      "encoder learning rate: 2.38e-04, decoder learning rate: 2.38e-04\n",
      "[220/300] At -4.0 dB, Train Loss: 0.6320995688438416 Train BER 0.03179459273815155,                  \n",
      " [220/300] At -2.0 dB, Train Loss: 0.033244773745536804 Train BER 0.001086486503481865\n",
      "Time for one full iteration is 7.2559 minutes\n",
      "encoder learning rate: 2.33e-04, decoder learning rate: 2.33e-04\n",
      "[221/300] At -4.0 dB, Train Loss: 0.6239504814147949 Train BER 0.030556757003068924,                  \n",
      " [221/300] At -2.0 dB, Train Loss: 0.040257856249809265 Train BER 0.001427027047611773\n",
      "Time for one full iteration is 7.2092 minutes\n",
      "encoder learning rate: 2.28e-04, decoder learning rate: 2.28e-04\n",
      "[222/300] At -4.0 dB, Train Loss: 0.6582217216491699 Train BER 0.033059459179639816,                  \n",
      " [222/300] At -2.0 dB, Train Loss: 0.03771792724728584 Train BER 0.0013837837614119053\n",
      "Time for one full iteration is 7.2612 minutes\n",
      "encoder learning rate: 2.22e-04, decoder learning rate: 2.22e-04\n",
      "[223/300] At -4.0 dB, Train Loss: 0.6186188459396362 Train BER 0.031070269644260406,                  \n",
      " [223/300] At -2.0 dB, Train Loss: 0.03650300204753876 Train BER 0.001210810849443078\n",
      "Time for one full iteration is 7.2609 minutes\n",
      "encoder learning rate: 2.17e-04, decoder learning rate: 2.17e-04\n",
      "[224/300] At -4.0 dB, Train Loss: 0.6139419674873352 Train BER 0.03060000017285347,                  \n",
      " [224/300] At -2.0 dB, Train Loss: 0.03703128919005394 Train BER 0.0012432432267814875\n",
      "Time for one full iteration is 7.1859 minutes\n",
      "encoder learning rate: 2.12e-04, decoder learning rate: 2.12e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[225/300] At -4.0 dB, Train Loss: 0.6553863286972046 Train BER 0.033859457820653915,                  \n",
      " [225/300] At -2.0 dB, Train Loss: 0.03659120202064514 Train BER 0.0013729729689657688\n",
      "Time for one full iteration is 7.5888 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[226/300] At -4.0 dB, Train Loss: 0.6216424703598022 Train BER 0.030956756323575974,                  \n",
      " [226/300] At -2.0 dB, Train Loss: 0.03727870434522629 Train BER 0.001497297314926982\n",
      "Time for one full iteration is 7.4950 minutes\n",
      "encoder learning rate: 2.02e-04, decoder learning rate: 2.02e-04\n",
      "[227/300] At -4.0 dB, Train Loss: 0.6168511509895325 Train BER 0.0316324308514595,                  \n",
      " [227/300] At -2.0 dB, Train Loss: 0.03739878907799721 Train BER 0.0013837837614119053\n",
      "Time for one full iteration is 7.3075 minutes\n",
      "encoder learning rate: 1.97e-04, decoder learning rate: 1.97e-04\n",
      "[228/300] At -4.0 dB, Train Loss: 0.6546746492385864 Train BER 0.033232431858778,                  \n",
      " [228/300] At -2.0 dB, Train Loss: 0.03727419674396515 Train BER 0.0014216216513887048\n",
      "Time for one full iteration is 7.2231 minutes\n",
      "encoder learning rate: 1.92e-04, decoder learning rate: 1.92e-04\n",
      "[229/300] At -4.0 dB, Train Loss: 0.6221956610679626 Train BER 0.031108107417821884,                  \n",
      " [229/300] At -2.0 dB, Train Loss: 0.03631381690502167 Train BER 0.0013675675727427006\n",
      "Time for one full iteration is 7.1891 minutes\n",
      "encoder learning rate: 1.87e-04, decoder learning rate: 1.87e-04\n",
      "[230/300] At -4.0 dB, Train Loss: 0.5984236598014832 Train BER 0.0299567561596632,                  \n",
      " [230/300] At -2.0 dB, Train Loss: 0.03697690740227699 Train BER 0.0013675675727427006\n",
      "Time for one full iteration is 7.2737 minutes\n",
      "encoder learning rate: 1.82e-04, decoder learning rate: 1.82e-04\n",
      "[231/300] At -4.0 dB, Train Loss: 0.6852334141731262 Train BER 0.03472432494163513,                  \n",
      " [231/300] At -2.0 dB, Train Loss: 0.04009505733847618 Train BER 0.0013891891576349735\n",
      "Time for one full iteration is 7.1504 minutes\n",
      "encoder learning rate: 1.77e-04, decoder learning rate: 1.77e-04\n",
      "[232/300] At -4.0 dB, Train Loss: 0.6169472932815552 Train BER 0.03103243187069893,                  \n",
      " [232/300] At -2.0 dB, Train Loss: 0.041264768689870834 Train BER 0.0015351350884884596\n",
      "Time for one full iteration is 7.2317 minutes\n",
      "encoder learning rate: 1.73e-04, decoder learning rate: 1.73e-04\n",
      "[233/300] At -4.0 dB, Train Loss: 0.608135461807251 Train BER 0.03139999881386757,                  \n",
      " [233/300] At -2.0 dB, Train Loss: 0.035979319363832474 Train BER 0.0013351350789889693\n",
      "Time for one full iteration is 7.2488 minutes\n",
      "encoder learning rate: 1.68e-04, decoder learning rate: 1.68e-04\n",
      "[234/300] At -4.0 dB, Train Loss: 0.6054621934890747 Train BER 0.0309351347386837,                  \n",
      " [234/300] At -2.0 dB, Train Loss: 0.034142084419727325 Train BER 0.0011135134845972061\n",
      "Time for one full iteration is 7.2294 minutes\n",
      "encoder learning rate: 1.63e-04, decoder learning rate: 1.63e-04\n",
      "[235/300] At -4.0 dB, Train Loss: 0.5479044318199158 Train BER 0.027135135605931282,                  \n",
      " [235/300] At -2.0 dB, Train Loss: 0.039095427840948105 Train BER 0.001378378365188837\n",
      "Time for one full iteration is 7.2527 minutes\n",
      "encoder learning rate: 1.59e-04, decoder learning rate: 1.59e-04\n",
      "[236/300] At -4.0 dB, Train Loss: 0.6076813340187073 Train BER 0.030654054135084152,                  \n",
      " [236/300] At -2.0 dB, Train Loss: 0.03690774366259575 Train BER 0.0012216216418892145\n",
      "Time for one full iteration is 7.3802 minutes\n",
      "encoder learning rate: 1.54e-04, decoder learning rate: 1.54e-04\n",
      "[237/300] At -4.0 dB, Train Loss: 0.6152409911155701 Train BER 0.03155134990811348,                  \n",
      " [237/300] At -2.0 dB, Train Loss: 0.035951439291238785 Train BER 0.001356756780296564\n",
      "Time for one full iteration is 7.2878 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[238/300] At -4.0 dB, Train Loss: 0.6415087580680847 Train BER 0.03250270336866379,                  \n",
      " [238/300] At -2.0 dB, Train Loss: 0.03709394112229347 Train BER 0.0014162162551656365\n",
      "Time for one full iteration is 7.2764 minutes\n",
      "encoder learning rate: 1.45e-04, decoder learning rate: 1.45e-04\n",
      "[239/300] At -4.0 dB, Train Loss: 0.6359497308731079 Train BER 0.03224864974617958,                  \n",
      " [239/300] At -2.0 dB, Train Loss: 0.040815457701683044 Train BER 0.0016540540382266045\n",
      "Time for one full iteration is 7.1458 minutes\n",
      "encoder learning rate: 1.41e-04, decoder learning rate: 1.41e-04\n",
      "[240/300] At -4.0 dB, Train Loss: 0.6262190341949463 Train BER 0.03196756914258003,                  \n",
      " [240/300] At -2.0 dB, Train Loss: 0.03621884807944298 Train BER 0.0012378378305584192\n",
      "Time for one full iteration is 7.2866 minutes\n",
      "encoder learning rate: 1.36e-04, decoder learning rate: 1.36e-04\n",
      "[241/300] At -4.0 dB, Train Loss: 0.6545681953430176 Train BER 0.03408108279109001,                  \n",
      " [241/300] At -2.0 dB, Train Loss: 0.03677244111895561 Train BER 0.001183783751912415\n",
      "Time for one full iteration is 7.3474 minutes\n",
      "encoder learning rate: 1.32e-04, decoder learning rate: 1.32e-04\n",
      "[242/300] At -4.0 dB, Train Loss: 0.6173468232154846 Train BER 0.030578378587961197,                  \n",
      " [242/300] At -2.0 dB, Train Loss: 0.034708403050899506 Train BER 0.0012918919092044234\n",
      "Time for one full iteration is 7.3210 minutes\n",
      "encoder learning rate: 1.28e-04, decoder learning rate: 1.28e-04\n",
      "[243/300] At -4.0 dB, Train Loss: 0.6218083500862122 Train BER 0.03124324232339859,                  \n",
      " [243/300] At -2.0 dB, Train Loss: 0.03645944967865944 Train BER 0.0013081080978736281\n",
      "Time for one full iteration is 7.2484 minutes\n",
      "encoder learning rate: 1.24e-04, decoder learning rate: 1.24e-04\n",
      "[244/300] At -4.0 dB, Train Loss: 0.6354781985282898 Train BER 0.03215675801038742,                  \n",
      " [244/300] At -2.0 dB, Train Loss: 0.03755410015583038 Train BER 0.0013513513840734959\n",
      "Time for one full iteration is 7.3871 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[245/300] At -4.0 dB, Train Loss: 0.6342881321907043 Train BER 0.03194054216146469,                  \n",
      " [245/300] At -2.0 dB, Train Loss: 0.029853323474526405 Train BER 0.0010108108399435878\n",
      "Time for one full iteration is 7.2606 minutes\n",
      "encoder learning rate: 1.16e-04, decoder learning rate: 1.16e-04\n",
      "[246/300] At -4.0 dB, Train Loss: 0.6258166432380676 Train BER 0.03126486390829086,                  \n",
      " [246/300] At -2.0 dB, Train Loss: 0.03280779719352722 Train BER 0.0011891891481354833\n",
      "Time for one full iteration is 7.2099 minutes\n",
      "encoder learning rate: 1.12e-04, decoder learning rate: 1.12e-04\n",
      "[247/300] At -4.0 dB, Train Loss: 0.6190305352210999 Train BER 0.030951350927352905,                  \n",
      " [247/300] At -2.0 dB, Train Loss: 0.03449881449341774 Train BER 0.0012972973054274917\n",
      "Time for one full iteration is 7.3393 minutes\n",
      "encoder learning rate: 1.08e-04, decoder learning rate: 1.08e-04\n",
      "[248/300] At -4.0 dB, Train Loss: 0.6186057925224304 Train BER 0.030929729342460632,                  \n",
      " [248/300] At -2.0 dB, Train Loss: 0.03482664003968239 Train BER 0.0013081080978736281\n",
      "Time for one full iteration is 7.1925 minutes\n",
      "encoder learning rate: 1.04e-04, decoder learning rate: 1.04e-04\n",
      "[249/300] At -4.0 dB, Train Loss: 0.6118338108062744 Train BER 0.030491892248392105,                  \n",
      " [249/300] At -2.0 dB, Train Loss: 0.0399971567094326 Train BER 0.001378378365188837\n",
      "Time for one full iteration is 7.3150 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[250/300] At -4.0 dB, Train Loss: 0.618699848651886 Train BER 0.03175675496459007,                  \n",
      " [250/300] At -2.0 dB, Train Loss: 0.03629972040653229 Train BER 0.0012594594154506922\n",
      "Time for one full iteration is 7.3798 minutes\n",
      "encoder learning rate: 9.64e-05, decoder learning rate: 9.64e-05\n",
      "[251/300] At -4.0 dB, Train Loss: 0.6166430115699768 Train BER 0.03136756643652916,                  \n",
      " [251/300] At -2.0 dB, Train Loss: 0.03290291875600815 Train BER 0.0010810811072587967\n",
      "Time for one full iteration is 7.2657 minutes\n",
      "encoder learning rate: 9.27e-05, decoder learning rate: 9.27e-05\n",
      "[252/300] At -4.0 dB, Train Loss: 0.6048008799552917 Train BER 0.030243244022130966,                  \n",
      " [252/300] At -2.0 dB, Train Loss: 0.03731090947985649 Train BER 0.0014162162551656365\n",
      "Time for one full iteration is 7.4049 minutes\n",
      "encoder learning rate: 8.91e-05, decoder learning rate: 8.91e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253/300] At -4.0 dB, Train Loss: 0.6053382158279419 Train BER 0.030486486852169037,                  \n",
      " [253/300] At -2.0 dB, Train Loss: 0.03727317601442337 Train BER 0.0013243242865428329\n",
      "Time for one full iteration is 7.5462 minutes\n",
      "encoder learning rate: 8.56e-05, decoder learning rate: 8.56e-05\n",
      "[254/300] At -4.0 dB, Train Loss: 0.6436408758163452 Train BER 0.032691892236471176,                  \n",
      " [254/300] At -2.0 dB, Train Loss: 0.03508336842060089 Train BER 0.0011729729594662786\n",
      "Time for one full iteration is 7.5042 minutes\n",
      "encoder learning rate: 8.22e-05, decoder learning rate: 8.22e-05\n",
      "[255/300] At -4.0 dB, Train Loss: 0.624248206615448 Train BER 0.03194054216146469,                  \n",
      " [255/300] At -2.0 dB, Train Loss: 0.032479796558618546 Train BER 0.0010270270286127925\n",
      "Time for one full iteration is 7.4158 minutes\n",
      "encoder learning rate: 7.88e-05, decoder learning rate: 7.88e-05\n",
      "[256/300] At -4.0 dB, Train Loss: 0.6590412259101868 Train BER 0.03372972831130028,                  \n",
      " [256/300] At -2.0 dB, Train Loss: 0.03703343868255615 Train BER 0.0013189188903197646\n",
      "Time for one full iteration is 7.3702 minutes\n",
      "encoder learning rate: 7.54e-05, decoder learning rate: 7.54e-05\n",
      "[257/300] At -4.0 dB, Train Loss: 0.6041304469108582 Train BER 0.03005405329167843,                  \n",
      " [257/300] At -2.0 dB, Train Loss: 0.03843608871102333 Train BER 0.001329729682765901\n",
      "Time for one full iteration is 7.5397 minutes\n",
      "encoder learning rate: 7.22e-05, decoder learning rate: 7.22e-05\n",
      "[258/300] At -4.0 dB, Train Loss: 0.6349181532859802 Train BER 0.03175675496459007,                  \n",
      " [258/300] At -2.0 dB, Train Loss: 0.03559145703911781 Train BER 0.0013351350789889693\n",
      "Time for one full iteration is 7.4751 minutes\n",
      "encoder learning rate: 6.90e-05, decoder learning rate: 6.90e-05\n",
      "[259/300] At -4.0 dB, Train Loss: 0.6350054144859314 Train BER 0.03319999948143959,                  \n",
      " [259/300] At -2.0 dB, Train Loss: 0.036217208951711655 Train BER 0.0011891891481354833\n",
      "Time for one full iteration is 7.5690 minutes\n",
      "encoder learning rate: 6.58e-05, decoder learning rate: 6.58e-05\n",
      "[260/300] At -4.0 dB, Train Loss: 0.6539609432220459 Train BER 0.032600000500679016,                  \n",
      " [260/300] At -2.0 dB, Train Loss: 0.03813864663243294 Train BER 0.0013837837614119053\n",
      "Time for one full iteration is 7.5453 minutes\n",
      "encoder learning rate: 6.28e-05, decoder learning rate: 6.28e-05\n",
      "[261/300] At -4.0 dB, Train Loss: 0.6016351580619812 Train BER 0.03023243322968483,                  \n",
      " [261/300] At -2.0 dB, Train Loss: 0.03429752215743065 Train BER 0.0012486486230045557\n",
      "Time for one full iteration is 7.5755 minutes\n",
      "encoder learning rate: 5.98e-05, decoder learning rate: 5.98e-05\n",
      "[262/300] At -4.0 dB, Train Loss: 0.6369956731796265 Train BER 0.03252432495355606,                  \n",
      " [262/300] At -2.0 dB, Train Loss: 0.036772098392248154 Train BER 0.001281081116758287\n",
      "Time for one full iteration is 7.5037 minutes\n",
      "encoder learning rate: 5.69e-05, decoder learning rate: 5.69e-05\n",
      "[263/300] At -4.0 dB, Train Loss: 0.6151710152626038 Train BER 0.030897296965122223,                  \n",
      " [263/300] At -2.0 dB, Train Loss: 0.03708285465836525 Train BER 0.0012594594154506922\n",
      "Time for one full iteration is 7.5577 minutes\n",
      "encoder learning rate: 5.40e-05, decoder learning rate: 5.40e-05\n",
      "[264/300] At -4.0 dB, Train Loss: 0.6212614178657532 Train BER 0.031054053455591202,                  \n",
      " [264/300] At -2.0 dB, Train Loss: 0.035141073167324066 Train BER 0.0012270270381122828\n",
      "Time for one full iteration is 7.7320 minutes\n",
      "encoder learning rate: 5.12e-05, decoder learning rate: 5.12e-05\n",
      "[265/300] At -4.0 dB, Train Loss: 0.6328946352005005 Train BER 0.03193513676524162,                  \n",
      " [265/300] At -2.0 dB, Train Loss: 0.033790603280067444 Train BER 0.0011729729594662786\n",
      "Time for one full iteration is 7.6094 minutes\n",
      "encoder learning rate: 4.85e-05, decoder learning rate: 4.85e-05\n",
      "[266/300] At -4.0 dB, Train Loss: 0.6195384860038757 Train BER 0.03169729560613632,                  \n",
      " [266/300] At -2.0 dB, Train Loss: 0.028768660500645638 Train BER 0.0009783783461898565\n",
      "Time for one full iteration is 7.6524 minutes\n",
      "encoder learning rate: 4.59e-05, decoder learning rate: 4.59e-05\n",
      "[267/300] At -4.0 dB, Train Loss: 0.607625424861908 Train BER 0.030486486852169037,                  \n",
      " [267/300] At -2.0 dB, Train Loss: 0.03423537313938141 Train BER 0.001135135185904801\n",
      "Time for one full iteration is 7.6118 minutes\n",
      "encoder learning rate: 4.33e-05, decoder learning rate: 4.33e-05\n",
      "[268/300] At -4.0 dB, Train Loss: 0.6109353303909302 Train BER 0.03146486356854439,                  \n",
      " [268/300] At -2.0 dB, Train Loss: 0.03233499825000763 Train BER 0.001059459405951202\n",
      "Time for one full iteration is 7.5188 minutes\n",
      "encoder learning rate: 4.08e-05, decoder learning rate: 4.08e-05\n",
      "[269/300] At -4.0 dB, Train Loss: 0.6426234245300293 Train BER 0.03286486491560936,                  \n",
      " [269/300] At -2.0 dB, Train Loss: 0.03318050131201744 Train BER 0.001135135185904801\n",
      "Time for one full iteration is 7.5343 minutes\n",
      "encoder learning rate: 3.84e-05, decoder learning rate: 3.84e-05\n",
      "[270/300] At -4.0 dB, Train Loss: 0.6092463731765747 Train BER 0.0305945947766304,                  \n",
      " [270/300] At -2.0 dB, Train Loss: 0.03250531107187271 Train BER 0.001086486503481865\n",
      "Time for one full iteration is 7.4530 minutes\n",
      "encoder learning rate: 3.61e-05, decoder learning rate: 3.61e-05\n",
      "[271/300] At -4.0 dB, Train Loss: 0.6711527705192566 Train BER 0.033675674349069595,                  \n",
      " [271/300] At -2.0 dB, Train Loss: 0.03876687213778496 Train BER 0.0014918919187039137\n",
      "Time for one full iteration is 7.4906 minutes\n",
      "encoder learning rate: 3.38e-05, decoder learning rate: 3.38e-05\n",
      "[272/300] At -4.0 dB, Train Loss: 0.6490341424942017 Train BER 0.03296216204762459,                  \n",
      " [272/300] At -2.0 dB, Train Loss: 0.03332572057843208 Train BER 0.001232432434335351\n",
      "Time for one full iteration is 7.3810 minutes\n",
      "encoder learning rate: 3.16e-05, decoder learning rate: 3.16e-05\n",
      "[273/300] At -4.0 dB, Train Loss: 0.5976614952087402 Train BER 0.030243244022130966,                  \n",
      " [273/300] At -2.0 dB, Train Loss: 0.03337844833731651 Train BER 0.0011459459783509374\n",
      "Time for one full iteration is 7.3676 minutes\n",
      "encoder learning rate: 2.95e-05, decoder learning rate: 2.95e-05\n",
      "[274/300] At -4.0 dB, Train Loss: 0.5900985598564148 Train BER 0.029459459707140923,                  \n",
      " [274/300] At -2.0 dB, Train Loss: 0.035405483096838 Train BER 0.0013729729689657688\n",
      "Time for one full iteration is 7.4078 minutes\n",
      "encoder learning rate: 2.74e-05, decoder learning rate: 2.74e-05\n",
      "[275/300] At -4.0 dB, Train Loss: 0.6614248156547546 Train BER 0.03377837687730789,                  \n",
      " [275/300] At -2.0 dB, Train Loss: 0.03463255986571312 Train BER 0.0012270270381122828\n",
      "Time for one full iteration is 7.3853 minutes\n",
      "encoder learning rate: 2.54e-05, decoder learning rate: 2.54e-05\n",
      "[276/300] At -4.0 dB, Train Loss: 0.6412873268127441 Train BER 0.03203243389725685,                  \n",
      " [276/300] At -2.0 dB, Train Loss: 0.03003704361617565 Train BER 0.0009783783461898565\n",
      "Time for one full iteration is 7.3505 minutes\n",
      "encoder learning rate: 2.35e-05, decoder learning rate: 2.35e-05\n",
      "[277/300] At -4.0 dB, Train Loss: 0.6424199938774109 Train BER 0.03285945951938629,                  \n",
      " [277/300] At -2.0 dB, Train Loss: 0.027769798412919044 Train BER 0.0007891891873441637\n",
      "Time for one full iteration is 7.3934 minutes\n",
      "encoder learning rate: 2.17e-05, decoder learning rate: 2.17e-05\n",
      "[278/300] At -4.0 dB, Train Loss: 0.6287687420845032 Train BER 0.032324325293302536,                  \n",
      " [278/300] At -2.0 dB, Train Loss: 0.03528575599193573 Train BER 0.0011027026921510696\n",
      "Time for one full iteration is 7.2938 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[279/300] At -4.0 dB, Train Loss: 0.6258277893066406 Train BER 0.03215675801038742,                  \n",
      " [279/300] At -2.0 dB, Train Loss: 0.03577358275651932 Train BER 0.001329729682765901\n",
      "Time for one full iteration is 7.4704 minutes\n",
      "encoder learning rate: 1.83e-05, decoder learning rate: 1.83e-05\n",
      "[280/300] At -4.0 dB, Train Loss: 0.6559615135192871 Train BER 0.033005405217409134,                  \n",
      " [280/300] At -2.0 dB, Train Loss: 0.0346173420548439 Train BER 0.0011243242770433426\n",
      "Time for one full iteration is 7.5345 minutes\n",
      "encoder learning rate: 1.67e-05, decoder learning rate: 1.67e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281/300] At -4.0 dB, Train Loss: 0.6374692320823669 Train BER 0.03288648650050163,                  \n",
      " [281/300] At -2.0 dB, Train Loss: 0.036231573671102524 Train BER 0.0013459459878504276\n",
      "Time for one full iteration is 7.4969 minutes\n",
      "encoder learning rate: 1.52e-05, decoder learning rate: 1.52e-05\n",
      "[282/300] At -4.0 dB, Train Loss: 0.6209403872489929 Train BER 0.031470268964767456,                  \n",
      " [282/300] At -2.0 dB, Train Loss: 0.031170226633548737 Train BER 0.0010000000474974513\n",
      "Time for one full iteration is 7.4750 minutes\n",
      "encoder learning rate: 1.37e-05, decoder learning rate: 1.37e-05\n",
      "[283/300] At -4.0 dB, Train Loss: 0.6170528531074524 Train BER 0.03108648583292961,                  \n",
      " [283/300] At -2.0 dB, Train Loss: 0.0353909395635128 Train BER 0.0012216216418892145\n",
      "Time for one full iteration is 7.3722 minutes\n",
      "encoder learning rate: 1.24e-05, decoder learning rate: 1.24e-05\n",
      "[284/300] At -4.0 dB, Train Loss: 0.6185605525970459 Train BER 0.03144864737987518,                  \n",
      " [284/300] At -2.0 dB, Train Loss: 0.035063616931438446 Train BER 0.001254054019227624\n",
      "Time for one full iteration is 7.4418 minutes\n",
      "encoder learning rate: 1.11e-05, decoder learning rate: 1.11e-05\n",
      "[285/300] At -4.0 dB, Train Loss: 0.6210401058197021 Train BER 0.031416215002536774,                  \n",
      " [285/300] At -2.0 dB, Train Loss: 0.0294078066945076 Train BER 0.0009621621575206518\n",
      "Time for one full iteration is 7.7100 minutes\n",
      "encoder learning rate: 9.85e-06, decoder learning rate: 9.85e-06\n",
      "[286/300] At -4.0 dB, Train Loss: 0.6544685363769531 Train BER 0.03314594551920891,                  \n",
      " [286/300] At -2.0 dB, Train Loss: 0.033900272101163864 Train BER 0.0012000000569969416\n",
      "Time for one full iteration is 7.6561 minutes\n",
      "encoder learning rate: 8.71e-06, decoder learning rate: 8.71e-06\n",
      "[287/300] At -4.0 dB, Train Loss: 0.6033951044082642 Train BER 0.03020000085234642,                  \n",
      " [287/300] At -2.0 dB, Train Loss: 0.03562939539551735 Train BER 0.001281081116758287\n",
      "Time for one full iteration is 7.3561 minutes\n",
      "encoder learning rate: 7.65e-06, decoder learning rate: 7.65e-06\n",
      "[288/300] At -4.0 dB, Train Loss: 0.6262965798377991 Train BER 0.031691890209913254,                  \n",
      " [288/300] At -2.0 dB, Train Loss: 0.03967982903122902 Train BER 0.0015513513935729861\n",
      "Time for one full iteration is 7.3062 minutes\n",
      "encoder learning rate: 6.67e-06, decoder learning rate: 6.67e-06\n",
      "[289/300] At -4.0 dB, Train Loss: 0.6030178070068359 Train BER 0.03038378432393074,                  \n",
      " [289/300] At -2.0 dB, Train Loss: 0.03734181448817253 Train BER 0.0013513513840734959\n",
      "Time for one full iteration is 7.4262 minutes\n",
      "encoder learning rate: 5.76e-06, decoder learning rate: 5.76e-06\n",
      "[290/300] At -4.0 dB, Train Loss: 0.6344478130340576 Train BER 0.032140541821718216,                  \n",
      " [290/300] At -2.0 dB, Train Loss: 0.04039068892598152 Train BER 0.0015297296922653913\n",
      "Time for one full iteration is 7.3209 minutes\n",
      "encoder learning rate: 4.94e-06, decoder learning rate: 4.94e-06\n",
      "[291/300] At -4.0 dB, Train Loss: 0.6168074011802673 Train BER 0.03136216104030609,                  \n",
      " [291/300] At -2.0 dB, Train Loss: 0.03424058482050896 Train BER 0.0010702703148126602\n",
      "Time for one full iteration is 7.3427 minutes\n",
      "encoder learning rate: 4.19e-06, decoder learning rate: 4.19e-06\n",
      "[292/300] At -4.0 dB, Train Loss: 0.5874344110488892 Train BER 0.029081081971526146,                  \n",
      " [292/300] At -2.0 dB, Train Loss: 0.03878050670027733 Train BER 0.0014594594249501824\n",
      "Time for one full iteration is 7.4599 minutes\n",
      "encoder learning rate: 3.52e-06, decoder learning rate: 3.52e-06\n",
      "[293/300] At -4.0 dB, Train Loss: 0.6161336898803711 Train BER 0.031075675040483475,                  \n",
      " [293/300] At -2.0 dB, Train Loss: 0.03814356029033661 Train BER 0.0014864865224808455\n",
      "Time for one full iteration is 7.5066 minutes\n",
      "encoder learning rate: 2.93e-06, decoder learning rate: 2.93e-06\n",
      "[294/300] At -4.0 dB, Train Loss: 0.601249098777771 Train BER 0.029551351442933083,                  \n",
      " [294/300] At -2.0 dB, Train Loss: 0.03426765277981758 Train BER 0.0011891891481354833\n",
      "Time for one full iteration is 7.3925 minutes\n",
      "encoder learning rate: 2.42e-06, decoder learning rate: 2.42e-06\n",
      "[295/300] At -4.0 dB, Train Loss: 0.6186460256576538 Train BER 0.031129729002714157,                  \n",
      " [295/300] At -2.0 dB, Train Loss: 0.03856982663273811 Train BER 0.0014648648211732507\n",
      "Time for one full iteration is 7.4863 minutes\n",
      "encoder learning rate: 1.99e-06, decoder learning rate: 1.99e-06\n",
      "[296/300] At -4.0 dB, Train Loss: 0.6297627091407776 Train BER 0.03117837756872177,                  \n",
      " [296/300] At -2.0 dB, Train Loss: 0.034741032868623734 Train BER 0.0012756757205352187\n",
      "Time for one full iteration is 7.5698 minutes\n",
      "encoder learning rate: 1.63e-06, decoder learning rate: 1.63e-06\n",
      "[297/300] At -4.0 dB, Train Loss: 0.6110900640487671 Train BER 0.030302703380584717,                  \n",
      " [297/300] At -2.0 dB, Train Loss: 0.027285676449537277 Train BER 0.0008270270191133022\n",
      "Time for one full iteration is 7.5042 minutes\n",
      "encoder learning rate: 1.35e-06, decoder learning rate: 1.35e-06\n",
      "[298/300] At -4.0 dB, Train Loss: 0.60430908203125 Train BER 0.02964864857494831,                  \n",
      " [298/300] At -2.0 dB, Train Loss: 0.03336898237466812 Train BER 0.0012648648116737604\n",
      "Time for one full iteration is 7.2488 minutes\n",
      "encoder learning rate: 1.16e-06, decoder learning rate: 1.16e-06\n",
      "[299/300] At -4.0 dB, Train Loss: 0.6131221055984497 Train BER 0.03044864907860756,                  \n",
      " [299/300] At -2.0 dB, Train Loss: 0.03387738764286041 Train BER 0.0011945945443585515\n",
      "Time for one full iteration is 7.2075 minutes\n",
      "encoder learning rate: 1.04e-06, decoder learning rate: 1.04e-06\n",
      "[300/300] At -4.0 dB, Train Loss: 0.6210132837295532 Train BER 0.030929729342460632,                  \n",
      " [300/300] At -2.0 dB, Train Loss: 0.03603220731019974 Train BER 0.0014108108589425683\n",
      "Time for one full iteration is 7.2127 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO__-2.0_Encoder_KO_-4.0_Decoder/epochs_300_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/shubham/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/scipy/stats/_morestats.py:1882: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeppolar Shapiro test W = 0.9262449741363525, p-value = 0.0\n",
      "Gaussian Shapiro test W = 1.000001311302185, p-value = 1.0\n",
      "Polar Shapiro test W = 0.6373023986816406, p-value = 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166434, BLER: 0.435151\n",
      "Deep - BER: 0.090675, BLER: 0.459936\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072516, BLER: 0.196626\n",
      "Deep - BER: 0.030750, BLER: 0.218728\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.019952, BLER: 0.055688\n",
      "Deep - BER: 0.006957, BLER: 0.080867\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003065, BLER: 0.008597\n",
      "Deep - BER: 0.001250, BLER: 0.026993\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000187, BLER: 0.000554\n",
      "Deep - BER: 0.000286, BLER: 0.009374\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.09067532446235418, 0.030749729650095106, 0.0069570810818113385, 0.001250108109874418, 0.00028586486506901565]\n",
      "BERs of SC decoding: [0.16643383772671222, 0.07251551350951195, 0.019951999983750284, 0.0030645945903670507, 0.00018651351362314016]\n",
      "BLERs of DeepPolar: [0.45993600000000046, 0.21872800000000017, 0.08086700000000001, 0.026992999999999902, 0.009373999999999955]\n",
      "BLERs of SC decoding: [0.4351509999999994, 0.19662599999999994, 0.05568799999999982, 0.008596999999999952, 0.0005540000000000003]\n",
      "time = 373.83906152645744 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
