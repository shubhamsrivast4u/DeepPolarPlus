{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = True  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 300  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = -1.  # SNR at which encoder is trained\n",
    "dec_train_snr = -3.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.0005  # Decoder Learning rate\n",
    "enc_lr = 0.0005  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}__{enc_train_snr}_Encoder_{decoder_type}_{dec_train_snr}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5) + criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=50,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/300] At -3.0 dB, Train Loss: 8.424901962280273 Train BER 0.47981080412864685,                  \n",
      " [1/300] At -1.0 dB, Train Loss: 8.14743709564209 Train BER 0.47412431240081787\n",
      "Time for one full iteration is 9.1590 minutes\n",
      "encoder learning rate: 1.00e-05, decoder learning rate: 1.00e-05\n",
      "[2/300] At -3.0 dB, Train Loss: 6.889469146728516 Train BER 0.47239458560943604,                  \n",
      " [2/300] At -1.0 dB, Train Loss: 6.787270545959473 Train BER 0.4678216278553009\n",
      "Time for one full iteration is 9.1618 minutes\n",
      "encoder learning rate: 2.00e-05, decoder learning rate: 2.00e-05\n",
      "[3/300] At -3.0 dB, Train Loss: 6.2750163078308105 Train BER 0.46676215529441833,                  \n",
      " [3/300] At -1.0 dB, Train Loss: 6.239981174468994 Train BER 0.4639405310153961\n",
      "Time for one full iteration is 9.1033 minutes\n",
      "encoder learning rate: 3.00e-05, decoder learning rate: 3.00e-05\n",
      "[4/300] At -3.0 dB, Train Loss: 6.01016902923584 Train BER 0.4645189046859741,                  \n",
      " [4/300] At -1.0 dB, Train Loss: 5.99002742767334 Train BER 0.4581945836544037\n",
      "Time for one full iteration is 8.9876 minutes\n",
      "encoder learning rate: 4.00e-05, decoder learning rate: 4.00e-05\n",
      "[5/300] At -3.0 dB, Train Loss: 5.842652797698975 Train BER 0.44401082396507263,                  \n",
      " [5/300] At -1.0 dB, Train Loss: 5.758577823638916 Train BER 0.42663782835006714\n",
      "Time for one full iteration is 8.7370 minutes\n",
      "encoder learning rate: 5.00e-05, decoder learning rate: 5.00e-05\n",
      "[6/300] At -3.0 dB, Train Loss: 5.6368818283081055 Train BER 0.4171946048736572,                  \n",
      " [6/300] At -1.0 dB, Train Loss: 5.392050266265869 Train BER 0.3815135061740875\n",
      "Time for one full iteration is 8.7581 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[7/300] At -3.0 dB, Train Loss: 3.980458974838257 Train BER 0.2472756803035736,                  \n",
      " [7/300] At -1.0 dB, Train Loss: 3.0150022506713867 Train BER 0.1779189258813858\n",
      "Time for one full iteration is 8.6820 minutes\n",
      "encoder learning rate: 7.00e-05, decoder learning rate: 7.00e-05\n",
      "[8/300] At -3.0 dB, Train Loss: 2.698373317718506 Train BER 0.14712432026863098,                  \n",
      " [8/300] At -1.0 dB, Train Loss: 1.3600823879241943 Train BER 0.06142162159085274\n",
      "Time for one full iteration is 8.8381 minutes\n",
      "encoder learning rate: 8.00e-05, decoder learning rate: 8.00e-05\n",
      "[9/300] At -3.0 dB, Train Loss: 1.975895881652832 Train BER 0.10133513808250427,                  \n",
      " [9/300] At -1.0 dB, Train Loss: 0.7706388831138611 Train BER 0.03397837653756142\n",
      "Time for one full iteration is 8.6550 minutes\n",
      "encoder learning rate: 9.00e-05, decoder learning rate: 9.00e-05\n",
      "[10/300] At -3.0 dB, Train Loss: 1.450713872909546 Train BER 0.07240540534257889,                  \n",
      " [10/300] At -1.0 dB, Train Loss: 0.4788409471511841 Train BER 0.02191351354122162\n",
      "Time for one full iteration is 8.7499 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[11/300] At -3.0 dB, Train Loss: 1.2117958068847656 Train BER 0.06005945801734924,                  \n",
      " [11/300] At -1.0 dB, Train Loss: 0.3753460943698883 Train BER 0.018032431602478027\n",
      "Time for one full iteration is 8.9278 minutes\n",
      "encoder learning rate: 1.10e-04, decoder learning rate: 1.10e-04\n",
      "[12/300] At -3.0 dB, Train Loss: 1.0276718139648438 Train BER 0.05025405436754227,                  \n",
      " [12/300] At -1.0 dB, Train Loss: 0.30899617075920105 Train BER 0.014248648658394814\n",
      "Time for one full iteration is 8.8150 minutes\n",
      "encoder learning rate: 1.20e-04, decoder learning rate: 1.20e-04\n",
      "[13/300] At -3.0 dB, Train Loss: 0.828921914100647 Train BER 0.037167567759752274,                  \n",
      " [13/300] At -1.0 dB, Train Loss: 0.1834692656993866 Train BER 0.0055891890078783035\n",
      "Time for one full iteration is 8.6580 minutes\n",
      "encoder learning rate: 1.30e-04, decoder learning rate: 1.30e-04\n",
      "[14/300] At -3.0 dB, Train Loss: 0.7100198864936829 Train BER 0.03154594451189041,                  \n",
      " [14/300] At -1.0 dB, Train Loss: 0.11954899877309799 Train BER 0.0033945946488529444\n",
      "Time for one full iteration is 8.7522 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[15/300] At -3.0 dB, Train Loss: 0.6173946261405945 Train BER 0.027254054322838783,                  \n",
      " [15/300] At -1.0 dB, Train Loss: 0.06944192200899124 Train BER 0.0020216216798871756\n",
      "Time for one full iteration is 8.6403 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[16/300] At -3.0 dB, Train Loss: 0.5459715723991394 Train BER 0.024367567151784897,                  \n",
      " [16/300] At -1.0 dB, Train Loss: 0.05017638951539993 Train BER 0.0011891891481354833\n",
      "Time for one full iteration is 8.7123 minutes\n",
      "encoder learning rate: 1.60e-04, decoder learning rate: 1.60e-04\n",
      "[17/300] At -3.0 dB, Train Loss: 0.518236517906189 Train BER 0.023205405101180077,                  \n",
      " [17/300] At -1.0 dB, Train Loss: 0.04793553054332733 Train BER 0.0012648648116737604\n",
      "Time for one full iteration is 9.0495 minutes\n",
      "encoder learning rate: 1.70e-04, decoder learning rate: 1.70e-04\n",
      "[18/300] At -3.0 dB, Train Loss: 0.460712194442749 Train BER 0.01997837796807289,                  \n",
      " [18/300] At -1.0 dB, Train Loss: 0.03988288342952728 Train BER 0.000994594651274383\n",
      "Time for one full iteration is 9.1145 minutes\n",
      "encoder learning rate: 1.80e-04, decoder learning rate: 1.80e-04\n",
      "[19/300] At -3.0 dB, Train Loss: 0.42533448338508606 Train BER 0.01896756701171398,                  \n",
      " [19/300] At -1.0 dB, Train Loss: 0.03720548376441002 Train BER 0.0011189188808202744\n",
      "Time for one full iteration is 9.0834 minutes\n",
      "encoder learning rate: 1.90e-04, decoder learning rate: 1.90e-04\n",
      "[20/300] At -3.0 dB, Train Loss: 0.4066914916038513 Train BER 0.018135135993361473,                  \n",
      " [20/300] At -1.0 dB, Train Loss: 0.025534456595778465 Train BER 0.0006378378602676094\n",
      "Time for one full iteration is 9.0598 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[21/300] At -3.0 dB, Train Loss: 0.39545145630836487 Train BER 0.017578378319740295,                  \n",
      " [21/300] At -1.0 dB, Train Loss: 0.022448617964982986 Train BER 0.0005729729891754687\n",
      "Time for one full iteration is 9.1734 minutes\n",
      "encoder learning rate: 2.10e-04, decoder learning rate: 2.10e-04\n",
      "[22/300] At -3.0 dB, Train Loss: 0.35167932510375977 Train BER 0.015583783388137817,                  \n",
      " [22/300] At -1.0 dB, Train Loss: 0.023046676069498062 Train BER 0.0007351351086981595\n",
      "Time for one full iteration is 9.1667 minutes\n",
      "encoder learning rate: 2.20e-04, decoder learning rate: 2.20e-04\n",
      "[23/300] At -3.0 dB, Train Loss: 0.3779442608356476 Train BER 0.016729729250073433,                  \n",
      " [23/300] At -1.0 dB, Train Loss: 0.023668095469474792 Train BER 0.0006324324058368802\n",
      "Time for one full iteration is 9.0435 minutes\n",
      "encoder learning rate: 2.30e-04, decoder learning rate: 2.30e-04\n",
      "[24/300] At -3.0 dB, Train Loss: 0.3395095765590668 Train BER 0.01572972908616066,                  \n",
      " [24/300] At -1.0 dB, Train Loss: 0.016746480017900467 Train BER 0.00047567568253725767\n",
      "Time for one full iteration is 9.0947 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[25/300] At -3.0 dB, Train Loss: 0.31487807631492615 Train BER 0.013978377915918827,                  \n",
      " [25/300] At -1.0 dB, Train Loss: 0.014547763392329216 Train BER 0.00033513514790683985\n",
      "Time for one full iteration is 9.2142 minutes\n",
      "encoder learning rate: 2.50e-04, decoder learning rate: 2.50e-04\n",
      "[26/300] At -3.0 dB, Train Loss: 0.3160923421382904 Train BER 0.014162162318825722,                  \n",
      " [26/300] At -1.0 dB, Train Loss: 0.01352654304355383 Train BER 0.0003621621581260115\n",
      "Time for one full iteration is 9.0489 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[27/300] At -3.0 dB, Train Loss: 0.30998411774635315 Train BER 0.014140540733933449,                  \n",
      " [27/300] At -1.0 dB, Train Loss: 0.014919484034180641 Train BER 0.00042162163299508393\n",
      "Time for one full iteration is 9.0806 minutes\n",
      "encoder learning rate: 2.70e-04, decoder learning rate: 2.70e-04\n",
      "[28/300] At -3.0 dB, Train Loss: 0.3006242513656616 Train BER 0.01390810776501894,                  \n",
      " [28/300] At -1.0 dB, Train Loss: 0.01301566045731306 Train BER 0.0003621621581260115\n",
      "Time for one full iteration is 9.0211 minutes\n",
      "encoder learning rate: 2.80e-04, decoder learning rate: 2.80e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/300] At -3.0 dB, Train Loss: 0.3101460933685303 Train BER 0.014313513413071632,                  \n",
      " [29/300] At -1.0 dB, Train Loss: 0.015411228872835636 Train BER 0.00047567568253725767\n",
      "Time for one full iteration is 8.9645 minutes\n",
      "encoder learning rate: 2.90e-04, decoder learning rate: 2.90e-04\n",
      "[30/300] At -3.0 dB, Train Loss: 0.29833462834358215 Train BER 0.013751351274549961,                  \n",
      " [30/300] At -1.0 dB, Train Loss: 0.011065156199038029 Train BER 0.0002918918908108026\n",
      "Time for one full iteration is 9.0264 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[31/300] At -3.0 dB, Train Loss: 0.34205162525177 Train BER 0.016086487099528313,                  \n",
      " [31/300] At -1.0 dB, Train Loss: 0.010930515825748444 Train BER 0.0002972972870338708\n",
      "Time for one full iteration is 9.0750 minutes\n",
      "encoder learning rate: 3.10e-04, decoder learning rate: 3.10e-04\n",
      "[32/300] At -3.0 dB, Train Loss: 0.27083152532577515 Train BER 0.012400000356137753,                  \n",
      " [32/300] At -1.0 dB, Train Loss: 0.01524851843714714 Train BER 0.0005027027218602598\n",
      "Time for one full iteration is 9.1635 minutes\n",
      "encoder learning rate: 3.20e-04, decoder learning rate: 3.20e-04\n",
      "[33/300] At -3.0 dB, Train Loss: 0.27555257081985474 Train BER 0.012459459714591503,                  \n",
      " [33/300] At -1.0 dB, Train Loss: 0.01633736491203308 Train BER 0.0005675675929524004\n",
      "Time for one full iteration is 9.0205 minutes\n",
      "encoder learning rate: 3.30e-04, decoder learning rate: 3.30e-04\n",
      "[34/300] At -3.0 dB, Train Loss: 0.27825212478637695 Train BER 0.012708107940852642,                  \n",
      " [34/300] At -1.0 dB, Train Loss: 0.01786286011338234 Train BER 0.0006702702958136797\n",
      "Time for one full iteration is 8.9955 minutes\n",
      "encoder learning rate: 3.40e-04, decoder learning rate: 3.40e-04\n",
      "[35/300] At -3.0 dB, Train Loss: 0.26245784759521484 Train BER 0.012059459462761879,                  \n",
      " [35/300] At -1.0 dB, Train Loss: 0.012292668223381042 Train BER 0.0003729729796759784\n",
      "Time for one full iteration is 9.0112 minutes\n",
      "encoder learning rate: 3.50e-04, decoder learning rate: 3.50e-04\n",
      "[36/300] At -3.0 dB, Train Loss: 0.2821851968765259 Train BER 0.013297297060489655,                  \n",
      " [36/300] At -1.0 dB, Train Loss: 0.01560946274548769 Train BER 0.0005621621385216713\n",
      "Time for one full iteration is 9.0441 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[37/300] At -3.0 dB, Train Loss: 0.25967270135879517 Train BER 0.012091891840100288,                  \n",
      " [37/300] At -1.0 dB, Train Loss: 0.013449236750602722 Train BER 0.00042162163299508393\n",
      "Time for one full iteration is 9.0037 minutes\n",
      "encoder learning rate: 3.70e-04, decoder learning rate: 3.70e-04\n",
      "[38/300] At -3.0 dB, Train Loss: 0.2732469439506531 Train BER 0.013172972947359085,                  \n",
      " [38/300] At -1.0 dB, Train Loss: 0.010260902345180511 Train BER 0.0002918918908108026\n",
      "Time for one full iteration is 9.0016 minutes\n",
      "encoder learning rate: 3.80e-04, decoder learning rate: 3.80e-04\n",
      "[39/300] At -3.0 dB, Train Loss: 0.25315070152282715 Train BER 0.011745945550501347,                  \n",
      " [39/300] At -1.0 dB, Train Loss: 0.009544826112687588 Train BER 0.0003027027123607695\n",
      "Time for one full iteration is 8.9835 minutes\n",
      "encoder learning rate: 3.90e-04, decoder learning rate: 3.90e-04\n",
      "[40/300] At -3.0 dB, Train Loss: 0.26278117299079895 Train BER 0.01208648644387722,                  \n",
      " [40/300] At -1.0 dB, Train Loss: 0.011436505243182182 Train BER 0.0003891891974490136\n",
      "Time for one full iteration is 8.9615 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[41/300] At -3.0 dB, Train Loss: 0.2497226893901825 Train BER 0.011913513764739037,                  \n",
      " [41/300] At -1.0 dB, Train Loss: 0.007961061783134937 Train BER 0.0002162162127206102\n",
      "Time for one full iteration is 8.9134 minutes\n",
      "encoder learning rate: 4.10e-04, decoder learning rate: 4.10e-04\n",
      "[42/300] At -3.0 dB, Train Loss: 0.26133736968040466 Train BER 0.012064864858984947,                  \n",
      " [42/300] At -1.0 dB, Train Loss: 0.009221483953297138 Train BER 0.0002756756730377674\n",
      "Time for one full iteration is 8.8882 minutes\n",
      "encoder learning rate: 4.20e-04, decoder learning rate: 4.20e-04\n",
      "[43/300] At -3.0 dB, Train Loss: 0.2530903220176697 Train BER 0.011837838217616081,                  \n",
      " [43/300] At -1.0 dB, Train Loss: 0.01142814103513956 Train BER 0.0004378378507681191\n",
      "Time for one full iteration is 8.9575 minutes\n",
      "encoder learning rate: 4.30e-04, decoder learning rate: 4.30e-04\n",
      "[44/300] At -3.0 dB, Train Loss: 0.2700864374637604 Train BER 0.012708107940852642,                  \n",
      " [44/300] At -1.0 dB, Train Loss: 0.012225436978042126 Train BER 0.0003459459403529763\n",
      "Time for one full iteration is 8.9804 minutes\n",
      "encoder learning rate: 4.40e-04, decoder learning rate: 4.40e-04\n",
      "[45/300] At -3.0 dB, Train Loss: 0.26703301072120667 Train BER 0.012518919073045254,                  \n",
      " [45/300] At -1.0 dB, Train Loss: 0.011240736581385136 Train BER 0.0003459459403529763\n",
      "Time for one full iteration is 8.9584 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[46/300] At -3.0 dB, Train Loss: 0.2554112672805786 Train BER 0.012221621349453926,                  \n",
      " [46/300] At -1.0 dB, Train Loss: 0.017757678404450417 Train BER 0.0006540540489368141\n",
      "Time for one full iteration is 9.0071 minutes\n",
      "encoder learning rate: 4.60e-04, decoder learning rate: 4.60e-04\n",
      "[47/300] At -3.0 dB, Train Loss: 0.2414167821407318 Train BER 0.011318919248878956,                  \n",
      " [47/300] At -1.0 dB, Train Loss: 0.010233065113425255 Train BER 0.0003027027123607695\n",
      "Time for one full iteration is 8.9574 minutes\n",
      "encoder learning rate: 4.70e-04, decoder learning rate: 4.70e-04\n",
      "[48/300] At -3.0 dB, Train Loss: 0.2741486132144928 Train BER 0.01280540507286787,                  \n",
      " [48/300] At -1.0 dB, Train Loss: 0.020032374188303947 Train BER 0.0007621621480211616\n",
      "Time for one full iteration is 9.0060 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[49/300] At -3.0 dB, Train Loss: 0.2785681486129761 Train BER 0.01348108146339655,                  \n",
      " [49/300] At -1.0 dB, Train Loss: 0.011763638816773891 Train BER 0.000410810811445117\n",
      "Time for one full iteration is 8.9738 minutes\n",
      "encoder learning rate: 4.90e-04, decoder learning rate: 4.90e-04\n",
      "[50/300] At -3.0 dB, Train Loss: 0.26772651076316833 Train BER 0.012491892091929913,                  \n",
      " [50/300] At -1.0 dB, Train Loss: 0.0161973275244236 Train BER 0.0005891891778446734\n",
      "Time for one full iteration is 9.0117 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[51/300] At -3.0 dB, Train Loss: 0.26330703496932983 Train BER 0.012535135261714458,                  \n",
      " [51/300] At -1.0 dB, Train Loss: 0.01041176076978445 Train BER 0.0003027027123607695\n",
      "Time for one full iteration is 8.9168 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[52/300] At -3.0 dB, Train Loss: 0.2613506317138672 Train BER 0.012502702884376049,                  \n",
      " [52/300] At -1.0 dB, Train Loss: 0.015345402993261814 Train BER 0.0005837837816216052\n",
      "Time for one full iteration is 8.9644 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[53/300] At -3.0 dB, Train Loss: 0.28121697902679443 Train BER 0.013324324041604996,                  \n",
      " [53/300] At -1.0 dB, Train Loss: 0.008751530200242996 Train BER 0.00022702703427057713\n",
      "Time for one full iteration is 8.9464 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[54/300] At -3.0 dB, Train Loss: 0.22618703544139862 Train BER 0.010427027009427547,                  \n",
      " [54/300] At -1.0 dB, Train Loss: 0.011258135549724102 Train BER 0.000351351365679875\n",
      "Time for one full iteration is 8.9368 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[55/300] At -3.0 dB, Train Loss: 0.23199942708015442 Train BER 0.011654053814709187,                  \n",
      " [55/300] At -1.0 dB, Train Loss: 0.007905056700110435 Train BER 0.00028108106926083565\n",
      "Time for one full iteration is 8.9243 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[56/300] At -3.0 dB, Train Loss: 0.24363520741462708 Train BER 0.011621621437370777,                  \n",
      " [56/300] At -1.0 dB, Train Loss: 0.009536733850836754 Train BER 0.00033513514790683985\n",
      "Time for one full iteration is 8.9743 minutes\n",
      "encoder learning rate: 4.99e-04, decoder learning rate: 4.99e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/300] At -3.0 dB, Train Loss: 0.2424715757369995 Train BER 0.011740540154278278,                  \n",
      " [57/300] At -1.0 dB, Train Loss: 0.007176538929343224 Train BER 0.00022702703427057713\n",
      "Time for one full iteration is 8.9807 minutes\n",
      "encoder learning rate: 4.99e-04, decoder learning rate: 4.99e-04\n",
      "[58/300] At -3.0 dB, Train Loss: 0.24051664769649506 Train BER 0.011243243701756,                  \n",
      " [58/300] At -1.0 dB, Train Loss: 0.008988621644675732 Train BER 0.0002648648514878005\n",
      "Time for one full iteration is 8.9798 minutes\n",
      "encoder learning rate: 4.99e-04, decoder learning rate: 4.99e-04\n",
      "[59/300] At -3.0 dB, Train Loss: 0.18594440817832947 Train BER 0.008551351726055145,                  \n",
      " [59/300] At -1.0 dB, Train Loss: 0.006830419879406691 Train BER 0.0002162162127206102\n",
      "Time for one full iteration is 8.9149 minutes\n",
      "encoder learning rate: 4.98e-04, decoder learning rate: 4.98e-04\n",
      "[60/300] At -3.0 dB, Train Loss: 0.21660080552101135 Train BER 0.010118918493390083,                  \n",
      " [60/300] At -1.0 dB, Train Loss: 0.009581897407770157 Train BER 0.000351351365679875\n",
      "Time for one full iteration is 8.9493 minutes\n",
      "encoder learning rate: 4.98e-04, decoder learning rate: 4.98e-04\n",
      "[61/300] At -3.0 dB, Train Loss: 0.2311120480298996 Train BER 0.011113513261079788,                  \n",
      " [61/300] At -1.0 dB, Train Loss: 0.00633765384554863 Train BER 0.0002162162127206102\n",
      "Time for one full iteration is 9.0179 minutes\n",
      "encoder learning rate: 4.98e-04, decoder learning rate: 4.98e-04\n",
      "[62/300] At -3.0 dB, Train Loss: 0.23263764381408691 Train BER 0.011254054494202137,                  \n",
      " [62/300] At -1.0 dB, Train Loss: 0.010623534210026264 Train BER 0.00039459459367208183\n",
      "Time for one full iteration is 9.0030 minutes\n",
      "encoder learning rate: 4.97e-04, decoder learning rate: 4.97e-04\n",
      "[63/300] At -3.0 dB, Train Loss: 0.20424173772335052 Train BER 0.009416216053068638,                  \n",
      " [63/300] At -1.0 dB, Train Loss: 0.006602526642382145 Train BER 0.00023243243049364537\n",
      "Time for one full iteration is 8.9746 minutes\n",
      "encoder learning rate: 4.97e-04, decoder learning rate: 4.97e-04\n",
      "[64/300] At -3.0 dB, Train Loss: 0.22324448823928833 Train BER 0.010551351122558117,                  \n",
      " [64/300] At -1.0 dB, Train Loss: 0.007668672129511833 Train BER 0.00024324323749169707\n",
      "Time for one full iteration is 8.9660 minutes\n",
      "encoder learning rate: 4.96e-04, decoder learning rate: 4.96e-04\n",
      "[65/300] At -3.0 dB, Train Loss: 0.20380550622940063 Train BER 0.009751351550221443,                  \n",
      " [65/300] At -1.0 dB, Train Loss: 0.00770146818831563 Train BER 0.0002918918908108026\n",
      "Time for one full iteration is 8.9602 minutes\n",
      "encoder learning rate: 4.96e-04, decoder learning rate: 4.96e-04\n",
      "[66/300] At -3.0 dB, Train Loss: 0.24742497503757477 Train BER 0.012108108028769493,                  \n",
      " [66/300] At -1.0 dB, Train Loss: 0.006685670465230942 Train BER 0.0001945945987245068\n",
      "Time for one full iteration is 8.9891 minutes\n",
      "encoder learning rate: 4.95e-04, decoder learning rate: 4.95e-04\n",
      "[67/300] At -3.0 dB, Train Loss: 0.216788187623024 Train BER 0.010248648934066296,                  \n",
      " [67/300] At -1.0 dB, Train Loss: 0.005388307850807905 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 8.9806 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[68/300] At -3.0 dB, Train Loss: 0.219727024435997 Train BER 0.010335135273635387,                  \n",
      " [68/300] At -1.0 dB, Train Loss: 0.007486883085221052 Train BER 0.00024864866281859577\n",
      "Time for one full iteration is 8.9371 minutes\n",
      "encoder learning rate: 4.94e-04, decoder learning rate: 4.94e-04\n",
      "[69/300] At -3.0 dB, Train Loss: 0.2065555602312088 Train BER 0.010140540078282356,                  \n",
      " [69/300] At -1.0 dB, Train Loss: 0.0060865385457873344 Train BER 0.00024324323749169707\n",
      "Time for one full iteration is 9.0036 minutes\n",
      "encoder learning rate: 4.93e-04, decoder learning rate: 4.93e-04\n",
      "[70/300] At -3.0 dB, Train Loss: 0.20960700511932373 Train BER 0.00985405407845974,                  \n",
      " [70/300] At -1.0 dB, Train Loss: 0.007813089527189732 Train BER 0.00028648649458773434\n",
      "Time for one full iteration is 9.0191 minutes\n",
      "encoder learning rate: 4.92e-04, decoder learning rate: 4.92e-04\n",
      "[71/300] At -3.0 dB, Train Loss: 0.20538349449634552 Train BER 0.009621622040867805,                  \n",
      " [71/300] At -1.0 dB, Train Loss: 0.009719357825815678 Train BER 0.00036756755434907973\n",
      "Time for one full iteration is 9.2386 minutes\n",
      "encoder learning rate: 4.91e-04, decoder learning rate: 4.91e-04\n",
      "[72/300] At -3.0 dB, Train Loss: 0.20219686627388 Train BER 0.009999999776482582,                  \n",
      " [72/300] At -1.0 dB, Train Loss: 0.0074080475606024265 Train BER 0.000254054059041664\n",
      "Time for one full iteration is 9.1566 minutes\n",
      "encoder learning rate: 4.91e-04, decoder learning rate: 4.91e-04\n",
      "[73/300] At -3.0 dB, Train Loss: 0.22286096215248108 Train BER 0.010940540581941605,                  \n",
      " [73/300] At -1.0 dB, Train Loss: 0.0031266272999346256 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 9.1668 minutes\n",
      "encoder learning rate: 4.90e-04, decoder learning rate: 4.90e-04\n",
      "[74/300] At -3.0 dB, Train Loss: 0.2188979983329773 Train BER 0.011264865286648273,                  \n",
      " [74/300] At -1.0 dB, Train Loss: 0.008371766656637192 Train BER 0.00024324323749169707\n",
      "Time for one full iteration is 9.1941 minutes\n",
      "encoder learning rate: 4.89e-04, decoder learning rate: 4.89e-04\n",
      "[75/300] At -3.0 dB, Train Loss: 0.18734629452228546 Train BER 0.00875135138630867,                  \n",
      " [75/300] At -1.0 dB, Train Loss: 0.0027926063630729914 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 9.2482 minutes\n",
      "encoder learning rate: 4.88e-04, decoder learning rate: 4.88e-04\n",
      "[76/300] At -3.0 dB, Train Loss: 0.2095368504524231 Train BER 0.009724324569106102,                  \n",
      " [76/300] At -1.0 dB, Train Loss: 0.007893729023635387 Train BER 0.00028648649458773434\n",
      "Time for one full iteration is 9.2012 minutes\n",
      "encoder learning rate: 4.87e-04, decoder learning rate: 4.87e-04\n",
      "[77/300] At -3.0 dB, Train Loss: 0.19240215420722961 Train BER 0.009275675751268864,                  \n",
      " [77/300] At -1.0 dB, Train Loss: 0.003998406231403351 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 9.1224 minutes\n",
      "encoder learning rate: 4.86e-04, decoder learning rate: 4.86e-04\n",
      "[78/300] At -3.0 dB, Train Loss: 0.20504923164844513 Train BER 0.01007026992738247,                  \n",
      " [78/300] At -1.0 dB, Train Loss: 0.015445621684193611 Train BER 0.0005837837816216052\n",
      "Time for one full iteration is 9.1848 minutes\n",
      "encoder learning rate: 4.85e-04, decoder learning rate: 4.85e-04\n",
      "[79/300] At -3.0 dB, Train Loss: 0.21244162321090698 Train BER 0.010410810820758343,                  \n",
      " [79/300] At -1.0 dB, Train Loss: 0.0032812224235385656 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 9.0758 minutes\n",
      "encoder learning rate: 4.84e-04, decoder learning rate: 4.84e-04\n",
      "[80/300] At -3.0 dB, Train Loss: 0.22119306027889252 Train BER 0.010762162506580353,                  \n",
      " [80/300] At -1.0 dB, Train Loss: 0.007708461955189705 Train BER 0.00024324323749169707\n",
      "Time for one full iteration is 9.0197 minutes\n",
      "encoder learning rate: 4.82e-04, decoder learning rate: 4.82e-04\n",
      "[81/300] At -3.0 dB, Train Loss: 0.1932504028081894 Train BER 0.009324324317276478,                  \n",
      " [81/300] At -1.0 dB, Train Loss: 0.003665383905172348 Train BER 0.00011351351713528857\n",
      "Time for one full iteration is 9.2398 minutes\n",
      "encoder learning rate: 4.81e-04, decoder learning rate: 4.81e-04\n",
      "[82/300] At -3.0 dB, Train Loss: 0.20967575907707214 Train BER 0.010524324141442776,                  \n",
      " [82/300] At -1.0 dB, Train Loss: 0.007558626122772694 Train BER 0.0002918918908108026\n",
      "Time for one full iteration is 9.2078 minutes\n",
      "encoder learning rate: 4.80e-04, decoder learning rate: 4.80e-04\n",
      "[83/300] At -3.0 dB, Train Loss: 0.2108893245458603 Train BER 0.010394594632089138,                  \n",
      " [83/300] At -1.0 dB, Train Loss: 0.003264748491346836 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 9.0633 minutes\n",
      "encoder learning rate: 4.79e-04, decoder learning rate: 4.79e-04\n",
      "[84/300] At -3.0 dB, Train Loss: 0.19653986394405365 Train BER 0.009832432493567467,                  \n",
      " [84/300] At -1.0 dB, Train Loss: 0.007363935932517052 Train BER 0.00025945945526473224\n",
      "Time for one full iteration is 9.0457 minutes\n",
      "encoder learning rate: 4.78e-04, decoder learning rate: 4.78e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/300] At -3.0 dB, Train Loss: 0.2000369429588318 Train BER 0.009432432241737843,                  \n",
      " [85/300] At -1.0 dB, Train Loss: 0.0032252809032797813 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 8.9195 minutes\n",
      "encoder learning rate: 4.76e-04, decoder learning rate: 4.76e-04\n",
      "[86/300] At -3.0 dB, Train Loss: 0.2296908050775528 Train BER 0.011783783324062824,                  \n",
      " [86/300] At -1.0 dB, Train Loss: 0.004544233903288841 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 8.7865 minutes\n",
      "encoder learning rate: 4.75e-04, decoder learning rate: 4.75e-04\n",
      "[87/300] At -3.0 dB, Train Loss: 0.16122400760650635 Train BER 0.008081081323325634,                  \n",
      " [87/300] At -1.0 dB, Train Loss: 0.0032540273386985064 Train BER 0.00011351351713528857\n",
      "Time for one full iteration is 8.9603 minutes\n",
      "encoder learning rate: 4.74e-04, decoder learning rate: 4.74e-04\n",
      "[88/300] At -3.0 dB, Train Loss: 0.20363929867744446 Train BER 0.010281081311404705,                  \n",
      " [88/300] At -1.0 dB, Train Loss: 0.0036753814201802015 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.7870 minutes\n",
      "encoder learning rate: 4.72e-04, decoder learning rate: 4.72e-04\n",
      "[89/300] At -3.0 dB, Train Loss: 0.19507412612438202 Train BER 0.009881081059575081,                  \n",
      " [89/300] At -1.0 dB, Train Loss: 0.0022411926183849573 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.8032 minutes\n",
      "encoder learning rate: 4.71e-04, decoder learning rate: 4.71e-04\n",
      "[90/300] At -3.0 dB, Train Loss: 0.2058718055486679 Train BER 0.010275675915181637,                  \n",
      " [90/300] At -1.0 dB, Train Loss: 0.006341823376715183 Train BER 0.0002162162127206102\n",
      "Time for one full iteration is 8.8990 minutes\n",
      "encoder learning rate: 4.69e-04, decoder learning rate: 4.69e-04\n",
      "[91/300] At -3.0 dB, Train Loss: 0.1723501831293106 Train BER 0.00878378376364708,                  \n",
      " [91/300] At -1.0 dB, Train Loss: 0.002372768707573414 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.9476 minutes\n",
      "encoder learning rate: 4.68e-04, decoder learning rate: 4.68e-04\n",
      "[92/300] At -3.0 dB, Train Loss: 0.1857687383890152 Train BER 0.00931891892105341,                  \n",
      " [92/300] At -1.0 dB, Train Loss: 0.0039033526554703712 Train BER 0.00011351351713528857\n",
      "Time for one full iteration is 8.8828 minutes\n",
      "encoder learning rate: 4.66e-04, decoder learning rate: 4.66e-04\n",
      "[93/300] At -3.0 dB, Train Loss: 0.18097221851348877 Train BER 0.009108108468353748,                  \n",
      " [93/300] At -1.0 dB, Train Loss: 0.0036154722329229116 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 8.7736 minutes\n",
      "encoder learning rate: 4.64e-04, decoder learning rate: 4.64e-04\n",
      "[94/300] At -3.0 dB, Train Loss: 0.17567411065101624 Train BER 0.008756756782531738,                  \n",
      " [94/300] At -1.0 dB, Train Loss: 0.0050100707449018955 Train BER 0.00018918918794952333\n",
      "Time for one full iteration is 8.7731 minutes\n",
      "encoder learning rate: 4.63e-04, decoder learning rate: 4.63e-04\n",
      "[95/300] At -3.0 dB, Train Loss: 0.1819462776184082 Train BER 0.008664865046739578,                  \n",
      " [95/300] At -1.0 dB, Train Loss: 0.002769956598058343 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.8447 minutes\n",
      "encoder learning rate: 4.61e-04, decoder learning rate: 4.61e-04\n",
      "[96/300] At -3.0 dB, Train Loss: 0.19236592948436737 Train BER 0.010048648342490196,                  \n",
      " [96/300] At -1.0 dB, Train Loss: 0.0046011037193238735 Train BER 0.000156756752403453\n",
      "Time for one full iteration is 8.7819 minutes\n",
      "encoder learning rate: 4.59e-04, decoder learning rate: 4.59e-04\n",
      "[97/300] At -3.0 dB, Train Loss: 0.19955913722515106 Train BER 0.009967567399144173,                  \n",
      " [97/300] At -1.0 dB, Train Loss: 0.003392366925254464 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 8.7684 minutes\n",
      "encoder learning rate: 4.58e-04, decoder learning rate: 4.58e-04\n",
      "[98/300] At -3.0 dB, Train Loss: 0.18583685159683228 Train BER 0.009767567738890648,                  \n",
      " [98/300] At -1.0 dB, Train Loss: 0.005775809288024902 Train BER 0.00024864866281859577\n",
      "Time for one full iteration is 8.8388 minutes\n",
      "encoder learning rate: 4.56e-04, decoder learning rate: 4.56e-04\n",
      "[99/300] At -3.0 dB, Train Loss: 0.17528335750102997 Train BER 0.008794594556093216,                  \n",
      " [99/300] At -1.0 dB, Train Loss: 0.0023945942521095276 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.7973 minutes\n",
      "encoder learning rate: 4.54e-04, decoder learning rate: 4.54e-04\n",
      "[100/300] At -3.0 dB, Train Loss: 0.19310307502746582 Train BER 0.009827027097344398,                  \n",
      " [100/300] At -1.0 dB, Train Loss: 0.004194870125502348 Train BER 0.00014054053463041782\n",
      "Time for one full iteration is 8.7372 minutes\n",
      "encoder learning rate: 4.52e-04, decoder learning rate: 4.52e-04\n",
      "[101/300] At -3.0 dB, Train Loss: 0.17124484479427338 Train BER 0.008664865046739578,                  \n",
      " [101/300] At -1.0 dB, Train Loss: 0.0049415575340390205 Train BER 0.00016216216317843646\n",
      "Time for one full iteration is 8.8649 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[102/300] At -3.0 dB, Train Loss: 0.18167829513549805 Train BER 0.008972972631454468,                  \n",
      " [102/300] At -1.0 dB, Train Loss: 0.005620606709271669 Train BER 0.0002054054057225585\n",
      "Time for one full iteration is 8.7430 minutes\n",
      "encoder learning rate: 4.49e-04, decoder learning rate: 4.49e-04\n",
      "[103/300] At -3.0 dB, Train Loss: 0.16349105536937714 Train BER 0.00783243216574192,                  \n",
      " [103/300] At -1.0 dB, Train Loss: 0.00538213923573494 Train BER 0.00023783784126862884\n",
      "Time for one full iteration is 8.8115 minutes\n",
      "encoder learning rate: 4.47e-04, decoder learning rate: 4.47e-04\n",
      "[104/300] At -3.0 dB, Train Loss: 0.16824766993522644 Train BER 0.008394594304263592,                  \n",
      " [104/300] At -1.0 dB, Train Loss: 0.002898719161748886 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.7573 minutes\n",
      "encoder learning rate: 4.45e-04, decoder learning rate: 4.45e-04\n",
      "[105/300] At -3.0 dB, Train Loss: 0.18090999126434326 Train BER 0.008956756442785263,                  \n",
      " [105/300] At -1.0 dB, Train Loss: 0.0032497651409357786 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.8300 minutes\n",
      "encoder learning rate: 4.43e-04, decoder learning rate: 4.43e-04\n",
      "[106/300] At -3.0 dB, Train Loss: 0.18925410509109497 Train BER 0.009783783927559853,                  \n",
      " [106/300] At -1.0 dB, Train Loss: 0.003679220797494054 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.7292 minutes\n",
      "encoder learning rate: 4.41e-04, decoder learning rate: 4.41e-04\n",
      "[107/300] At -3.0 dB, Train Loss: 0.1667436957359314 Train BER 0.008259459398686886,                  \n",
      " [107/300] At -1.0 dB, Train Loss: 0.0033943066373467445 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 8.7475 minutes\n",
      "encoder learning rate: 4.39e-04, decoder learning rate: 4.39e-04\n",
      "[108/300] At -3.0 dB, Train Loss: 0.19778136909008026 Train BER 0.009989188984036446,                  \n",
      " [108/300] At -1.0 dB, Train Loss: 0.0044632540084421635 Train BER 0.00016216216317843646\n",
      "Time for one full iteration is 8.7319 minutes\n",
      "encoder learning rate: 4.37e-04, decoder learning rate: 4.37e-04\n",
      "[109/300] At -3.0 dB, Train Loss: 0.18719296157360077 Train BER 0.009572972543537617,                  \n",
      " [109/300] At -1.0 dB, Train Loss: 0.004098534118384123 Train BER 0.0001945945987245068\n",
      "Time for one full iteration is 8.7585 minutes\n",
      "encoder learning rate: 4.35e-04, decoder learning rate: 4.35e-04\n",
      "[110/300] At -3.0 dB, Train Loss: 0.1991806924343109 Train BER 0.010437837801873684,                  \n",
      " [110/300] At -1.0 dB, Train Loss: 0.0030329935252666473 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.6544 minutes\n",
      "encoder learning rate: 4.32e-04, decoder learning rate: 4.32e-04\n",
      "[111/300] At -3.0 dB, Train Loss: 0.18490417301654816 Train BER 0.009156757034361362,                  \n",
      " [111/300] At -1.0 dB, Train Loss: 0.0020420660730451345 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.8040 minutes\n",
      "encoder learning rate: 4.30e-04, decoder learning rate: 4.30e-04\n",
      "[112/300] At -3.0 dB, Train Loss: 0.18421004712581635 Train BER 0.009156757034361362,                  \n",
      " [112/300] At -1.0 dB, Train Loss: 0.005254686344414949 Train BER 0.00016756757395341992\n",
      "Time for one full iteration is 8.8044 minutes\n",
      "encoder learning rate: 4.28e-04, decoder learning rate: 4.28e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/300] At -3.0 dB, Train Loss: 0.15997172892093658 Train BER 0.007486486341804266,                  \n",
      " [113/300] At -1.0 dB, Train Loss: 0.00409298250451684 Train BER 0.00016756757395341992\n",
      "Time for one full iteration is 8.7757 minutes\n",
      "encoder learning rate: 4.26e-04, decoder learning rate: 4.26e-04\n",
      "[114/300] At -3.0 dB, Train Loss: 0.2104858160018921 Train BER 0.010897297412157059,                  \n",
      " [114/300] At -1.0 dB, Train Loss: 0.004103465937077999 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 8.8008 minutes\n",
      "encoder learning rate: 4.24e-04, decoder learning rate: 4.24e-04\n",
      "[115/300] At -3.0 dB, Train Loss: 0.1821819692850113 Train BER 0.009021621197462082,                  \n",
      " [115/300] At -1.0 dB, Train Loss: 0.0031181529629975557 Train BER 0.00011351351713528857\n",
      "Time for one full iteration is 8.6587 minutes\n",
      "encoder learning rate: 4.21e-04, decoder learning rate: 4.21e-04\n",
      "[116/300] At -3.0 dB, Train Loss: 0.17509308457374573 Train BER 0.008745945990085602,                  \n",
      " [116/300] At -1.0 dB, Train Loss: 0.003265524283051491 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.4925 minutes\n",
      "encoder learning rate: 4.19e-04, decoder learning rate: 4.19e-04\n",
      "[117/300] At -3.0 dB, Train Loss: 0.1605197638273239 Train BER 0.008086486719548702,                  \n",
      " [117/300] At -1.0 dB, Train Loss: 0.00338297663256526 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 8.7026 minutes\n",
      "encoder learning rate: 4.17e-04, decoder learning rate: 4.17e-04\n",
      "[118/300] At -3.0 dB, Train Loss: 0.17970025539398193 Train BER 0.009200000204145908,                  \n",
      " [118/300] At -1.0 dB, Train Loss: 0.0034707405138760805 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 8.6938 minutes\n",
      "encoder learning rate: 4.14e-04, decoder learning rate: 4.14e-04\n",
      "[119/300] At -3.0 dB, Train Loss: 0.1731688678264618 Train BER 0.008281080983579159,                  \n",
      " [119/300] At -1.0 dB, Train Loss: 0.0028014371637254953 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.7262 minutes\n",
      "encoder learning rate: 4.12e-04, decoder learning rate: 4.12e-04\n",
      "[120/300] At -3.0 dB, Train Loss: 0.18528583645820618 Train BER 0.009118919260799885,                  \n",
      " [120/300] At -1.0 dB, Train Loss: 0.0033039283007383347 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 8.6953 minutes\n",
      "encoder learning rate: 4.10e-04, decoder learning rate: 4.10e-04\n",
      "[121/300] At -3.0 dB, Train Loss: 0.17236998677253723 Train BER 0.008513513952493668,                  \n",
      " [121/300] At -1.0 dB, Train Loss: 0.0027386785950511694 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 8.7630 minutes\n",
      "encoder learning rate: 4.07e-04, decoder learning rate: 4.07e-04\n",
      "[122/300] At -3.0 dB, Train Loss: 0.19733527302742004 Train BER 0.010027026757597923,                  \n",
      " [122/300] At -1.0 dB, Train Loss: 0.0048902942799031734 Train BER 0.00017297297017648816\n",
      "Time for one full iteration is 8.7908 minutes\n",
      "encoder learning rate: 4.05e-04, decoder learning rate: 4.05e-04\n",
      "[123/300] At -3.0 dB, Train Loss: 0.189814031124115 Train BER 0.009535134769976139,                  \n",
      " [123/300] At -1.0 dB, Train Loss: 0.003836322808638215 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 8.7485 minutes\n",
      "encoder learning rate: 4.02e-04, decoder learning rate: 4.02e-04\n",
      "[124/300] At -3.0 dB, Train Loss: 0.2006618082523346 Train BER 0.010518918745219707,                  \n",
      " [124/300] At -1.0 dB, Train Loss: 0.005063636228442192 Train BER 0.00019999999494757503\n",
      "Time for one full iteration is 8.5880 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[125/300] At -3.0 dB, Train Loss: 0.1697918176651001 Train BER 0.008113513700664043,                  \n",
      " [125/300] At -1.0 dB, Train Loss: 0.0020384315866976976 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.6578 minutes\n",
      "encoder learning rate: 3.97e-04, decoder learning rate: 3.97e-04\n",
      "[126/300] At -3.0 dB, Train Loss: 0.18608810007572174 Train BER 0.009286486543715,                  \n",
      " [126/300] At -1.0 dB, Train Loss: 0.004316543694585562 Train BER 0.0001459459454054013\n",
      "Time for one full iteration is 8.7583 minutes\n",
      "encoder learning rate: 3.95e-04, decoder learning rate: 3.95e-04\n",
      "[127/300] At -3.0 dB, Train Loss: 0.17236179113388062 Train BER 0.008010811172425747,                  \n",
      " [127/300] At -1.0 dB, Train Loss: 0.007553001865744591 Train BER 0.00030810810858383775\n",
      "Time for one full iteration is 8.7785 minutes\n",
      "encoder learning rate: 3.92e-04, decoder learning rate: 3.92e-04\n",
      "[128/300] At -3.0 dB, Train Loss: 0.18053795397281647 Train BER 0.009286486543715,                  \n",
      " [128/300] At -1.0 dB, Train Loss: 0.005201701074838638 Train BER 0.00017837838095147163\n",
      "Time for one full iteration is 8.7708 minutes\n",
      "encoder learning rate: 3.89e-04, decoder learning rate: 3.89e-04\n",
      "[129/300] At -3.0 dB, Train Loss: 0.17261557281017303 Train BER 0.008735135197639465,                  \n",
      " [129/300] At -1.0 dB, Train Loss: 0.0027543813921511173 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.7453 minutes\n",
      "encoder learning rate: 3.87e-04, decoder learning rate: 3.87e-04\n",
      "[130/300] At -3.0 dB, Train Loss: 0.1707300841808319 Train BER 0.008572973310947418,                  \n",
      " [130/300] At -1.0 dB, Train Loss: 0.0040621827356517315 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 8.7302 minutes\n",
      "encoder learning rate: 3.84e-04, decoder learning rate: 3.84e-04\n",
      "[131/300] At -3.0 dB, Train Loss: 0.16608278453350067 Train BER 0.008227027021348476,                  \n",
      " [131/300] At -1.0 dB, Train Loss: 0.0020860524382442236 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.7836 minutes\n",
      "encoder learning rate: 3.82e-04, decoder learning rate: 3.82e-04\n",
      "[132/300] At -3.0 dB, Train Loss: 0.17622825503349304 Train BER 0.009037837386131287,                  \n",
      " [132/300] At -1.0 dB, Train Loss: 0.003889766987413168 Train BER 0.0001945945987245068\n",
      "Time for one full iteration is 8.7838 minutes\n",
      "encoder learning rate: 3.79e-04, decoder learning rate: 3.79e-04\n",
      "[133/300] At -3.0 dB, Train Loss: 0.16119031608104706 Train BER 0.008113513700664043,                  \n",
      " [133/300] At -1.0 dB, Train Loss: 0.0031406318303197622 Train BER 0.00010270270286127925\n",
      "Time for one full iteration is 8.7383 minutes\n",
      "encoder learning rate: 3.76e-04, decoder learning rate: 3.76e-04\n",
      "[134/300] At -3.0 dB, Train Loss: 0.17134249210357666 Train BER 0.00856756791472435,                  \n",
      " [134/300] At -1.0 dB, Train Loss: 0.0024047289043664932 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.5612 minutes\n",
      "encoder learning rate: 3.73e-04, decoder learning rate: 3.73e-04\n",
      "[135/300] At -3.0 dB, Train Loss: 0.14412447810173035 Train BER 0.00702162180095911,                  \n",
      " [135/300] At -1.0 dB, Train Loss: 0.003923150710761547 Train BER 0.00012432433140929788\n",
      "Time for one full iteration is 8.7394 minutes\n",
      "encoder learning rate: 3.71e-04, decoder learning rate: 3.71e-04\n",
      "[136/300] At -3.0 dB, Train Loss: 0.14846362173557281 Train BER 0.007302702870219946,                  \n",
      " [136/300] At -1.0 dB, Train Loss: 0.001897956244647503 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.7550 minutes\n",
      "encoder learning rate: 3.68e-04, decoder learning rate: 3.68e-04\n",
      "[137/300] At -3.0 dB, Train Loss: 0.16447214782238007 Train BER 0.008081081323325634,                  \n",
      " [137/300] At -1.0 dB, Train Loss: 0.0035367056261748075 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 8.6212 minutes\n",
      "encoder learning rate: 3.65e-04, decoder learning rate: 3.65e-04\n",
      "[138/300] At -3.0 dB, Train Loss: 0.16204532980918884 Train BER 0.0083081079646945,                  \n",
      " [138/300] At -1.0 dB, Train Loss: 0.005922283977270126 Train BER 0.0002648648514878005\n",
      "Time for one full iteration is 8.5725 minutes\n",
      "encoder learning rate: 3.62e-04, decoder learning rate: 3.62e-04\n",
      "[139/300] At -3.0 dB, Train Loss: 0.15834838151931763 Train BER 0.008227027021348476,                  \n",
      " [139/300] At -1.0 dB, Train Loss: 0.004837476182729006 Train BER 0.0001945945987245068\n",
      "Time for one full iteration is 8.5641 minutes\n",
      "encoder learning rate: 3.60e-04, decoder learning rate: 3.60e-04\n",
      "[140/300] At -3.0 dB, Train Loss: 0.18677940964698792 Train BER 0.009724324569106102,                  \n",
      " [140/300] At -1.0 dB, Train Loss: 0.0042210789397358894 Train BER 0.00011351351713528857\n",
      "Time for one full iteration is 8.5277 minutes\n",
      "encoder learning rate: 3.57e-04, decoder learning rate: 3.57e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/300] At -3.0 dB, Train Loss: 0.15677185356616974 Train BER 0.007978378795087337,                  \n",
      " [141/300] At -1.0 dB, Train Loss: 0.004388382192701101 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 8.5652 minutes\n",
      "encoder learning rate: 3.54e-04, decoder learning rate: 3.54e-04\n",
      "[142/300] At -3.0 dB, Train Loss: 0.16309547424316406 Train BER 0.007935134693980217,                  \n",
      " [142/300] At -1.0 dB, Train Loss: 0.0034198141656816006 Train BER 0.00011351351713528857\n",
      "Time for one full iteration is 8.9467 minutes\n",
      "encoder learning rate: 3.51e-04, decoder learning rate: 3.51e-04\n",
      "[143/300] At -3.0 dB, Train Loss: 0.1639174371957779 Train BER 0.00834054034203291,                  \n",
      " [143/300] At -1.0 dB, Train Loss: 0.002078091958537698 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 9.0220 minutes\n",
      "encoder learning rate: 3.48e-04, decoder learning rate: 3.48e-04\n",
      "[144/300] At -3.0 dB, Train Loss: 0.17635653913021088 Train BER 0.009383783675730228,                  \n",
      " [144/300] At -1.0 dB, Train Loss: 0.004817848559468985 Train BER 0.00019999999494757503\n",
      "Time for one full iteration is 8.9080 minutes\n",
      "encoder learning rate: 3.45e-04, decoder learning rate: 3.45e-04\n",
      "[145/300] At -3.0 dB, Train Loss: 0.16049207746982574 Train BER 0.008070270530879498,                  \n",
      " [145/300] At -1.0 dB, Train Loss: 0.00383188109844923 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 8.8586 minutes\n",
      "encoder learning rate: 3.42e-04, decoder learning rate: 3.42e-04\n",
      "[146/300] At -3.0 dB, Train Loss: 0.1701757311820984 Train BER 0.008529730141162872,                  \n",
      " [146/300] At -1.0 dB, Train Loss: 0.0033158923033624887 Train BER 0.00012432433140929788\n",
      "Time for one full iteration is 8.8080 minutes\n",
      "encoder learning rate: 3.39e-04, decoder learning rate: 3.39e-04\n",
      "[147/300] At -3.0 dB, Train Loss: 0.13858027756214142 Train BER 0.006924324203282595,                  \n",
      " [147/300] At -1.0 dB, Train Loss: 0.0021976118441671133 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.8652 minutes\n",
      "encoder learning rate: 3.36e-04, decoder learning rate: 3.36e-04\n",
      "[148/300] At -3.0 dB, Train Loss: 0.16588415205478668 Train BER 0.008410810492932796,                  \n",
      " [148/300] At -1.0 dB, Train Loss: 0.0031070911791175604 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 8.9997 minutes\n",
      "encoder learning rate: 3.34e-04, decoder learning rate: 3.34e-04\n",
      "[149/300] At -3.0 dB, Train Loss: 0.16489183902740479 Train BER 0.008016216568648815,                  \n",
      " [149/300] At -1.0 dB, Train Loss: 0.002065080450847745 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8419 minutes\n",
      "encoder learning rate: 3.31e-04, decoder learning rate: 3.31e-04\n",
      "[150/300] At -3.0 dB, Train Loss: 0.20472189784049988 Train BER 0.01044864859431982,                  \n",
      " [150/300] At -1.0 dB, Train Loss: 0.0032273957040160894 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 8.8735 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[151/300] At -3.0 dB, Train Loss: 0.15703663229942322 Train BER 0.007902702316641808,                  \n",
      " [151/300] At -1.0 dB, Train Loss: 0.002010891679674387 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.8615 minutes\n",
      "encoder learning rate: 3.25e-04, decoder learning rate: 3.25e-04\n",
      "[152/300] At -3.0 dB, Train Loss: 0.14092865586280823 Train BER 0.006800000090152025,                  \n",
      " [152/300] At -1.0 dB, Train Loss: 0.0017800648929551244 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.9326 minutes\n",
      "encoder learning rate: 3.22e-04, decoder learning rate: 3.22e-04\n",
      "[153/300] At -3.0 dB, Train Loss: 0.13839656114578247 Train BER 0.006416216026991606,                  \n",
      " [153/300] At -1.0 dB, Train Loss: 0.001247839885763824 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9463 minutes\n",
      "encoder learning rate: 3.19e-04, decoder learning rate: 3.19e-04\n",
      "[154/300] At -3.0 dB, Train Loss: 0.148772731423378 Train BER 0.007340540643781424,                  \n",
      " [154/300] At -1.0 dB, Train Loss: 0.0028499916661530733 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 8.9981 minutes\n",
      "encoder learning rate: 3.16e-04, decoder learning rate: 3.16e-04\n",
      "[155/300] At -3.0 dB, Train Loss: 0.14852578938007355 Train BER 0.007102702744305134,                  \n",
      " [155/300] At -1.0 dB, Train Loss: 0.0013496665051206946 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.8702 minutes\n",
      "encoder learning rate: 3.13e-04, decoder learning rate: 3.13e-04\n",
      "[156/300] At -3.0 dB, Train Loss: 0.17148976027965546 Train BER 0.008551351726055145,                  \n",
      " [156/300] At -1.0 dB, Train Loss: 0.004156490787863731 Train BER 0.00024324323749169707\n",
      "Time for one full iteration is 9.0150 minutes\n",
      "encoder learning rate: 3.10e-04, decoder learning rate: 3.10e-04\n",
      "[157/300] At -3.0 dB, Train Loss: 0.14125733077526093 Train BER 0.007059459574520588,                  \n",
      " [157/300] At -1.0 dB, Train Loss: 0.0016676458762958646 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.9233 minutes\n",
      "encoder learning rate: 3.06e-04, decoder learning rate: 3.06e-04\n",
      "[158/300] At -3.0 dB, Train Loss: 0.18520937860012054 Train BER 0.009713513776659966,                  \n",
      " [158/300] At -1.0 dB, Train Loss: 0.004052743781358004 Train BER 0.00014054053463041782\n",
      "Time for one full iteration is 8.8876 minutes\n",
      "encoder learning rate: 3.03e-04, decoder learning rate: 3.03e-04\n",
      "[159/300] At -3.0 dB, Train Loss: 0.1459050178527832 Train BER 0.0070486487820744514,                  \n",
      " [159/300] At -1.0 dB, Train Loss: 0.001988379517570138 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.9556 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[160/300] At -3.0 dB, Train Loss: 0.16211694478988647 Train BER 0.00827567558735609,                  \n",
      " [160/300] At -1.0 dB, Train Loss: 0.003211572766304016 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 9.0618 minutes\n",
      "encoder learning rate: 2.97e-04, decoder learning rate: 2.97e-04\n",
      "[161/300] At -3.0 dB, Train Loss: 0.1446075588464737 Train BER 0.006994594819843769,                  \n",
      " [161/300] At -1.0 dB, Train Loss: 0.0014875751221552491 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8259 minutes\n",
      "encoder learning rate: 2.94e-04, decoder learning rate: 2.94e-04\n",
      "[162/300] At -3.0 dB, Train Loss: 0.15745151042938232 Train BER 0.008118919096887112,                  \n",
      " [162/300] At -1.0 dB, Train Loss: 0.0035499369259923697 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.7558 minutes\n",
      "encoder learning rate: 2.91e-04, decoder learning rate: 2.91e-04\n",
      "[163/300] At -3.0 dB, Train Loss: 0.14749619364738464 Train BER 0.007508107926696539,                  \n",
      " [163/300] At -1.0 dB, Train Loss: 0.001726449467241764 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.9356 minutes\n",
      "encoder learning rate: 2.88e-04, decoder learning rate: 2.88e-04\n",
      "[164/300] At -3.0 dB, Train Loss: 0.1552160680294037 Train BER 0.00777297280728817,                  \n",
      " [164/300] At -1.0 dB, Train Loss: 0.0019437564769759774 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.9694 minutes\n",
      "encoder learning rate: 2.85e-04, decoder learning rate: 2.85e-04\n",
      "[165/300] At -3.0 dB, Train Loss: 0.1452447921037674 Train BER 0.0071189189329743385,                  \n",
      " [165/300] At -1.0 dB, Train Loss: 0.001229889690876007 Train BER 0.0\n",
      "Time for one full iteration is 8.8665 minutes\n",
      "encoder learning rate: 2.82e-04, decoder learning rate: 2.82e-04\n",
      "[166/300] At -3.0 dB, Train Loss: 0.16353434324264526 Train BER 0.007945945486426353,                  \n",
      " [166/300] At -1.0 dB, Train Loss: 0.0026150124613195658 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.9692 minutes\n",
      "encoder learning rate: 2.79e-04, decoder learning rate: 2.79e-04\n",
      "[167/300] At -3.0 dB, Train Loss: 0.14414510130882263 Train BER 0.007124324329197407,                  \n",
      " [167/300] At -1.0 dB, Train Loss: 0.0018369471654295921 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.9323 minutes\n",
      "encoder learning rate: 2.76e-04, decoder learning rate: 2.76e-04\n",
      "[168/300] At -3.0 dB, Train Loss: 0.14070458710193634 Train BER 0.007129729725420475,                  \n",
      " [168/300] At -1.0 dB, Train Loss: 0.002370915375649929 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.9470 minutes\n",
      "encoder learning rate: 2.72e-04, decoder learning rate: 2.72e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169/300] At -3.0 dB, Train Loss: 0.16256582736968994 Train BER 0.008486486040055752,                  \n",
      " [169/300] At -1.0 dB, Train Loss: 0.0012479083379730582 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.8839 minutes\n",
      "encoder learning rate: 2.69e-04, decoder learning rate: 2.69e-04\n",
      "[170/300] At -3.0 dB, Train Loss: 0.15317948162555695 Train BER 0.007654054090380669,                  \n",
      " [170/300] At -1.0 dB, Train Loss: 0.002740798518061638 Train BER 0.000156756752403453\n",
      "Time for one full iteration is 8.9006 minutes\n",
      "encoder learning rate: 2.66e-04, decoder learning rate: 2.66e-04\n",
      "[171/300] At -3.0 dB, Train Loss: 0.1388884335756302 Train BER 0.006718919146806002,                  \n",
      " [171/300] At -1.0 dB, Train Loss: 0.002687714295461774 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 8.9335 minutes\n",
      "encoder learning rate: 2.63e-04, decoder learning rate: 2.63e-04\n",
      "[172/300] At -3.0 dB, Train Loss: 0.1537340134382248 Train BER 0.007513513322919607,                  \n",
      " [172/300] At -1.0 dB, Train Loss: 0.0019046670058742166 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.8950 minutes\n",
      "encoder learning rate: 2.60e-04, decoder learning rate: 2.60e-04\n",
      "[173/300] At -3.0 dB, Train Loss: 0.14028076827526093 Train BER 0.007654054090380669,                  \n",
      " [173/300] At -1.0 dB, Train Loss: 0.0019454037537798285 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.8495 minutes\n",
      "encoder learning rate: 2.57e-04, decoder learning rate: 2.57e-04\n",
      "[174/300] At -3.0 dB, Train Loss: 0.15680333971977234 Train BER 0.007859459146857262,                  \n",
      " [174/300] At -1.0 dB, Train Loss: 0.003002828685566783 Train BER 0.00014054053463041782\n",
      "Time for one full iteration is 8.9360 minutes\n",
      "encoder learning rate: 2.54e-04, decoder learning rate: 2.54e-04\n",
      "[175/300] At -3.0 dB, Train Loss: 0.15740734338760376 Train BER 0.008281080983579159,                  \n",
      " [175/300] At -1.0 dB, Train Loss: 0.001605405705049634 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.9398 minutes\n",
      "encoder learning rate: 2.51e-04, decoder learning rate: 2.51e-04\n",
      "[176/300] At -3.0 dB, Train Loss: 0.157409206032753 Train BER 0.008210810832679272,                  \n",
      " [176/300] At -1.0 dB, Train Loss: 0.0025914697907865047 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 8.8049 minutes\n",
      "encoder learning rate: 2.47e-04, decoder learning rate: 2.47e-04\n",
      "[177/300] At -3.0 dB, Train Loss: 0.16538779437541962 Train BER 0.00871891900897026,                  \n",
      " [177/300] At -1.0 dB, Train Loss: 0.0019839194137603045 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.7790 minutes\n",
      "encoder learning rate: 2.44e-04, decoder learning rate: 2.44e-04\n",
      "[178/300] At -3.0 dB, Train Loss: 0.14739049971103668 Train BER 0.007470270153135061,                  \n",
      " [178/300] At -1.0 dB, Train Loss: 0.0020052031613886356 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.8723 minutes\n",
      "encoder learning rate: 2.41e-04, decoder learning rate: 2.41e-04\n",
      "[179/300] At -3.0 dB, Train Loss: 0.1447887420654297 Train BER 0.007162162102758884,                  \n",
      " [179/300] At -1.0 dB, Train Loss: 0.0019193926127627492 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.9536 minutes\n",
      "encoder learning rate: 2.38e-04, decoder learning rate: 2.38e-04\n",
      "[180/300] At -3.0 dB, Train Loss: 0.15192270278930664 Train BER 0.0076270271092653275,                  \n",
      " [180/300] At -1.0 dB, Train Loss: 0.0025956451427191496 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.9501 minutes\n",
      "encoder learning rate: 2.35e-04, decoder learning rate: 2.35e-04\n",
      "[181/300] At -3.0 dB, Train Loss: 0.14692074060440063 Train BER 0.007124324329197407,                  \n",
      " [181/300] At -1.0 dB, Train Loss: 0.0019584358669817448 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.9258 minutes\n",
      "encoder learning rate: 2.32e-04, decoder learning rate: 2.32e-04\n",
      "[182/300] At -3.0 dB, Train Loss: 0.14600500464439392 Train BER 0.007324324455112219,                  \n",
      " [182/300] At -1.0 dB, Train Loss: 0.0011029335437342525 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0148 minutes\n",
      "encoder learning rate: 2.29e-04, decoder learning rate: 2.29e-04\n",
      "[183/300] At -3.0 dB, Train Loss: 0.13677163422107697 Train BER 0.006718919146806002,                  \n",
      " [183/300] At -1.0 dB, Train Loss: 0.0023719354066997766 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.8157 minutes\n",
      "encoder learning rate: 2.25e-04, decoder learning rate: 2.25e-04\n",
      "[184/300] At -3.0 dB, Train Loss: 0.15753455460071564 Train BER 0.008016216568648815,                  \n",
      " [184/300] At -1.0 dB, Train Loss: 0.0028755369130522013 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.7885 minutes\n",
      "encoder learning rate: 2.22e-04, decoder learning rate: 2.22e-04\n",
      "[185/300] At -3.0 dB, Train Loss: 0.13890831172466278 Train BER 0.006956756580621004,                  \n",
      " [185/300] At -1.0 dB, Train Loss: 0.003416071180254221 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 9.0059 minutes\n",
      "encoder learning rate: 2.19e-04, decoder learning rate: 2.19e-04\n",
      "[186/300] At -3.0 dB, Train Loss: 0.14582082629203796 Train BER 0.00723783764988184,                  \n",
      " [186/300] At -1.0 dB, Train Loss: 0.001742159016430378 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.8985 minutes\n",
      "encoder learning rate: 2.16e-04, decoder learning rate: 2.16e-04\n",
      "[187/300] At -3.0 dB, Train Loss: 0.14011166989803314 Train BER 0.006718919146806002,                  \n",
      " [187/300] At -1.0 dB, Train Loss: 0.002156709088012576 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.7745 minutes\n",
      "encoder learning rate: 2.13e-04, decoder learning rate: 2.13e-04\n",
      "[188/300] At -3.0 dB, Train Loss: 0.14549972116947174 Train BER 0.007313513662666082,                  \n",
      " [188/300] At -1.0 dB, Train Loss: 0.0022650877945125103 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 9.0094 minutes\n",
      "encoder learning rate: 2.10e-04, decoder learning rate: 2.10e-04\n",
      "[189/300] At -3.0 dB, Train Loss: 0.14285096526145935 Train BER 0.007129729725420475,                  \n",
      " [189/300] At -1.0 dB, Train Loss: 0.0015672118170186877 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.9722 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[190/300] At -3.0 dB, Train Loss: 0.15576212108135223 Train BER 0.007956757210195065,                  \n",
      " [190/300] At -1.0 dB, Train Loss: 0.0012508882209658623 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9679 minutes\n",
      "encoder learning rate: 2.04e-04, decoder learning rate: 2.04e-04\n",
      "[191/300] At -3.0 dB, Train Loss: 0.14327116310596466 Train BER 0.007151351310312748,                  \n",
      " [191/300] At -1.0 dB, Train Loss: 0.0016080652130767703 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8596 minutes\n",
      "encoder learning rate: 2.01e-04, decoder learning rate: 2.01e-04\n",
      "[192/300] At -3.0 dB, Train Loss: 0.12630636990070343 Train BER 0.006437837611883879,                  \n",
      " [192/300] At -1.0 dB, Train Loss: 0.002859904896467924 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 8.9294 minutes\n",
      "encoder learning rate: 1.98e-04, decoder learning rate: 1.98e-04\n",
      "[193/300] At -3.0 dB, Train Loss: 0.1455988734960556 Train BER 0.0071405405178666115,                  \n",
      " [193/300] At -1.0 dB, Train Loss: 0.0026167635805904865 Train BER 0.00010270270286127925\n",
      "Time for one full iteration is 8.8765 minutes\n",
      "encoder learning rate: 1.95e-04, decoder learning rate: 1.95e-04\n",
      "[194/300] At -3.0 dB, Train Loss: 0.15191107988357544 Train BER 0.007302702870219946,                  \n",
      " [194/300] At -1.0 dB, Train Loss: 0.00126229552552104 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.8954 minutes\n",
      "encoder learning rate: 1.91e-04, decoder learning rate: 1.91e-04\n",
      "[195/300] At -3.0 dB, Train Loss: 0.13512668013572693 Train BER 0.006789189297705889,                  \n",
      " [195/300] At -1.0 dB, Train Loss: 0.004048862494528294 Train BER 0.00014054053463041782\n",
      "Time for one full iteration is 8.7811 minutes\n",
      "encoder learning rate: 1.88e-04, decoder learning rate: 1.88e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[196/300] At -3.0 dB, Train Loss: 0.13128647208213806 Train BER 0.00651891902089119,                  \n",
      " [196/300] At -1.0 dB, Train Loss: 0.001848324085585773 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.7104 minutes\n",
      "encoder learning rate: 1.85e-04, decoder learning rate: 1.85e-04\n",
      "[197/300] At -3.0 dB, Train Loss: 0.13776732981204987 Train BER 0.006475675851106644,                  \n",
      " [197/300] At -1.0 dB, Train Loss: 0.0008603332098573446 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9080 minutes\n",
      "encoder learning rate: 1.82e-04, decoder learning rate: 1.82e-04\n",
      "[198/300] At -3.0 dB, Train Loss: 0.13665638864040375 Train BER 0.00672432454302907,                  \n",
      " [198/300] At -1.0 dB, Train Loss: 0.001188112422823906 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8427 minutes\n",
      "encoder learning rate: 1.79e-04, decoder learning rate: 1.79e-04\n",
      "[199/300] At -3.0 dB, Train Loss: 0.13704131543636322 Train BER 0.00705405417829752,                  \n",
      " [199/300] At -1.0 dB, Train Loss: 0.001706704031676054 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8281 minutes\n",
      "encoder learning rate: 1.76e-04, decoder learning rate: 1.76e-04\n",
      "[200/300] At -3.0 dB, Train Loss: 0.14361843466758728 Train BER 0.006994594819843769,                  \n",
      " [200/300] At -1.0 dB, Train Loss: 0.002365788910537958 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.9833 minutes\n",
      "encoder learning rate: 1.73e-04, decoder learning rate: 1.73e-04\n",
      "[201/300] At -3.0 dB, Train Loss: 0.14323024451732635 Train BER 0.0072702704928815365,                  \n",
      " [201/300] At -1.0 dB, Train Loss: 0.0017636979464441538 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8136 minutes\n",
      "encoder learning rate: 1.70e-04, decoder learning rate: 1.70e-04\n",
      "[202/300] At -3.0 dB, Train Loss: 0.12443423271179199 Train BER 0.006264864932745695,                  \n",
      " [202/300] At -1.0 dB, Train Loss: 0.003998048137873411 Train BER 0.00017297297017648816\n",
      "Time for one full iteration is 8.8088 minutes\n",
      "encoder learning rate: 1.67e-04, decoder learning rate: 1.67e-04\n",
      "[203/300] At -3.0 dB, Train Loss: 0.15193918347358704 Train BER 0.007940540090203285,                  \n",
      " [203/300] At -1.0 dB, Train Loss: 0.0025637480430305004 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.8761 minutes\n",
      "encoder learning rate: 1.65e-04, decoder learning rate: 1.65e-04\n",
      "[204/300] At -3.0 dB, Train Loss: 0.13464345037937164 Train BER 0.006275675725191832,                  \n",
      " [204/300] At -1.0 dB, Train Loss: 0.0013801640598103404 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.8875 minutes\n",
      "encoder learning rate: 1.62e-04, decoder learning rate: 1.62e-04\n",
      "[205/300] At -3.0 dB, Train Loss: 0.14964868128299713 Train BER 0.007751351222395897,                  \n",
      " [205/300] At -1.0 dB, Train Loss: 0.001042739488184452 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.7922 minutes\n",
      "encoder learning rate: 1.59e-04, decoder learning rate: 1.59e-04\n",
      "[206/300] At -3.0 dB, Train Loss: 0.13705891370773315 Train BER 0.0069189188070595264,                  \n",
      " [206/300] At -1.0 dB, Train Loss: 0.0022176208440214396 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.7539 minutes\n",
      "encoder learning rate: 1.56e-04, decoder learning rate: 1.56e-04\n",
      "[207/300] At -3.0 dB, Train Loss: 0.1342514008283615 Train BER 0.0064486488699913025,                  \n",
      " [207/300] At -1.0 dB, Train Loss: 0.0009590745321474969 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.7826 minutes\n",
      "encoder learning rate: 1.53e-04, decoder learning rate: 1.53e-04\n",
      "[208/300] At -3.0 dB, Train Loss: 0.12825976312160492 Train BER 0.0064324322156608105,                  \n",
      " [208/300] At -1.0 dB, Train Loss: 0.0017908733570948243 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.7639 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[209/300] At -3.0 dB, Train Loss: 0.12762239575386047 Train BER 0.006437837611883879,                  \n",
      " [209/300] At -1.0 dB, Train Loss: 0.000853680248837918 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7324 minutes\n",
      "encoder learning rate: 1.47e-04, decoder learning rate: 1.47e-04\n",
      "[210/300] At -3.0 dB, Train Loss: 0.1433066427707672 Train BER 0.007767567411065102,                  \n",
      " [210/300] At -1.0 dB, Train Loss: 0.002478704322129488 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.7850 minutes\n",
      "encoder learning rate: 1.44e-04, decoder learning rate: 1.44e-04\n",
      "[211/300] At -3.0 dB, Train Loss: 0.14939242601394653 Train BER 0.007843242958188057,                  \n",
      " [211/300] At -1.0 dB, Train Loss: 0.0015024598687887192 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.8146 minutes\n",
      "encoder learning rate: 1.41e-04, decoder learning rate: 1.41e-04\n",
      "[212/300] At -3.0 dB, Train Loss: 0.1295827478170395 Train BER 0.006454054266214371,                  \n",
      " [212/300] At -1.0 dB, Train Loss: 0.0011250338284298778 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8655 minutes\n",
      "encoder learning rate: 1.39e-04, decoder learning rate: 1.39e-04\n",
      "[213/300] At -3.0 dB, Train Loss: 0.13439209759235382 Train BER 0.006902702618390322,                  \n",
      " [213/300] At -1.0 dB, Train Loss: 0.0011526535963639617 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9233 minutes\n",
      "encoder learning rate: 1.36e-04, decoder learning rate: 1.36e-04\n",
      "[214/300] At -3.0 dB, Train Loss: 0.15055902302265167 Train BER 0.007989189587533474,                  \n",
      " [214/300] At -1.0 dB, Train Loss: 0.0013993729371577501 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.7856 minutes\n",
      "encoder learning rate: 1.33e-04, decoder learning rate: 1.33e-04\n",
      "[215/300] At -3.0 dB, Train Loss: 0.1404598206281662 Train BER 0.007043243385851383,                  \n",
      " [215/300] At -1.0 dB, Train Loss: 0.0023213352542370558 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.9757 minutes\n",
      "encoder learning rate: 1.30e-04, decoder learning rate: 1.30e-04\n",
      "[216/300] At -3.0 dB, Train Loss: 0.1409149318933487 Train BER 0.007227026857435703,                  \n",
      " [216/300] At -1.0 dB, Train Loss: 0.0028392162639647722 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.9841 minutes\n",
      "encoder learning rate: 1.28e-04, decoder learning rate: 1.28e-04\n",
      "[217/300] At -3.0 dB, Train Loss: 0.14225618541240692 Train BER 0.007183783687651157,                  \n",
      " [217/300] At -1.0 dB, Train Loss: 0.0012902166927233338 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.9168 minutes\n",
      "encoder learning rate: 1.25e-04, decoder learning rate: 1.25e-04\n",
      "[218/300] At -3.0 dB, Train Loss: 0.1589755415916443 Train BER 0.007794594392180443,                  \n",
      " [218/300] At -1.0 dB, Train Loss: 0.0020339032635092735 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.8935 minutes\n",
      "encoder learning rate: 1.22e-04, decoder learning rate: 1.22e-04\n",
      "[219/300] At -3.0 dB, Train Loss: 0.11704996973276138 Train BER 0.005935135297477245,                  \n",
      " [219/300] At -1.0 dB, Train Loss: 0.00142675603274256 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.8791 minutes\n",
      "encoder learning rate: 1.19e-04, decoder learning rate: 1.19e-04\n",
      "[220/300] At -3.0 dB, Train Loss: 0.13932839035987854 Train BER 0.006827027071267366,                  \n",
      " [220/300] At -1.0 dB, Train Loss: 0.0018143856432288885 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.9337 minutes\n",
      "encoder learning rate: 1.17e-04, decoder learning rate: 1.17e-04\n",
      "[221/300] At -3.0 dB, Train Loss: 0.15037225186824799 Train BER 0.007502702530473471,                  \n",
      " [221/300] At -1.0 dB, Train Loss: 0.001485279412008822 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.8010 minutes\n",
      "encoder learning rate: 1.14e-04, decoder learning rate: 1.14e-04\n",
      "[222/300] At -3.0 dB, Train Loss: 0.13282984495162964 Train BER 0.006491892039775848,                  \n",
      " [222/300] At -1.0 dB, Train Loss: 0.0018660645000636578 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.6848 minutes\n",
      "encoder learning rate: 1.12e-04, decoder learning rate: 1.12e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[223/300] At -3.0 dB, Train Loss: 0.13415580987930298 Train BER 0.00681621627882123,                  \n",
      " [223/300] At -1.0 dB, Train Loss: 0.003050653962418437 Train BER 0.00012432433140929788\n",
      "Time for one full iteration is 8.7514 minutes\n",
      "encoder learning rate: 1.09e-04, decoder learning rate: 1.09e-04\n",
      "[224/300] At -3.0 dB, Train Loss: 0.135183185338974 Train BER 0.007064864970743656,                  \n",
      " [224/300] At -1.0 dB, Train Loss: 0.0013672569766640663 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.7696 minutes\n",
      "encoder learning rate: 1.06e-04, decoder learning rate: 1.06e-04\n",
      "[225/300] At -3.0 dB, Train Loss: 0.1303614377975464 Train BER 0.006572972983121872,                  \n",
      " [225/300] At -1.0 dB, Train Loss: 0.001778098288923502 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.8567 minutes\n",
      "encoder learning rate: 1.04e-04, decoder learning rate: 1.04e-04\n",
      "[226/300] At -3.0 dB, Train Loss: 0.12369422614574432 Train BER 0.00618918938562274,                  \n",
      " [226/300] At -1.0 dB, Train Loss: 0.0016248238971456885 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.8240 minutes\n",
      "encoder learning rate: 1.01e-04, decoder learning rate: 1.01e-04\n",
      "[227/300] At -3.0 dB, Train Loss: 0.12485940754413605 Train BER 0.006145945750176907,                  \n",
      " [227/300] At -1.0 dB, Train Loss: 0.0018406878225505352 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 8.7104 minutes\n",
      "encoder learning rate: 9.88e-05, decoder learning rate: 9.88e-05\n",
      "[228/300] At -3.0 dB, Train Loss: 0.11728905886411667 Train BER 0.005643243435770273,                  \n",
      " [228/300] At -1.0 dB, Train Loss: 0.0011491657933220267 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.6860 minutes\n",
      "encoder learning rate: 9.63e-05, decoder learning rate: 9.63e-05\n",
      "[229/300] At -3.0 dB, Train Loss: 0.1219538077712059 Train BER 0.006059459410607815,                  \n",
      " [229/300] At -1.0 dB, Train Loss: 0.002168047009035945 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.7249 minutes\n",
      "encoder learning rate: 9.39e-05, decoder learning rate: 9.39e-05\n",
      "[230/300] At -3.0 dB, Train Loss: 0.13814353942871094 Train BER 0.006983783561736345,                  \n",
      " [230/300] At -1.0 dB, Train Loss: 0.0030551061499863863 Train BER 0.0001459459454054013\n",
      "Time for one full iteration is 8.7558 minutes\n",
      "encoder learning rate: 9.15e-05, decoder learning rate: 9.15e-05\n",
      "[231/300] At -3.0 dB, Train Loss: 0.12262435257434845 Train BER 0.006081080995500088,                  \n",
      " [231/300] At -1.0 dB, Train Loss: 0.0016253517242148519 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.7442 minutes\n",
      "encoder learning rate: 8.91e-05, decoder learning rate: 8.91e-05\n",
      "[232/300] At -3.0 dB, Train Loss: 0.13829253613948822 Train BER 0.007151351310312748,                  \n",
      " [232/300] At -1.0 dB, Train Loss: 0.0018868152983486652 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 8.7802 minutes\n",
      "encoder learning rate: 8.67e-05, decoder learning rate: 8.67e-05\n",
      "[233/300] At -3.0 dB, Train Loss: 0.14090900123119354 Train BER 0.006891891825944185,                  \n",
      " [233/300] At -1.0 dB, Train Loss: 0.0014114847872406244 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.8193 minutes\n",
      "encoder learning rate: 8.43e-05, decoder learning rate: 8.43e-05\n",
      "[234/300] At -3.0 dB, Train Loss: 0.1185954138636589 Train BER 0.0056486488319933414,                  \n",
      " [234/300] At -1.0 dB, Train Loss: 0.001182997366413474 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.8445 minutes\n",
      "encoder learning rate: 8.20e-05, decoder learning rate: 8.20e-05\n",
      "[235/300] At -3.0 dB, Train Loss: 0.13708971440792084 Train BER 0.007151351310312748,                  \n",
      " [235/300] At -1.0 dB, Train Loss: 0.0016781665617600083 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.9158 minutes\n",
      "encoder learning rate: 7.97e-05, decoder learning rate: 7.97e-05\n",
      "[236/300] At -3.0 dB, Train Loss: 0.13284753262996674 Train BER 0.006681080907583237,                  \n",
      " [236/300] At -1.0 dB, Train Loss: 0.002993762493133545 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 8.7642 minutes\n",
      "encoder learning rate: 7.74e-05, decoder learning rate: 7.74e-05\n",
      "[237/300] At -3.0 dB, Train Loss: 0.14820219576358795 Train BER 0.007654054090380669,                  \n",
      " [237/300] At -1.0 dB, Train Loss: 0.0009191136341542006 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.8647 minutes\n",
      "encoder learning rate: 7.52e-05, decoder learning rate: 7.52e-05\n",
      "[238/300] At -3.0 dB, Train Loss: 0.10929093509912491 Train BER 0.005248648580163717,                  \n",
      " [238/300] At -1.0 dB, Train Loss: 0.001439546700567007 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.8881 minutes\n",
      "encoder learning rate: 7.30e-05, decoder learning rate: 7.30e-05\n",
      "[239/300] At -3.0 dB, Train Loss: 0.12731972336769104 Train BER 0.006913513410836458,                  \n",
      " [239/300] At -1.0 dB, Train Loss: 0.0012857770780101418 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9088 minutes\n",
      "encoder learning rate: 7.08e-05, decoder learning rate: 7.08e-05\n",
      "[240/300] At -3.0 dB, Train Loss: 0.12741956114768982 Train BER 0.006497297435998917,                  \n",
      " [240/300] At -1.0 dB, Train Loss: 0.0007773799006827176 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9434 minutes\n",
      "encoder learning rate: 6.86e-05, decoder learning rate: 6.86e-05\n",
      "[241/300] At -3.0 dB, Train Loss: 0.15851333737373352 Train BER 0.008178378455340862,                  \n",
      " [241/300] At -1.0 dB, Train Loss: 0.001550612272694707 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.7711 minutes\n",
      "encoder learning rate: 6.65e-05, decoder learning rate: 6.65e-05\n",
      "[242/300] At -3.0 dB, Train Loss: 0.1341077834367752 Train BER 0.007172972895205021,                  \n",
      " [242/300] At -1.0 dB, Train Loss: 0.0010327874915674329 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.8820 minutes\n",
      "encoder learning rate: 6.44e-05, decoder learning rate: 6.44e-05\n",
      "[243/300] At -3.0 dB, Train Loss: 0.12325452268123627 Train BER 0.006232432555407286,                  \n",
      " [243/300] At -1.0 dB, Train Loss: 0.001615091459825635 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.7850 minutes\n",
      "encoder learning rate: 6.23e-05, decoder learning rate: 6.23e-05\n",
      "[244/300] At -3.0 dB, Train Loss: 0.12425799667835236 Train BER 0.006502702832221985,                  \n",
      " [244/300] At -1.0 dB, Train Loss: 0.0007079687784425914 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.7861 minutes\n",
      "encoder learning rate: 6.03e-05, decoder learning rate: 6.03e-05\n",
      "[245/300] At -3.0 dB, Train Loss: 0.13925229012966156 Train BER 0.007075675763189793,                  \n",
      " [245/300] At -1.0 dB, Train Loss: 0.001755614415742457 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.6474 minutes\n",
      "encoder learning rate: 5.83e-05, decoder learning rate: 5.83e-05\n",
      "[246/300] At -3.0 dB, Train Loss: 0.1364177167415619 Train BER 0.00690810801461339,                  \n",
      " [246/300] At -1.0 dB, Train Loss: 0.0018333689076825976 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.6542 minutes\n",
      "encoder learning rate: 5.63e-05, decoder learning rate: 5.63e-05\n",
      "[247/300] At -3.0 dB, Train Loss: 0.12187372893095016 Train BER 0.006156756542623043,                  \n",
      " [247/300] At -1.0 dB, Train Loss: 0.0014784745872020721 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.6950 minutes\n",
      "encoder learning rate: 5.43e-05, decoder learning rate: 5.43e-05\n",
      "[248/300] At -3.0 dB, Train Loss: 0.12140718847513199 Train BER 0.006005405448377132,                  \n",
      " [248/300] At -1.0 dB, Train Loss: 0.0025633766781538725 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.8142 minutes\n",
      "encoder learning rate: 5.24e-05, decoder learning rate: 5.24e-05\n",
      "[249/300] At -3.0 dB, Train Loss: 0.1518137902021408 Train BER 0.00789729692041874,                  \n",
      " [249/300] At -1.0 dB, Train Loss: 0.0031939218752086163 Train BER 0.000156756752403453\n",
      "Time for one full iteration is 8.8328 minutes\n",
      "encoder learning rate: 5.05e-05, decoder learning rate: 5.05e-05\n",
      "[250/300] At -3.0 dB, Train Loss: 0.136537566781044 Train BER 0.007183783687651157,                  \n",
      " [250/300] At -1.0 dB, Train Loss: 0.001352257444523275 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.7750 minutes\n",
      "encoder learning rate: 4.87e-05, decoder learning rate: 4.87e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[251/300] At -3.0 dB, Train Loss: 0.12309078127145767 Train BER 0.0060540540143847466,                  \n",
      " [251/300] At -1.0 dB, Train Loss: 0.0019278189865872264 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.9654 minutes\n",
      "encoder learning rate: 4.68e-05, decoder learning rate: 4.68e-05\n",
      "[252/300] At -3.0 dB, Train Loss: 0.12359282374382019 Train BER 0.006232432555407286,                  \n",
      " [252/300] At -1.0 dB, Train Loss: 0.0010853306157514453 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8661 minutes\n",
      "encoder learning rate: 4.50e-05, decoder learning rate: 4.50e-05\n",
      "[253/300] At -3.0 dB, Train Loss: 0.13164037466049194 Train BER 0.006232432555407286,                  \n",
      " [253/300] At -1.0 dB, Train Loss: 0.001648566103540361 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.9461 minutes\n",
      "encoder learning rate: 4.33e-05, decoder learning rate: 4.33e-05\n",
      "[254/300] At -3.0 dB, Train Loss: 0.11689941585063934 Train BER 0.00618918938562274,                  \n",
      " [254/300] At -1.0 dB, Train Loss: 0.0029616295360028744 Train BER 0.00015135135618038476\n",
      "Time for one full iteration is 8.9292 minutes\n",
      "encoder learning rate: 4.15e-05, decoder learning rate: 4.15e-05\n",
      "[255/300] At -3.0 dB, Train Loss: 0.13181933760643005 Train BER 0.006756756920367479,                  \n",
      " [255/300] At -1.0 dB, Train Loss: 0.001218695193529129 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.8220 minutes\n",
      "encoder learning rate: 3.98e-05, decoder learning rate: 3.98e-05\n",
      "[256/300] At -3.0 dB, Train Loss: 0.14677268266677856 Train BER 0.007318919058889151,                  \n",
      " [256/300] At -1.0 dB, Train Loss: 0.0011359816417098045 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9327 minutes\n",
      "encoder learning rate: 3.82e-05, decoder learning rate: 3.82e-05\n",
      "[257/300] At -3.0 dB, Train Loss: 0.13202667236328125 Train BER 0.007097297348082066,                  \n",
      " [257/300] At -1.0 dB, Train Loss: 0.0020212118979543447 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.9371 minutes\n",
      "encoder learning rate: 3.65e-05, decoder learning rate: 3.65e-05\n",
      "[258/300] At -3.0 dB, Train Loss: 0.13814617693424225 Train BER 0.006983783561736345,                  \n",
      " [258/300] At -1.0 dB, Train Loss: 0.0010966273257508874 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9014 minutes\n",
      "encoder learning rate: 3.50e-05, decoder learning rate: 3.50e-05\n",
      "[259/300] At -3.0 dB, Train Loss: 0.1565021127462387 Train BER 0.00824864860624075,                  \n",
      " [259/300] At -1.0 dB, Train Loss: 0.0019002403132617474 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.9050 minutes\n",
      "encoder learning rate: 3.34e-05, decoder learning rate: 3.34e-05\n",
      "[260/300] At -3.0 dB, Train Loss: 0.12085708230733871 Train BER 0.005691892001777887,                  \n",
      " [260/300] At -1.0 dB, Train Loss: 0.000783153809607029 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7706 minutes\n",
      "encoder learning rate: 3.19e-05, decoder learning rate: 3.19e-05\n",
      "[261/300] At -3.0 dB, Train Loss: 0.1379825919866562 Train BER 0.0067837839014828205,                  \n",
      " [261/300] At -1.0 dB, Train Loss: 0.001980809262022376 Train BER 9.72972993622534e-05\n",
      "Time for one full iteration is 8.8635 minutes\n",
      "encoder learning rate: 3.04e-05, decoder learning rate: 3.04e-05\n",
      "[262/300] At -3.0 dB, Train Loss: 0.1260906308889389 Train BER 0.006156756542623043,                  \n",
      " [262/300] At -1.0 dB, Train Loss: 0.0014581091236323118 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.7558 minutes\n",
      "encoder learning rate: 2.89e-05, decoder learning rate: 2.89e-05\n",
      "[263/300] At -3.0 dB, Train Loss: 0.12315817922353745 Train BER 0.006237837951630354,                  \n",
      " [263/300] At -1.0 dB, Train Loss: 0.001453432603739202 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.7515 minutes\n",
      "encoder learning rate: 2.75e-05, decoder learning rate: 2.75e-05\n",
      "[264/300] At -3.0 dB, Train Loss: 0.12191701680421829 Train BER 0.006308108102530241,                  \n",
      " [264/300] At -1.0 dB, Train Loss: 0.001365249278023839 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 8.8555 minutes\n",
      "encoder learning rate: 2.61e-05, decoder learning rate: 2.61e-05\n",
      "[265/300] At -3.0 dB, Train Loss: 0.09865841269493103 Train BER 0.00488108117133379,                  \n",
      " [265/300] At -1.0 dB, Train Loss: 0.001376437721773982 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.7932 minutes\n",
      "encoder learning rate: 2.47e-05, decoder learning rate: 2.47e-05\n",
      "[266/300] At -3.0 dB, Train Loss: 0.12418551743030548 Train BER 0.006237837951630354,                  \n",
      " [266/300] At -1.0 dB, Train Loss: 0.0013802427565678954 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.8339 minutes\n",
      "encoder learning rate: 2.34e-05, decoder learning rate: 2.34e-05\n",
      "[267/300] At -3.0 dB, Train Loss: 0.1351412683725357 Train BER 0.006551351398229599,                  \n",
      " [267/300] At -1.0 dB, Train Loss: 0.002126863459125161 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 8.7530 minutes\n",
      "encoder learning rate: 2.21e-05, decoder learning rate: 2.21e-05\n",
      "[268/300] At -3.0 dB, Train Loss: 0.12181565910577774 Train BER 0.006059459410607815,                  \n",
      " [268/300] At -1.0 dB, Train Loss: 0.001137215062044561 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.8257 minutes\n",
      "encoder learning rate: 2.09e-05, decoder learning rate: 2.09e-05\n",
      "[269/300] At -3.0 dB, Train Loss: 0.11375678330659866 Train BER 0.005729729775339365,                  \n",
      " [269/300] At -1.0 dB, Train Loss: 0.0013106635306030512 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.8722 minutes\n",
      "encoder learning rate: 1.97e-05, decoder learning rate: 1.97e-05\n",
      "[270/300] At -3.0 dB, Train Loss: 0.1412043273448944 Train BER 0.006994594819843769,                  \n",
      " [270/300] At -1.0 dB, Train Loss: 0.00167767982929945 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8639 minutes\n",
      "encoder learning rate: 1.85e-05, decoder learning rate: 1.85e-05\n",
      "[271/300] At -3.0 dB, Train Loss: 0.12562920153141022 Train BER 0.006162161938846111,                  \n",
      " [271/300] At -1.0 dB, Train Loss: 0.001289093866944313 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.8984 minutes\n",
      "encoder learning rate: 1.74e-05, decoder learning rate: 1.74e-05\n",
      "[272/300] At -3.0 dB, Train Loss: 0.1195344552397728 Train BER 0.006000000052154064,                  \n",
      " [272/300] At -1.0 dB, Train Loss: 0.000965610146522522 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.8799 minutes\n",
      "encoder learning rate: 1.63e-05, decoder learning rate: 1.63e-05\n",
      "[273/300] At -3.0 dB, Train Loss: 0.11821223795413971 Train BER 0.005816216114908457,                  \n",
      " [273/300] At -1.0 dB, Train Loss: 0.002455002861097455 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 8.7887 minutes\n",
      "encoder learning rate: 1.52e-05, decoder learning rate: 1.52e-05\n",
      "[274/300] At -3.0 dB, Train Loss: 0.13198626041412354 Train BER 0.006745946127921343,                  \n",
      " [274/300] At -1.0 dB, Train Loss: 0.0016077522886916995 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.8167 minutes\n",
      "encoder learning rate: 1.42e-05, decoder learning rate: 1.42e-05\n",
      "[275/300] At -3.0 dB, Train Loss: 0.11615481972694397 Train BER 0.005875675473362207,                  \n",
      " [275/300] At -1.0 dB, Train Loss: 0.003056533634662628 Train BER 0.000156756752403453\n",
      "Time for one full iteration is 8.9220 minutes\n",
      "encoder learning rate: 1.32e-05, decoder learning rate: 1.32e-05\n",
      "[276/300] At -3.0 dB, Train Loss: 0.11309190839529037 Train BER 0.005783783737570047,                  \n",
      " [276/300] At -1.0 dB, Train Loss: 0.0022568823769688606 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.7663 minutes\n",
      "encoder learning rate: 1.23e-05, decoder learning rate: 1.23e-05\n",
      "[277/300] At -3.0 dB, Train Loss: 0.1343252807855606 Train BER 0.006713513284921646,                  \n",
      " [277/300] At -1.0 dB, Train Loss: 0.0015810724580660462 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 8.9598 minutes\n",
      "encoder learning rate: 1.13e-05, decoder learning rate: 1.13e-05\n",
      "[278/300] At -3.0 dB, Train Loss: 0.1260923445224762 Train BER 0.0066324323415756226,                  \n",
      " [278/300] At -1.0 dB, Train Loss: 0.0025532101280987263 Train BER 8.108108158921823e-05\n",
      "Time for one full iteration is 8.8473 minutes\n",
      "encoder learning rate: 1.05e-05, decoder learning rate: 1.05e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[279/300] At -3.0 dB, Train Loss: 0.13131780922412872 Train BER 0.006459459662437439,                  \n",
      " [279/300] At -1.0 dB, Train Loss: 0.001097804051823914 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.6783 minutes\n",
      "encoder learning rate: 9.64e-06, decoder learning rate: 9.64e-06\n",
      "[280/300] At -3.0 dB, Train Loss: 0.11567474156618118 Train BER 0.005643243435770273,                  \n",
      " [280/300] At -1.0 dB, Train Loss: 0.00206084456294775 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 8.9027 minutes\n",
      "encoder learning rate: 8.84e-06, decoder learning rate: 8.84e-06\n",
      "[281/300] At -3.0 dB, Train Loss: 0.1266666054725647 Train BER 0.006854054052382708,                  \n",
      " [281/300] At -1.0 dB, Train Loss: 0.001399889588356018 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.9680 minutes\n",
      "encoder learning rate: 8.08e-06, decoder learning rate: 8.08e-06\n",
      "[282/300] At -3.0 dB, Train Loss: 0.1424039751291275 Train BER 0.007437837775796652,                  \n",
      " [282/300] At -1.0 dB, Train Loss: 0.001499298494309187 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.9863 minutes\n",
      "encoder learning rate: 7.36e-06, decoder learning rate: 7.36e-06\n",
      "[283/300] At -3.0 dB, Train Loss: 0.12834598124027252 Train BER 0.006491892039775848,                  \n",
      " [283/300] At -1.0 dB, Train Loss: 0.0018559531308710575 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 8.8769 minutes\n",
      "encoder learning rate: 6.67e-06, decoder learning rate: 6.67e-06\n",
      "[284/300] At -3.0 dB, Train Loss: 0.1246759220957756 Train BER 0.0063405404798686504,                  \n",
      " [284/300] At -1.0 dB, Train Loss: 0.0009525948553346097 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.8644 minutes\n",
      "encoder learning rate: 6.03e-06, decoder learning rate: 6.03e-06\n",
      "[285/300] At -3.0 dB, Train Loss: 0.11388944089412689 Train BER 0.005389189347624779,                  \n",
      " [285/300] At -1.0 dB, Train Loss: 0.0018259156495332718 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.8985 minutes\n",
      "encoder learning rate: 5.42e-06, decoder learning rate: 5.42e-06\n",
      "[286/300] At -3.0 dB, Train Loss: 0.13566537201404572 Train BER 0.006789189297705889,                  \n",
      " [286/300] At -1.0 dB, Train Loss: 0.0010476178722456098 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 8.8982 minutes\n",
      "encoder learning rate: 4.85e-06, decoder learning rate: 4.85e-06\n",
      "[287/300] At -3.0 dB, Train Loss: 0.1205390989780426 Train BER 0.005708108190447092,                  \n",
      " [287/300] At -1.0 dB, Train Loss: 0.0011962236603721976 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.9227 minutes\n",
      "encoder learning rate: 4.32e-06, decoder learning rate: 4.32e-06\n",
      "[288/300] At -3.0 dB, Train Loss: 0.11697696149349213 Train BER 0.005762162152677774,                  \n",
      " [288/300] At -1.0 dB, Train Loss: 0.0012459857389330864 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.9472 minutes\n",
      "encoder learning rate: 3.83e-06, decoder learning rate: 3.83e-06\n",
      "[289/300] At -3.0 dB, Train Loss: 0.13752911984920502 Train BER 0.006951351184397936,                  \n",
      " [289/300] At -1.0 dB, Train Loss: 0.0026437402702867985 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.8189 minutes\n",
      "encoder learning rate: 3.38e-06, decoder learning rate: 3.38e-06\n",
      "[290/300] At -3.0 dB, Train Loss: 0.13566772639751434 Train BER 0.006827027071267366,                  \n",
      " [290/300] At -1.0 dB, Train Loss: 0.0010893847793340683 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9344 minutes\n",
      "encoder learning rate: 2.97e-06, decoder learning rate: 2.97e-06\n",
      "[291/300] At -3.0 dB, Train Loss: 0.13417494297027588 Train BER 0.006891891825944185,                  \n",
      " [291/300] At -1.0 dB, Train Loss: 0.0012304268311709166 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.8152 minutes\n",
      "encoder learning rate: 2.59e-06, decoder learning rate: 2.59e-06\n",
      "[292/300] At -3.0 dB, Train Loss: 0.12997318804264069 Train BER 0.006502702832221985,                  \n",
      " [292/300] At -1.0 dB, Train Loss: 0.0012124307686462998 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.8765 minutes\n",
      "encoder learning rate: 2.26e-06, decoder learning rate: 2.26e-06\n",
      "[293/300] At -3.0 dB, Train Loss: 0.13578997552394867 Train BER 0.007010811008512974,                  \n",
      " [293/300] At -1.0 dB, Train Loss: 0.002581607084721327 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 8.9606 minutes\n",
      "encoder learning rate: 1.96e-06, decoder learning rate: 1.96e-06\n",
      "[294/300] At -3.0 dB, Train Loss: 0.136734738945961 Train BER 0.006794594693928957,                  \n",
      " [294/300] At -1.0 dB, Train Loss: 0.001043733791448176 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.6988 minutes\n",
      "encoder learning rate: 1.71e-06, decoder learning rate: 1.71e-06\n",
      "[295/300] At -3.0 dB, Train Loss: 0.13067811727523804 Train BER 0.006956756580621004,                  \n",
      " [295/300] At -1.0 dB, Train Loss: 0.00287417508661747 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 8.6849 minutes\n",
      "encoder learning rate: 1.49e-06, decoder learning rate: 1.49e-06\n",
      "[296/300] At -3.0 dB, Train Loss: 0.12557655572891235 Train BER 0.0061837839893996716,                  \n",
      " [296/300] At -1.0 dB, Train Loss: 0.0013663116842508316 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.7862 minutes\n",
      "encoder learning rate: 1.32e-06, decoder learning rate: 1.32e-06\n",
      "[297/300] At -3.0 dB, Train Loss: 0.13358943164348602 Train BER 0.006508108228445053,                  \n",
      " [297/300] At -1.0 dB, Train Loss: 0.0011234076227992773 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.7582 minutes\n",
      "encoder learning rate: 1.18e-06, decoder learning rate: 1.18e-06\n",
      "[298/300] At -3.0 dB, Train Loss: 0.1422877013683319 Train BER 0.006870270241051912,                  \n",
      " [298/300] At -1.0 dB, Train Loss: 0.002414998598396778 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 8.7888 minutes\n",
      "encoder learning rate: 1.08e-06, decoder learning rate: 1.08e-06\n",
      "[299/300] At -3.0 dB, Train Loss: 0.13015837967395782 Train BER 0.006729729939252138,                  \n",
      " [299/300] At -1.0 dB, Train Loss: 0.0010341097367927432 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 8.8589 minutes\n",
      "encoder learning rate: 1.02e-06, decoder learning rate: 1.02e-06\n",
      "[300/300] At -3.0 dB, Train Loss: 0.1326984167098999 Train BER 0.006756756920367479,                  \n",
      " [300/300] At -1.0 dB, Train Loss: 0.001863237819634378 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 8.8403 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO__-1.0_Encoder_KO_-3.0_Decoder/epochs_300_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/shubham19/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/scipy/stats/_morestats.py:1882: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeppolar Shapiro test W = 0.8606563210487366, p-value = 0.0\n",
      "Gaussian Shapiro test W = 0.9999827146530151, p-value = 0.5396721959114075\n",
      "Polar Shapiro test W = 0.6372725963592529, p-value = 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166670, BLER: 0.435868\n",
      "Deep - BER: 0.104884, BLER: 0.418323\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072762, BLER: 0.197385\n",
      "Deep - BER: 0.034050, BLER: 0.170131\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.019964, BLER: 0.055585\n",
      "Deep - BER: 0.006386, BLER: 0.044224\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003046, BLER: 0.008585\n",
      "Deep - BER: 0.000681, BLER: 0.007924\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000213, BLER: 0.000599\n",
      "Deep - BER: 0.000050, BLER: 0.001093\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.10488427016884089, 0.03404983779788017, 0.00638583783688955, 0.0006812162182559405, 4.97567565053032e-05]\n",
      "BERs of SC decoding: [0.16666972993314266, 0.07276227032393218, 0.01996367567498237, 0.0030464054057665634, 0.00021259459473003516]\n",
      "BLERs of DeepPolar: [0.4183229999999996, 0.17013100000000048, 0.044223999999999826, 0.00792399999999996, 0.0010929999999999959]\n",
      "BLERs of SC decoding: [0.43586799999999987, 0.19738499999999978, 0.05558499999999979, 0.008584999999999957, 0.0005990000000000005]\n",
      "time = 438.23745897213615 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
