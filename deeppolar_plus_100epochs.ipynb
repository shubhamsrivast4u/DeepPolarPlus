{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = False  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 100  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = 0.  # SNR at which encoder is trained\n",
    "dec_train_snr = -2.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.001  # Decoder Learning rate\n",
    "enc_lr = 0.001  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}_Encoder_{decoder_type}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=20,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=20,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/100] At -2.0 dB, Train Loss: 8.272284507751465 Train BER 0.4766054153442383,                  \n",
      " [1/100] At 0.0 dB, Train Loss: 8.057847023010254 Train BER 0.4734216332435608\n",
      "Time for one full iteration is 9.0080 minutes\n",
      "encoder learning rate: 5.00e-05, decoder learning rate: 5.00e-05\n",
      "[2/100] At -2.0 dB, Train Loss: 6.042236328125 Train BER 0.4656108021736145,                  \n",
      " [2/100] At 0.0 dB, Train Loss: 6.010614395141602 Train BER 0.4556594491004944\n",
      "Time for one full iteration is 9.0723 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[3/100] At -2.0 dB, Train Loss: 5.341467380523682 Train BER 0.369518905878067,                  \n",
      " [3/100] At 0.0 dB, Train Loss: 4.844986915588379 Train BER 0.3160054087638855\n",
      "Time for one full iteration is 9.0671 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[4/100] At -2.0 dB, Train Loss: 1.7477322816848755 Train BER 0.08950269967317581,                  \n",
      " [4/100] At 0.0 dB, Train Loss: 0.6260882616043091 Train BER 0.027729729190468788\n",
      "Time for one full iteration is 9.0560 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[5/100] At -2.0 dB, Train Loss: 0.8308014869689941 Train BER 0.041016217321157455,                  \n",
      " [5/100] At 0.0 dB, Train Loss: 0.2611684799194336 Train BER 0.014897297136485577\n",
      "Time for one full iteration is 8.9984 minutes\n",
      "encoder learning rate: 2.50e-04, decoder learning rate: 2.50e-04\n",
      "[6/100] At -2.0 dB, Train Loss: 0.49140799045562744 Train BER 0.02127026952803135,                  \n",
      " [6/100] At 0.0 dB, Train Loss: 0.14015434682369232 Train BER 0.004237837623804808\n",
      "Time for one full iteration is 8.8867 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[7/100] At -2.0 dB, Train Loss: 0.27626460790634155 Train BER 0.011237837374210358,                  \n",
      " [7/100] At 0.0 dB, Train Loss: 0.035332296043634415 Train BER 0.0009027026826515794\n",
      "Time for one full iteration is 9.0296 minutes\n",
      "encoder learning rate: 3.50e-04, decoder learning rate: 3.50e-04\n",
      "[8/100] At -2.0 dB, Train Loss: 0.2099446952342987 Train BER 0.008178378455340862,                  \n",
      " [8/100] At 0.0 dB, Train Loss: 0.019146740436553955 Train BER 0.00042702702921815217\n",
      "Time for one full iteration is 8.9848 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[9/100] At -2.0 dB, Train Loss: 0.1846356838941574 Train BER 0.007340540643781424,                  \n",
      " [9/100] At 0.0 dB, Train Loss: 0.016854364424943924 Train BER 0.0004810810787603259\n",
      "Time for one full iteration is 8.8939 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[10/100] At -2.0 dB, Train Loss: 0.1561010777950287 Train BER 0.006210810970515013,                  \n",
      " [10/100] At 0.0 dB, Train Loss: 0.010313191451132298 Train BER 0.00023783784126862884\n",
      "Time for one full iteration is 8.9812 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[11/100] At -2.0 dB, Train Loss: 0.13326556980609894 Train BER 0.005297297146171331,                  \n",
      " [11/100] At 0.0 dB, Train Loss: 0.006467399653047323 Train BER 0.00012432433140929788\n",
      "Time for one full iteration is 9.0243 minutes\n",
      "encoder learning rate: 5.50e-04, decoder learning rate: 5.50e-04\n",
      "[12/100] At -2.0 dB, Train Loss: 0.11436211317777634 Train BER 0.004891891963779926,                  \n",
      " [12/100] At 0.0 dB, Train Loss: 0.003828289918601513 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 8.9727 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[13/100] At -2.0 dB, Train Loss: 0.09586910903453827 Train BER 0.003886486403644085,                  \n",
      " [13/100] At 0.0 dB, Train Loss: 0.002356687793508172 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 9.0353 minutes\n",
      "encoder learning rate: 6.50e-04, decoder learning rate: 6.50e-04\n",
      "[14/100] At -2.0 dB, Train Loss: 0.09616254270076752 Train BER 0.004351351410150528,                  \n",
      " [14/100] At 0.0 dB, Train Loss: 0.002894532633945346 Train BER 8.648648508824408e-05\n",
      "Time for one full iteration is 9.0390 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n",
      "[15/100] At -2.0 dB, Train Loss: 0.09024307876825333 Train BER 0.003843243233859539,                  \n",
      " [15/100] At 0.0 dB, Train Loss: 0.0014877388020977378 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 8.9922 minutes\n",
      "encoder learning rate: 7.50e-04, decoder learning rate: 7.50e-04\n",
      "[16/100] At -2.0 dB, Train Loss: 0.0788993164896965 Train BER 0.003221621736884117,                  \n",
      " [16/100] At 0.0 dB, Train Loss: 0.0015341710532084107 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.9731 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[17/100] At -2.0 dB, Train Loss: 0.07504800707101822 Train BER 0.0031999999191612005,                  \n",
      " [17/100] At 0.0 dB, Train Loss: 0.0009340278338640928 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9735 minutes\n",
      "encoder learning rate: 8.50e-04, decoder learning rate: 8.50e-04\n",
      "[18/100] At -2.0 dB, Train Loss: 0.08031003922224045 Train BER 0.0034216216299682856,                  \n",
      " [18/100] At 0.0 dB, Train Loss: 0.002051361370831728 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 8.9564 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[19/100] At -2.0 dB, Train Loss: 0.07550056278705597 Train BER 0.003237837925553322,                  \n",
      " [19/100] At 0.0 dB, Train Loss: 0.002189570339396596 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 9.0619 minutes\n",
      "encoder learning rate: 9.50e-04, decoder learning rate: 9.50e-04\n",
      "[20/100] At -2.0 dB, Train Loss: 0.07861670106649399 Train BER 0.0033243242651224136,                  \n",
      " [20/100] At 0.0 dB, Train Loss: 0.0015392019413411617 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 9.1172 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[21/100] At -2.0 dB, Train Loss: 0.07458794116973877 Train BER 0.0033243242651224136,                  \n",
      " [21/100] At 0.0 dB, Train Loss: 0.0021298928186297417 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 9.1030 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[22/100] At -2.0 dB, Train Loss: 0.08035607635974884 Train BER 0.003551351372152567,                  \n",
      " [22/100] At 0.0 dB, Train Loss: 0.0016841645119711757 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 9.0456 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[23/100] At -2.0 dB, Train Loss: 0.06739365309476852 Train BER 0.0027027027681469917,                  \n",
      " [23/100] At 0.0 dB, Train Loss: 0.002562235575169325 Train BER 0.0001081081063603051\n",
      "Time for one full iteration is 9.0419 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[24/100] At -2.0 dB, Train Loss: 0.06632331013679504 Train BER 0.002659459365531802,                  \n",
      " [24/100] At 0.0 dB, Train Loss: 0.0015548592200502753 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 9.0276 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[25/100] At -2.0 dB, Train Loss: 0.07413329929113388 Train BER 0.0033945946488529444,                  \n",
      " [25/100] At 0.0 dB, Train Loss: 0.010486497543752193 Train BER 0.00037837837589904666\n",
      "Time for one full iteration is 9.0083 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[26/100] At -2.0 dB, Train Loss: 0.06946106255054474 Train BER 0.0029675676487386227,                  \n",
      " [26/100] At 0.0 dB, Train Loss: 0.0015199600020423532 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 8.9743 minutes\n",
      "encoder learning rate: 9.86e-04, decoder learning rate: 9.86e-04\n",
      "[27/100] At -2.0 dB, Train Loss: 0.06419745087623596 Train BER 0.002654053969308734,                  \n",
      " [27/100] At 0.0 dB, Train Loss: 0.007480223663151264 Train BER 0.00028108106926083565\n",
      "Time for one full iteration is 9.0429 minutes\n",
      "encoder learning rate: 9.81e-04, decoder learning rate: 9.81e-04\n",
      "[28/100] At -2.0 dB, Train Loss: 0.08460433781147003 Train BER 0.003772973082959652,                  \n",
      " [28/100] At 0.0 dB, Train Loss: 0.0010589356534183025 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0569 minutes\n",
      "encoder learning rate: 9.76e-04, decoder learning rate: 9.76e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/100] At -2.0 dB, Train Loss: 0.07429811358451843 Train BER 0.003124324372038245,                  \n",
      " [29/100] At 0.0 dB, Train Loss: 0.0028723401483148336 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 9.0455 minutes\n",
      "encoder learning rate: 9.69e-04, decoder learning rate: 9.69e-04\n",
      "[30/100] At -2.0 dB, Train Loss: 0.05328834429383278 Train BER 0.002221621572971344,                  \n",
      " [30/100] At 0.0 dB, Train Loss: 0.0005916479276493192 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0582 minutes\n",
      "encoder learning rate: 9.62e-04, decoder learning rate: 9.62e-04\n",
      "[31/100] At -2.0 dB, Train Loss: 0.0532478466629982 Train BER 0.0025189188309013844,                  \n",
      " [31/100] At 0.0 dB, Train Loss: 0.001164704910479486 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0549 minutes\n",
      "encoder learning rate: 9.54e-04, decoder learning rate: 9.54e-04\n",
      "[32/100] At -2.0 dB, Train Loss: 0.0508025661110878 Train BER 0.0022594593465328217,                  \n",
      " [32/100] At 0.0 dB, Train Loss: 0.00047512276796624064 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0718 minutes\n",
      "encoder learning rate: 9.46e-04, decoder learning rate: 9.46e-04\n",
      "[33/100] At -2.0 dB, Train Loss: 0.044350720942020416 Train BER 0.0018216216703876853,                  \n",
      " [33/100] At 0.0 dB, Train Loss: 0.0019190002931281924 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 9.0398 minutes\n",
      "encoder learning rate: 9.36e-04, decoder learning rate: 9.36e-04\n",
      "[34/100] At -2.0 dB, Train Loss: 0.04409762844443321 Train BER 0.002005405491217971,                  \n",
      " [34/100] At 0.0 dB, Train Loss: 0.001479819999076426 Train BER 7.027026731520891e-05\n",
      "Time for one full iteration is 9.0059 minutes\n",
      "encoder learning rate: 9.26e-04, decoder learning rate: 9.26e-04\n",
      "[35/100] At -2.0 dB, Train Loss: 0.051967885345220566 Train BER 0.0021675676107406616,                  \n",
      " [35/100] At 0.0 dB, Train Loss: 0.00025901070330291986 Train BER 0.0\n",
      "Time for one full iteration is 9.0350 minutes\n",
      "encoder learning rate: 9.16e-04, decoder learning rate: 9.16e-04\n",
      "[36/100] At -2.0 dB, Train Loss: 0.05782286822795868 Train BER 0.0025027026422321796,                  \n",
      " [36/100] At 0.0 dB, Train Loss: 0.0007225010776892304 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 9.0115 minutes\n",
      "encoder learning rate: 9.05e-04, decoder learning rate: 9.05e-04\n",
      "[37/100] At -2.0 dB, Train Loss: 0.04193059355020523 Train BER 0.0019351351074874401,                  \n",
      " [37/100] At 0.0 dB, Train Loss: 0.0006089291418902576 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9861 minutes\n",
      "encoder learning rate: 8.93e-04, decoder learning rate: 8.93e-04\n",
      "[38/100] At -2.0 dB, Train Loss: 0.04401858523488045 Train BER 0.0020000000949949026,                  \n",
      " [38/100] At 0.0 dB, Train Loss: 0.00033797399373725057 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0738 minutes\n",
      "encoder learning rate: 8.80e-04, decoder learning rate: 8.80e-04\n",
      "[39/100] At -2.0 dB, Train Loss: 0.04817507043480873 Train BER 0.0022162161767482758,                  \n",
      " [39/100] At 0.0 dB, Train Loss: 0.00024635554291307926 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0307 minutes\n",
      "encoder learning rate: 8.67e-04, decoder learning rate: 8.67e-04\n",
      "[40/100] At -2.0 dB, Train Loss: 0.052719905972480774 Train BER 0.002583783818408847,                  \n",
      " [40/100] At 0.0 dB, Train Loss: 0.0003254763432778418 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0524 minutes\n",
      "encoder learning rate: 8.54e-04, decoder learning rate: 8.54e-04\n",
      "[41/100] At -2.0 dB, Train Loss: 0.03941073268651962 Train BER 0.001740540494211018,                  \n",
      " [41/100] At 0.0 dB, Train Loss: 0.0002885630528908223 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0633 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n",
      "[42/100] At -2.0 dB, Train Loss: 0.03526531159877777 Train BER 0.001691891928203404,                  \n",
      " [42/100] At 0.0 dB, Train Loss: 0.00045646479702554643 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0551 minutes\n",
      "encoder learning rate: 8.25e-04, decoder learning rate: 8.25e-04\n",
      "[43/100] At -2.0 dB, Train Loss: 0.03560810536146164 Train BER 0.0014432432362809777,                  \n",
      " [43/100] At 0.0 dB, Train Loss: 0.00029160233680158854 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0508 minutes\n",
      "encoder learning rate: 8.10e-04, decoder learning rate: 8.10e-04\n",
      "[44/100] At -2.0 dB, Train Loss: 0.04359881952404976 Train BER 0.0019945946987718344,                  \n",
      " [44/100] At 0.0 dB, Train Loss: 0.0004607755981851369 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 8.9972 minutes\n",
      "encoder learning rate: 7.94e-04, decoder learning rate: 7.94e-04\n",
      "[45/100] At -2.0 dB, Train Loss: 0.042014993727207184 Train BER 0.0019405405037105083,                  \n",
      " [45/100] At 0.0 dB, Train Loss: 0.0006394031806848943 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 9.0417 minutes\n",
      "encoder learning rate: 7.78e-04, decoder learning rate: 7.78e-04\n",
      "[46/100] At -2.0 dB, Train Loss: 0.04240720346570015 Train BER 0.0019189189188182354,                  \n",
      " [46/100] At 0.0 dB, Train Loss: 0.00017695485439617187 Train BER 0.0\n",
      "Time for one full iteration is 9.0322 minutes\n",
      "encoder learning rate: 7.61e-04, decoder learning rate: 7.61e-04\n",
      "[47/100] At -2.0 dB, Train Loss: 0.03747335076332092 Train BER 0.0017621621955186129,                  \n",
      " [47/100] At 0.0 dB, Train Loss: 0.0009602322825230658 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 9.0658 minutes\n",
      "encoder learning rate: 7.45e-04, decoder learning rate: 7.45e-04\n",
      "[48/100] At -2.0 dB, Train Loss: 0.035644203424453735 Train BER 0.0015999999595806003,                  \n",
      " [48/100] At 0.0 dB, Train Loss: 0.00020551680063363165 Train BER 0.0\n",
      "Time for one full iteration is 9.0229 minutes\n",
      "encoder learning rate: 7.27e-04, decoder learning rate: 7.27e-04\n",
      "[49/100] At -2.0 dB, Train Loss: 0.04030833765864372 Train BER 0.0019297297112643719,                  \n",
      " [49/100] At 0.0 dB, Train Loss: 0.00015974827692843974 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0238 minutes\n",
      "encoder learning rate: 7.10e-04, decoder learning rate: 7.10e-04\n",
      "[50/100] At -2.0 dB, Train Loss: 0.03364017605781555 Train BER 0.0015513513935729861,                  \n",
      " [50/100] At 0.0 dB, Train Loss: 0.00034997129114344716 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0034 minutes\n",
      "encoder learning rate: 6.92e-04, decoder learning rate: 6.92e-04\n",
      "[51/100] At -2.0 dB, Train Loss: 0.03833499550819397 Train BER 0.0017837837804108858,                  \n",
      " [51/100] At 0.0 dB, Train Loss: 0.00015193472790997475 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0736 minutes\n",
      "encoder learning rate: 6.73e-04, decoder learning rate: 6.73e-04\n",
      "[52/100] At -2.0 dB, Train Loss: 0.03386480361223221 Train BER 0.0016594594344496727,                  \n",
      " [52/100] At 0.0 dB, Train Loss: 0.00013781517918687314 Train BER 0.0\n",
      "Time for one full iteration is 9.0780 minutes\n",
      "encoder learning rate: 6.55e-04, decoder learning rate: 6.55e-04\n",
      "[53/100] At -2.0 dB, Train Loss: 0.042050886899232864 Train BER 0.001886486541479826,                  \n",
      " [53/100] At 0.0 dB, Train Loss: 0.00033628649543970823 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0702 minutes\n",
      "encoder learning rate: 6.36e-04, decoder learning rate: 6.36e-04\n",
      "[54/100] At -2.0 dB, Train Loss: 0.031848322600126266 Train BER 0.001470270217396319,                  \n",
      " [54/100] At 0.0 dB, Train Loss: 0.00043779233237728477 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0263 minutes\n",
      "encoder learning rate: 6.17e-04, decoder learning rate: 6.17e-04\n",
      "[55/100] At -2.0 dB, Train Loss: 0.04265270754694939 Train BER 0.0021405406296253204,                  \n",
      " [55/100] At 0.0 dB, Train Loss: 0.000381760997697711 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0526 minutes\n",
      "encoder learning rate: 5.98e-04, decoder learning rate: 5.98e-04\n",
      "[56/100] At -2.0 dB, Train Loss: 0.035709261894226074 Train BER 0.0017297297017648816,                  \n",
      " [56/100] At 0.0 dB, Train Loss: 0.0009111020481213927 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 9.0747 minutes\n",
      "encoder learning rate: 5.79e-04, decoder learning rate: 5.79e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/100] At -2.0 dB, Train Loss: 0.03525664284825325 Train BER 0.0018054053653031588,                  \n",
      " [57/100] At 0.0 dB, Train Loss: 0.00023707114451099187 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0200 minutes\n",
      "encoder learning rate: 5.59e-04, decoder learning rate: 5.59e-04\n",
      "[58/100] At -2.0 dB, Train Loss: 0.025215398520231247 Train BER 0.0011027026921510696,                  \n",
      " [58/100] At 0.0 dB, Train Loss: 0.00030311706359498203 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0346 minutes\n",
      "encoder learning rate: 5.40e-04, decoder learning rate: 5.40e-04\n",
      "[59/100] At -2.0 dB, Train Loss: 0.031441494822502136 Train BER 0.0013189188903197646,                  \n",
      " [59/100] At 0.0 dB, Train Loss: 0.00010341669985791668 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0167 minutes\n",
      "encoder learning rate: 5.20e-04, decoder learning rate: 5.20e-04\n",
      "[60/100] At -2.0 dB, Train Loss: 0.02954462729394436 Train BER 0.001378378365188837,                  \n",
      " [60/100] At 0.0 dB, Train Loss: 0.00011962303688051179 Train BER 0.0\n",
      "Time for one full iteration is 9.0182 minutes\n",
      "encoder learning rate: 5.01e-04, decoder learning rate: 5.01e-04\n",
      "[61/100] At -2.0 dB, Train Loss: 0.030271047726273537 Train BER 0.00139999995008111,                  \n",
      " [61/100] At 0.0 dB, Train Loss: 6.077398938941769e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0444 minutes\n",
      "encoder learning rate: 4.81e-04, decoder learning rate: 4.81e-04\n",
      "[62/100] At -2.0 dB, Train Loss: 0.03286509960889816 Train BER 0.0015081081073731184,                  \n",
      " [62/100] At 0.0 dB, Train Loss: 8.281347254524007e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0439 minutes\n",
      "encoder learning rate: 4.61e-04, decoder learning rate: 4.61e-04\n",
      "[63/100] At -2.0 dB, Train Loss: 0.04555077105760574 Train BER 0.002270270371809602,                  \n",
      " [63/100] At 0.0 dB, Train Loss: 0.00022299551346804947 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0225 minutes\n",
      "encoder learning rate: 4.42e-04, decoder learning rate: 4.42e-04\n",
      "[64/100] At -2.0 dB, Train Loss: 0.027049396187067032 Train BER 0.0012918919092044234,                  \n",
      " [64/100] At 0.0 dB, Train Loss: 0.00013987446436658502 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0047 minutes\n",
      "encoder learning rate: 4.22e-04, decoder learning rate: 4.22e-04\n",
      "[65/100] At -2.0 dB, Train Loss: 0.03282737359404564 Train BER 0.0015297296922653913,                  \n",
      " [65/100] At 0.0 dB, Train Loss: 0.00022813135001342744 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 9.0365 minutes\n",
      "encoder learning rate: 4.03e-04, decoder learning rate: 4.03e-04\n",
      "[66/100] At -2.0 dB, Train Loss: 0.030903242528438568 Train BER 0.0014864865224808455,                  \n",
      " [66/100] At 0.0 dB, Train Loss: 0.00019930425332859159 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 9.0624 minutes\n",
      "encoder learning rate: 3.84e-04, decoder learning rate: 3.84e-04\n",
      "[67/100] At -2.0 dB, Train Loss: 0.027531404048204422 Train BER 0.001232432434335351,                  \n",
      " [67/100] At 0.0 dB, Train Loss: 5.121675712871365e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0546 minutes\n",
      "encoder learning rate: 3.65e-04, decoder learning rate: 3.65e-04\n",
      "[68/100] At -2.0 dB, Train Loss: 0.026479095220565796 Train BER 0.0012162162456661463,                  \n",
      " [68/100] At 0.0 dB, Train Loss: 6.364742148434743e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0298 minutes\n",
      "encoder learning rate: 3.46e-04, decoder learning rate: 3.46e-04\n",
      "[69/100] At -2.0 dB, Train Loss: 0.02387102320790291 Train BER 0.001183783751912415,                  \n",
      " [69/100] At 0.0 dB, Train Loss: 0.00020256053539924324 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0411 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[70/100] At -2.0 dB, Train Loss: 0.028405049815773964 Train BER 0.0013351350789889693,                  \n",
      " [70/100] At 0.0 dB, Train Loss: 0.00010604873386910185 Train BER 0.0\n",
      "Time for one full iteration is 8.7148 minutes\n",
      "encoder learning rate: 3.09e-04, decoder learning rate: 3.09e-04\n",
      "[71/100] At -2.0 dB, Train Loss: 0.02857224829494953 Train BER 0.0013405405916273594,                  \n",
      " [71/100] At 0.0 dB, Train Loss: 3.804549123742618e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7039 minutes\n",
      "encoder learning rate: 2.91e-04, decoder learning rate: 2.91e-04\n",
      "[72/100] At -2.0 dB, Train Loss: 0.03289957344532013 Train BER 0.0015621621860191226,                  \n",
      " [72/100] At 0.0 dB, Train Loss: 7.896923489170149e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7852 minutes\n",
      "encoder learning rate: 2.74e-04, decoder learning rate: 2.74e-04\n",
      "[73/100] At -2.0 dB, Train Loss: 0.022806992754340172 Train BER 0.0011135134845972061,                  \n",
      " [73/100] At 0.0 dB, Train Loss: 0.00011835914483526722 Train BER 0.0\n",
      "Time for one full iteration is 8.7748 minutes\n",
      "encoder learning rate: 2.56e-04, decoder learning rate: 2.56e-04\n",
      "[74/100] At -2.0 dB, Train Loss: 0.020679615437984467 Train BER 0.0008270270191133022,                  \n",
      " [74/100] At 0.0 dB, Train Loss: 0.00016031852283049375 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 8.9849 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[75/100] At -2.0 dB, Train Loss: 0.023625200614333153 Train BER 0.0012594594154506922,                  \n",
      " [75/100] At 0.0 dB, Train Loss: 0.0004071970470249653 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 8.7372 minutes\n",
      "encoder learning rate: 2.23e-04, decoder learning rate: 2.23e-04\n",
      "[76/100] At -2.0 dB, Train Loss: 0.022703155875205994 Train BER 0.0009135135333053768,                  \n",
      " [76/100] At 0.0 dB, Train Loss: 3.061233655898832e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.9208 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[77/100] At -2.0 dB, Train Loss: 0.022824669256806374 Train BER 0.0009675675537437201,                  \n",
      " [77/100] At 0.0 dB, Train Loss: 0.00011780646309489384 Train BER 0.0\n",
      "Time for one full iteration is 8.7609 minutes\n",
      "encoder learning rate: 1.91e-04, decoder learning rate: 1.91e-04\n",
      "[78/100] At -2.0 dB, Train Loss: 0.03780393302440643 Train BER 0.0019405405037105083,                  \n",
      " [78/100] At 0.0 dB, Train Loss: 3.2313779229298234e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8727 minutes\n",
      "encoder learning rate: 1.76e-04, decoder learning rate: 1.76e-04\n",
      "[79/100] At -2.0 dB, Train Loss: 0.018698325380682945 Train BER 0.0009297297219745815,                  \n",
      " [79/100] At 0.0 dB, Train Loss: 5.5955955758690834e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.8072 minutes\n",
      "encoder learning rate: 1.61e-04, decoder learning rate: 1.61e-04\n",
      "[80/100] At -2.0 dB, Train Loss: 0.024294834583997726 Train BER 0.0010972972959280014,                  \n",
      " [80/100] At 0.0 dB, Train Loss: 7.882730278652161e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7450 minutes\n",
      "encoder learning rate: 1.47e-04, decoder learning rate: 1.47e-04\n",
      "[81/100] At -2.0 dB, Train Loss: 0.03435543179512024 Train BER 0.00183783785905689,                  \n",
      " [81/100] At 0.0 dB, Train Loss: 9.849837806541473e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.8368 minutes\n",
      "encoder learning rate: 1.34e-04, decoder learning rate: 1.34e-04\n",
      "[82/100] At -2.0 dB, Train Loss: 0.025467660278081894 Train BER 0.0013513513840734959,                  \n",
      " [82/100] At 0.0 dB, Train Loss: 3.774739889195189e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7697 minutes\n",
      "encoder learning rate: 1.21e-04, decoder learning rate: 1.21e-04\n",
      "[83/100] At -2.0 dB, Train Loss: 0.02877870202064514 Train BER 0.001329729682765901,                  \n",
      " [83/100] At 0.0 dB, Train Loss: 6.753172783646733e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0265 minutes\n",
      "encoder learning rate: 1.08e-04, decoder learning rate: 1.08e-04\n",
      "[84/100] At -2.0 dB, Train Loss: 0.020107118412852287 Train BER 0.0008864864939823747,                  \n",
      " [84/100] At 0.0 dB, Train Loss: 0.00023099993995856494 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0760 minutes\n",
      "encoder learning rate: 9.64e-05, decoder learning rate: 9.64e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/100] At -2.0 dB, Train Loss: 0.025877445936203003 Train BER 0.0012918919092044234,                  \n",
      " [85/100] At 0.0 dB, Train Loss: 3.7658064684364945e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0843 minutes\n",
      "encoder learning rate: 8.52e-05, decoder learning rate: 8.52e-05\n",
      "[86/100] At -2.0 dB, Train Loss: 0.020138071849942207 Train BER 0.0007891891873441637,                  \n",
      " [86/100] At 0.0 dB, Train Loss: 7.963881944306195e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0266 minutes\n",
      "encoder learning rate: 7.46e-05, decoder learning rate: 7.46e-05\n",
      "[87/100] At -2.0 dB, Train Loss: 0.026594936847686768 Train BER 0.001356756780296564,                  \n",
      " [87/100] At 0.0 dB, Train Loss: 0.00010901474161073565 Train BER 0.0\n",
      "Time for one full iteration is 9.0628 minutes\n",
      "encoder learning rate: 6.47e-05, decoder learning rate: 6.47e-05\n",
      "[88/100] At -2.0 dB, Train Loss: 0.028712015599012375 Train BER 0.001427027047611773,                  \n",
      " [88/100] At 0.0 dB, Train Loss: 9.645431418903172e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 9.0455 minutes\n",
      "encoder learning rate: 5.54e-05, decoder learning rate: 5.54e-05\n",
      "[89/100] At -2.0 dB, Train Loss: 0.03158346563577652 Train BER 0.0015675675822421908,                  \n",
      " [89/100] At 0.0 dB, Train Loss: 2.4490218493156135e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.1438 minutes\n",
      "encoder learning rate: 4.69e-05, decoder learning rate: 4.69e-05\n",
      "[90/100] At -2.0 dB, Train Loss: 0.02321416698396206 Train BER 0.0011459459783509374,                  \n",
      " [90/100] At 0.0 dB, Train Loss: 4.0880720916902646e-05 Train BER 0.0\n",
      "Time for one full iteration is 9.0025 minutes\n",
      "encoder learning rate: 3.90e-05, decoder learning rate: 3.90e-05\n",
      "[91/100] At -2.0 dB, Train Loss: 0.01895865984261036 Train BER 0.0008864864939823747,                  \n",
      " [91/100] At 0.0 dB, Train Loss: 0.00010869808465940878 Train BER 0.0\n",
      "Time for one full iteration is 8.6037 minutes\n",
      "encoder learning rate: 3.19e-05, decoder learning rate: 3.19e-05\n",
      "[92/100] At -2.0 dB, Train Loss: 0.024014810100197792 Train BER 0.0011081080883741379,                  \n",
      " [92/100] At 0.0 dB, Train Loss: 4.318884020904079e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6356 minutes\n",
      "encoder learning rate: 2.54e-05, decoder learning rate: 2.54e-05\n",
      "[93/100] At -2.0 dB, Train Loss: 0.017166871577501297 Train BER 0.0007081081275828183,                  \n",
      " [93/100] At 0.0 dB, Train Loss: 7.594465569127351e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6232 minutes\n",
      "encoder learning rate: 1.98e-05, decoder learning rate: 1.98e-05\n",
      "[94/100] At -2.0 dB, Train Loss: 0.021278707310557365 Train BER 0.0009621621575206518,                  \n",
      " [94/100] At 0.0 dB, Train Loss: 0.0001973873149836436 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.6434 minutes\n",
      "encoder learning rate: 1.48e-05, decoder learning rate: 1.48e-05\n",
      "[95/100] At -2.0 dB, Train Loss: 0.020217545330524445 Train BER 0.0009297297219745815,                  \n",
      " [95/100] At 0.0 dB, Train Loss: 3.827712862403132e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6387 minutes\n",
      "encoder learning rate: 1.06e-05, decoder learning rate: 1.06e-05\n",
      "[96/100] At -2.0 dB, Train Loss: 0.026933744549751282 Train BER 0.001329729682765901,                  \n",
      " [96/100] At 0.0 dB, Train Loss: 7.249317422974855e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.7051 minutes\n",
      "encoder learning rate: 7.15e-06, decoder learning rate: 7.15e-06\n",
      "[97/100] At -2.0 dB, Train Loss: 0.02231445536017418 Train BER 0.0011081080883741379,                  \n",
      " [97/100] At 0.0 dB, Train Loss: 4.160450407653116e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6360 minutes\n",
      "encoder learning rate: 4.46e-06, decoder learning rate: 4.46e-06\n",
      "[98/100] At -2.0 dB, Train Loss: 0.02442486770451069 Train BER 0.0011567567707970738,                  \n",
      " [98/100] At 0.0 dB, Train Loss: 3.2080173696158454e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6574 minutes\n",
      "encoder learning rate: 2.54e-06, decoder learning rate: 2.54e-06\n",
      "[99/100] At -2.0 dB, Train Loss: 0.022332096472382545 Train BER 0.0010486486135050654,                  \n",
      " [99/100] At 0.0 dB, Train Loss: 3.541060505085625e-05 Train BER 0.0\n",
      "Time for one full iteration is 8.6216 minutes\n",
      "encoder learning rate: 1.39e-06, decoder learning rate: 1.39e-06\n",
      "[100/100] At -2.0 dB, Train Loss: 0.01502589788287878 Train BER 0.00070270273135975,                  \n",
      " [100/100] At 0.0 dB, Train Loss: 8.432616596110165e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 8.9062 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO_Encoder_KO_Decoder/epochs_100_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166776, BLER: 0.435589\n",
      "Deep - BER: 0.130515, BLER: 0.499748\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072330, BLER: 0.196038\n",
      "Deep - BER: 0.047510, BLER: 0.221721\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.019923, BLER: 0.055449\n",
      "Deep - BER: 0.010000, BLER: 0.059748\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.002988, BLER: 0.008472\n",
      "Deep - BER: 0.001147, BLER: 0.009738\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000204, BLER: 0.000568\n",
      "Deep - BER: 0.000070, BLER: 0.000984\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.13051494604349137, 0.047509513515979054, 0.0100004054014571, 0.0011468918935715919, 7.024324301710294e-05]\n",
      "BERs of SC decoding: [0.16677567571401597, 0.07232967572659255, 0.01992283785995096, 0.0029879999954882806, 0.0002041081088500505]\n",
      "BLERs of DeepPolar: [0.49974799999999975, 0.22172100000000042, 0.059747999999999926, 0.009737999999999938, 0.0009840000000000007]\n",
      "BLERs of SC decoding: [0.43558900000000067, 0.19603799999999977, 0.05544899999999985, 0.008471999999999941, 0.0005680000000000004]\n",
      "time = 432.6645388921102 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
