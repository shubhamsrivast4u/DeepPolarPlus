{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8752b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acc45a",
   "metadata": {},
   "source": [
    "# Configuration variables (previously args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b957ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256  # Block length\n",
    "K = 37   # Message size\n",
    "kernel_size = 16  # Kernel size (ell)\n",
    "rate_profile = 'polar'  # Rate profiling; choices=['RM', 'polar', 'sorted', 'last', 'rev_polar', 'custom']\n",
    "infty = 1000.  # Infinity value for frozen position LLR in polar dec\n",
    "lse = 'minsum'  # LSE function; choices=['minsum', 'lse']\n",
    "hard_decision = False  # Polar code sc decoding hard decision?\n",
    "\n",
    "# DeepPolar parameters\n",
    "encoder_type = 'KO'  # Type of encoding; choices=['KO', 'scaled', 'polar']\n",
    "decoder_type = 'KO'  # Type of decoding; choices=['KO', 'SC', 'KO_parallel', 'KO_last_parallel']\n",
    "enc_activation = 'selu'  # Activation function\n",
    "dec_activation = 'selu'  # Activation function\n",
    "dropout_p = 0.\n",
    "dec_hidden_size = 128  # Neural network size\n",
    "enc_hidden_size = 64   # Neural network size\n",
    "f_depth = 3  # Decoder neural network depth\n",
    "g_depth = 3  # Encoder neural network depth\n",
    "g_skip_depth = 1  # Encoder neural network skip depth\n",
    "g_skip_layer = 1  # Encoder neural network skip layer\n",
    "onehot = False  # Use onehot representation of prev_decoded_bits\n",
    "shared = False  # Share weights across depth\n",
    "use_skip = True  # Use skip connections\n",
    "use_norm = False  # Use normalization\n",
    "binary = False  # Use binary quantization\n",
    "\n",
    "# Infrastructure parameters\n",
    "id = None  # Optional ID for multiple runs\n",
    "test = False  # Testing mode flag\n",
    "pairwise = False  # Plot codeword pairwise distances\n",
    "epos = False  # Plot error positions\n",
    "seed = None  # Random seed\n",
    "anomaly = False  # Enable anomaly detection\n",
    "dataparallel = False  # Use dataparallel\n",
    "\n",
    "\n",
    "\n",
    "# Model architecture parameters\n",
    "polar_depths = []  # List of depths to use polar encoding/decoding\n",
    "last_ell = None  # Use kernel last_ell last layer\n",
    "\n",
    "\n",
    "# Channel parameters\n",
    "radar_power = None  # Radar power parameter\n",
    "radar_prob = 0.1  # Radar probability parameter\n",
    "\n",
    "# Training parameters\n",
    "full_iters = 200  # Full iterations\n",
    "enc_train_iters = 30  # Encoder iterations\n",
    "dec_train_iters = 300  # Decoder iterations\n",
    "enc_train_snr = 0.  # SNR at which encoder is trained\n",
    "dec_train_snr = -2.  # SNR at which decoder is trained\n",
    "weight_decay = 0.0\n",
    "dec_lr = 0.001  # Decoder Learning rate\n",
    "enc_lr = 0.001  # Encoder Learning rate\n",
    "batch_size = 20000  # Size of batches\n",
    "small_batch_size = 5000  # Size of small batches\n",
    "noise_type = 'awgn'  # Noise type; choices=['fading', 'awgn', 'radar']\n",
    "regularizer = None  # Regularizer type; choices=['std', 'max_deviation','polar']\n",
    "regularizer_weight = 0.001\n",
    "loss_type = 'BCE' # loss function; choices=['MSE', 'BCE', 'BCE_reg', 'L1', 'huber', 'focal', 'BCE_bler']\n",
    "initialization = 'random'  # Initialization type; choices=['random', 'zeros']\n",
    "optim_name = 'Adam'  # Optimizer type; choices=['Adam', 'RMS', 'SGD', 'AdamW']\n",
    "\n",
    "# Testing parameters\n",
    "test_batch_size = 1000  # Size of test batches\n",
    "num_errors = 100  # Test until _ block errors\n",
    "test_snr_start = -5.  # Testing SNR start\n",
    "test_snr_end = -1.   # Testing SNR end\n",
    "snr_points = 5       # Testing SNR num points\n",
    "\n",
    "\n",
    "\n",
    "# Model saving/loading parameters\n",
    "model_save_per = 100  # Model save frequency\n",
    "model_iters = None  # Option to load specific model iteration\n",
    "test_load_path = None  # Path to load test model\n",
    "\n",
    "load_path = None  # Load path \n",
    "kernel_load_path = 'Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new'   # Kernel load path\n",
    "no_fig = False  # Plot figure option\n",
    "\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler = 'cosine' # choices = ['reduce', '1cycle', 'cosine']\n",
    "scheduler_patience = None  # Scheduler patience\n",
    "batch_schedule = False  # Use batch scheduler\n",
    "batch_patience = 50  # Batch scheduler patience \n",
    "batch_factor = 2  # Batch multiplication factor\n",
    "min_batch_size = 500  # Minimum batch size\n",
    "max_batch_size = 50000  # Maximum batch size\n",
    "\n",
    "# Device configuration \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da887ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_path = f\"DeepPolar_Results/attention_Polar_{kernel_size}({N},{K})/Scheme_{rate_profile}/{encoder_type}_Encoder_{decoder_type}_Decoder/epochs_{full_iters}_batchsize_{batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8140b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_save_path, exist_ok=True)\n",
    "os.makedirs(results_save_path +'/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e521",
   "metadata": {},
   "source": [
    "# Part 1: Core Utilities and Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_db2sigma(train_snr):\n",
    "    return 10**(-train_snr*1.0/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a23a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bb73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a smoother version using product of bit probabilities\n",
    "def soft_bler_loss(logits, targets):\n",
    "    bit_probs = torch.sigmoid(logits)  # For correct bits\n",
    "    bit_probs = torch.where(targets == 1., bit_probs, 1 - bit_probs)\n",
    "    block_probs = torch.prod(bit_probs, dim=1)  # Probability of whole block being correct\n",
    "    return -torch.mean(torch.log(block_probs + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b989d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_ber(y_true, y_pred, mask=None):\n",
    "    if mask == None:\n",
    "        mask=torch.ones(y_true.size(),device=y_true.device)\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "    mask = mask.view(mask.shape[0], -1, 1)\n",
    "    myOtherTensor = (mask*torch.ne(torch.round(y_true), torch.round(y_pred))).float()\n",
    "    res = sum(sum(myOtherTensor))/(torch.sum(mask))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ebc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_bler(y_true, y_pred, get_pos = False):\n",
    "    y_true = y_true.view(y_true.shape[0], -1, 1)\n",
    "    y_pred = y_pred.view(y_pred.shape[0], -1, 1)\n",
    "\n",
    "    decoded_bits = torch.round(y_pred).cpu()\n",
    "    X_test = torch.round(y_true).cpu()\n",
    "    tp0 = (abs(decoded_bits-X_test)).view([X_test.shape[0],X_test.shape[1]])\n",
    "    tp0 = tp0.detach().cpu().numpy()\n",
    "    bler_err_rate = sum(np.sum(tp0,axis=1)>0)*1.0/(X_test.shape[0])\n",
    "\n",
    "    if not get_pos:\n",
    "        return bler_err_rate\n",
    "    else:\n",
    "        err_pos = list(np.nonzero((np.sum(tp0,axis=1)>0).astype(int))[0])\n",
    "        return bler_err_rate, err_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_signal(input_signal, sigma = 1.0, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 0.05):\n",
    "    data_shape = input_signal.shape\n",
    "    device = input_signal.device\n",
    "    if noise_type == 'awgn':\n",
    "        dist = torch.distributions.Normal(torch.tensor([0.0], device=device), torch.tensor([sigma], device=device))\n",
    "        noise = dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 'fading':\n",
    "        fading_h = torch.sqrt(torch.randn_like(input_signal)**2 + torch.randn_like(input_signal)**2)/np.sqrt(3.14/2.0)\n",
    "        noise = sigma * torch.randn_like(input_signal)\n",
    "        corrupted_signal = fading_h *(input_signal) + noise\n",
    "\n",
    "    elif noise_type == 'radar':\n",
    "        add_pos = np.random.choice([0.0, 1.0], data_shape, p=[1 - radar_prob, radar_prob])\n",
    "        corrupted_signal = radar_power* np.random.standard_normal(size=data_shape) * add_pos\n",
    "        noise = sigma * torch.randn_like(input_signal) +\\\n",
    "                    torch.from_numpy(corrupted_signal).float().to(input_signal.device)\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    elif noise_type == 't-dist':\n",
    "        dist = torch.distributions.StudentT(torch.tensor([vv], device=device))\n",
    "        noise = sigma* dist.sample(input_signal.shape).squeeze()\n",
    "        corrupted_signal = input_signal + noise\n",
    "\n",
    "    return corrupted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e97bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp(x, y):\n",
    "    log_sum_ms = torch.min(torch.abs(x), torch.abs(y))*torch.sign(x)*torch.sign(y)\n",
    "    return log_sum_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5937279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sum_log_sum_exp_4(x_1, x_2, x_3, x_4):\n",
    "    return min_sum_log_sum_exp(min_sum_log_sum_exp(x_1, x_2), min_sum_log_sum_exp(x_3, x_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c239bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, y):\n",
    "    def log_sum_exp_(LLR_vector):\n",
    "        sum_vector = LLR_vector.sum(dim=1, keepdim=True)\n",
    "        sum_concat = torch.cat([sum_vector, torch.zeros_like(sum_vector)], dim=1)\n",
    "        return torch.logsumexp(sum_concat, dim=1)- torch.logsumexp(LLR_vector, dim=1) \n",
    "\n",
    "    Lv = log_sum_exp_(torch.cat([x.unsqueeze(2), y.unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "    return Lv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655fe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bitarray(in_number, bit_width):\n",
    "    binary_string = bin(in_number)\n",
    "    length = len(binary_string)\n",
    "    bitarray = np.zeros(bit_width, 'int')\n",
    "    for i in range(length-2):\n",
    "        bitarray[bit_width-i-1] = int(binary_string[length-i-1])\n",
    "    return bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSetBits(n):\n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEQuantize(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, enc_quantize_level = 2, enc_value_limit = 1.0, enc_grad_limit = 0.01, enc_clipping = 'both'):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        assert enc_clipping in ['both', 'inputs']\n",
    "        ctx.enc_clipping = enc_clipping\n",
    "        ctx.enc_value_limit = enc_value_limit\n",
    "        ctx.enc_quantize_level = enc_quantize_level\n",
    "        ctx.enc_grad_limit = enc_grad_limit\n",
    "\n",
    "        x_lim_abs = enc_value_limit\n",
    "        x_lim_range = 2.0 * x_lim_abs\n",
    "        x_input_norm = torch.clamp(inputs, -x_lim_abs, x_lim_abs)\n",
    "\n",
    "        if enc_quantize_level == 2:\n",
    "            outputs_int = torch.sign(x_input_norm)\n",
    "        else:\n",
    "            outputs_int = torch.round((x_input_norm +x_lim_abs) * ((enc_quantize_level - 1.0)/x_lim_range)) * x_lim_range/(enc_quantize_level - 1.0) - x_lim_abs\n",
    "\n",
    "        return outputs_int\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.enc_clipping in ['inputs', 'both']:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input>ctx.enc_value_limit]=0\n",
    "            grad_output[input<-ctx.enc_value_limit]=0\n",
    "\n",
    "        if ctx.enc_clipping in ['gradient', 'both']:\n",
    "            grad_output = torch.clamp(grad_output, -ctx.enc_grad_limit, ctx.enc_grad_limit)\n",
    "        grad_input = grad_output.clone()\n",
    "\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d695a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'tanh':\n",
    "        return F.tanh\n",
    "    elif activation == 'elu':\n",
    "        return F.elu\n",
    "    elif activation == 'relu':\n",
    "        return F.relu\n",
    "    elif activation == 'selu':\n",
    "        return F.selu\n",
    "    elif activation == 'sigmoid':\n",
    "        return F.sigmoid\n",
    "    elif activation == 'gelu':\n",
    "        return F.gelu\n",
    "    elif activation == 'silu':\n",
    "        return F.silu\n",
    "    elif activation == 'mish':\n",
    "        return F.mish\n",
    "    elif activation == 'linear':\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Activation function {activation} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class g_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, depth=3, skip_depth=1, skip_layer=1, ell=2, activation='selu', use_skip=False, augment=False):\n",
    "        super(g_Full, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.ell = ell\n",
    "        self.ell_input_size = input_size//self.ell\n",
    "        self.augment = augment\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.skip_depth = skip_depth\n",
    "        self.skip_layer = skip_layer\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            self.skip = nn.ModuleList([nn.Linear(self.input_size + self.output_size, self.hidden_size, bias=True)])\n",
    "            self.skip.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.skip_depth)])\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        self.linears.extend([nn.Linear(self.hidden_size, self.hidden_size, bias=True) for ii in range(1, self.depth)])\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_augment(msg, ell):\n",
    "        u = msg.clone()\n",
    "        n = int(np.log2(ell))\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, ell, 2*num_bits):\n",
    "                if len(u.shape) == 2:\n",
    "                    u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                elif len(u.shape) == 3:\n",
    "                    u = torch.cat((u[:, :, :i], u[:, :, i:i+num_bits].clone() * u[:, :, i+num_bits: i+2*num_bits], u[:, :, i+num_bits:]), dim=2)\n",
    "\n",
    "        if len(u.shape) == 3:\n",
    "            return u[:, :, :-1]\n",
    "        elif len(u.shape) == 2:\n",
    "            return u[:, :-1]\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = y.clone()\n",
    "        for ii, layer in enumerate(self.linears):\n",
    "            if ii != self.depth:\n",
    "                x = self.activation_fn(layer(x))\n",
    "                if self.use_skip and ii == self.skip_layer:\n",
    "                    if len(x.shape) == 3:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=2)\n",
    "                    elif len(x.shape) == 2:\n",
    "                        skip_input = torch.cat([y, g_Full.get_augment(y, self.ell)], dim=1)\n",
    "                    for jj, skip_layer in enumerate(self.skip):\n",
    "                        skip_input = self.activation_fn(skip_layer(skip_input))\n",
    "                    x = x + skip_input\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if self.augment:\n",
    "                    x = x + g_Full.get_augment(y, self.ell)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d72065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be: (batch_size, seq_len, hidden_dim)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class f_Full(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0., activation='selu', depth=3, use_norm=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.use_norm = use_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "\n",
    "        # Initial layers same as original f_Full\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.input_size, self.hidden_size, bias=True)])\n",
    "        if self.use_norm:\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(self.hidden_size)])\n",
    "        \n",
    "        # Attention layer after first linear\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,  # Reduced number of heads\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Remaining layers same as original\n",
    "        for ii in range(1, self.depth):\n",
    "            self.linears.append(nn.Linear(self.hidden_size, self.hidden_size, bias=True))\n",
    "            if self.use_norm:\n",
    "                self.norms.append(nn.LayerNorm(self.hidden_size))\n",
    "        self.linears.append(nn.Linear(self.hidden_size, self.output_size, bias=True))\n",
    "\n",
    "    def forward(self, y, aug=None):\n",
    "        x = y.clone()\n",
    "        \n",
    "        # First linear layer\n",
    "        x = self.linears[0](x)\n",
    "        if self.use_norm:\n",
    "            x = self.norms[0](x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        # Reshape for attention: [batch, seq_len, hidden]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = attn_out if len(y.shape) == 3 else attn_out.squeeze(1)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for ii in range(1, len(self.linears)):\n",
    "            if ii != self.depth:\n",
    "                x = self.linears[ii](x)\n",
    "                if self.use_norm:\n",
    "                    x = self.norms[ii](x)\n",
    "                x = self.activation_fn(x)\n",
    "            else:\n",
    "                x = self.linears[ii](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10845154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        try:\n",
    "            m.bias.data.fill_(0.)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e38e3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(actions):\n",
    "    inds = (0.5 + 0.5*actions).long()\n",
    "    return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f46",
   "metadata": {},
   "source": [
    "# Part 2: Core PolarCode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da23a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarCode:\n",
    "\n",
    "    def __init__(self, n, K, Fr = None, rs = None, use_cuda = True, infty = 1000., hard_decision = False, lse = 'lse'):\n",
    "\n",
    "        assert n>=1\n",
    "        self.n = n\n",
    "        self.N = 2**n\n",
    "        self.K = K\n",
    "        self.G2 = np.array([[1,1],[0,1]])\n",
    "        self.G = np.array([1])\n",
    "        for i in range(n):\n",
    "            self.G = np.kron(self.G, self.G2)\n",
    "        self.G = torch.from_numpy(self.G).float()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        self.infty = infty\n",
    "        self.hard_decision = hard_decision\n",
    "        self.lse = lse\n",
    "\n",
    "        if Fr is not None:\n",
    "            assert len(Fr) == self.N - self.K\n",
    "            self.frozen_positions = Fr\n",
    "            self.unsorted_frozen_positions = self.frozen_positions\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "            self.info_positions = np.array(list(set(self.frozen_positions) ^ set(np.arange(self.N))))\n",
    "            self.unsorted_info_positions = self.info_positions\n",
    "            self.info_positions.sort()\n",
    "            \n",
    "        else:\n",
    "            if rs is None:\n",
    "                # in increasing order of reliability\n",
    "                self.reliability_seq = np.arange(1023, -1, -1)\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "            else:\n",
    "                self.reliability_seq = rs\n",
    "                self.rs = self.reliability_seq[self.reliability_seq<self.N]\n",
    "\n",
    "                assert len(self.rs) == self.N\n",
    "            # best K bits\n",
    "            self.info_positions = self.rs[:self.K]\n",
    "            self.unsorted_info_positions = self.reliability_seq[self.reliability_seq<self.N][:self.K]\n",
    "            self.info_positions.sort()\n",
    "            self.unsorted_info_positions=np.flip(self.unsorted_info_positions)\n",
    "            # worst N-K bits\n",
    "            self.frozen_positions = self.rs[self.K:]\n",
    "            self.unsorted_frozen_positions = self.rs[self.K:]\n",
    "            self.frozen_positions.sort()\n",
    "\n",
    "\n",
    "            self.CRC_polynomials = {\n",
    "            3: torch.Tensor([1, 0, 1, 1]).int(),\n",
    "            8: torch.Tensor([1, 1, 1, 0, 1, 0, 1, 0, 1]).int(),\n",
    "            16: torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]).int(),\n",
    "                                    }\n",
    "\n",
    "    def get_G(self, ell):\n",
    "        n = int(np.log2(ell))\n",
    "        G = np.array([1])\n",
    "        for i in range(n):\n",
    "            G = np.kron(G, self.G2)\n",
    "        return G\n",
    "\n",
    "    def encode_plotkin(self, message, scaling = None, custom_info_positions = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "        if custom_info_positions is not None:\n",
    "            info_positions = custom_info_positions\n",
    "        else:\n",
    "            info_positions = self.info_positions\n",
    "        u = torch.ones(message.shape[0], self.N, dtype=torch.float).to(message.device)\n",
    "        u[:, info_positions] = message\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "                # u[:, i:i+num_bits] = u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits].clone\n",
    "        if scaling is not None:\n",
    "            u = (scaling * np.sqrt(self.N)*u)/torch.norm(scaling)\n",
    "        return u\n",
    "    \n",
    "    def channel(self, code, snr, noise_type = 'awgn', vv =5.0, radar_power = 20.0, radar_prob = 5e-2):\n",
    "        if noise_type != \"bsc\":\n",
    "            sigma = snr_db2sigma(snr)\n",
    "        else:\n",
    "            sigma = snr\n",
    "\n",
    "        r = corrupt_signal(code, sigma, noise_type, vv, radar_power, radar_prob)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def define_partial_arrays(self, llrs):\n",
    "        # Initialize arrays to store llrs and partial_sums useful to compute the partial successive cancellation process.\n",
    "        llr_array = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        llr_array[:, self.n] = llrs\n",
    "        partial_sums = torch.zeros(llrs.shape[0], self.n+1, self.N, device=llrs.device)\n",
    "        return llr_array, partial_sums\n",
    "\n",
    "\n",
    "    def updateLLR(self, leaf_position, llrs, partial_llrs = None, prior = None):\n",
    "\n",
    "        #START\n",
    "        depth = self.n\n",
    "        decoded_bits = partial_llrs[:,0].clone()\n",
    "        if prior is None:\n",
    "            prior = torch.zeros(self.N) #priors\n",
    "        llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth, 0, leaf_position, prior, decoded_bits)\n",
    "        return llrs, decoded_bits\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def partial_decode(self, llrs, partial_llrs, depth, bit_position, leaf_position, prior, decoded_bits=None):\n",
    "        # Function to call recursively, for partial SC decoder.\n",
    "        # We are assuming that u_0, u_1, .... , u_{leaf_position -1} bits are known.\n",
    "        # Partial sums computes the sums got through Plotkin encoding operations of known bits, to avoid recomputation.\n",
    "        # this function is implemented for rate 1 (not accounting for frozen bits in polar SC decoding)\n",
    "\n",
    "        # print(\"DEPTH = {}, bit_position = {}\".format(depth, bit_position))\n",
    "        half_index = 2 ** (depth - 1)\n",
    "        leaf_position_at_depth = leaf_position // 2**(depth-1) # will tell us whether left_child or right_child\n",
    "\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            # Left child\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position:left_bit_position+1]\n",
    "            elif leaf_position_at_depth == left_bit_position:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                elif self.lse == 'lse':\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]).sum(dim=1, keepdim=True)\n",
    "                #print(Lu.device, prior.device, torch.ones_like(Lu).device)\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu + prior[left_bit_position]*torch.ones_like(Lu)\n",
    "                if self.hard_decision:\n",
    "                    u_hat = torch.sign(Lu)\n",
    "                else:\n",
    "                    u_hat = torch.tanh(Lu/2)\n",
    "\n",
    "                decoded_bits[:, left_bit_position] = u_hat.squeeze(1)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # Right child\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "            if leaf_position_at_depth > right_bit_position:\n",
    "                pass\n",
    "            elif leaf_position_at_depth == right_bit_position:\n",
    "                Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "                llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv + prior[right_bit_position] * torch.ones_like(Lv)\n",
    "                if self.hard_decision:\n",
    "                    v_hat = torch.sign(Lv)\n",
    "                else:\n",
    "                    v_hat = torch.tanh(Lv/2)\n",
    "                decoded_bits[:, right_bit_position] = v_hat.squeeze(1)\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            # LEFT CHILD\n",
    "            # Find likelihood of (u xor v) xor (v) = u\n",
    "            # Lu = log_sum_exp(torch.cat([llrs[:, :half_index].unsqueeze(2), llrs[:, half_index:].unsqueeze(2)], dim=2).permute(0, 2, 1))\n",
    "\n",
    "            left_bit_position = 2*bit_position\n",
    "            if leaf_position_at_depth > left_bit_position:\n",
    "                Lu = llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "                u_hat = partial_llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index]\n",
    "            else:\n",
    "                if self.lse == 'minsum':\n",
    "                    Lu = min_sum_log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                elif self.lse == 'lse':\n",
    "                    # Lu = log_sum_avoid_zero_NaN(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "                    Lu = log_sum_exp(llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index], llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index])\n",
    "\n",
    "                llrs[:, depth-1, left_bit_position*half_index:(left_bit_position+1)*half_index] = Lu\n",
    "                llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, left_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "                return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "            # RIGHT CHILD\n",
    "            right_bit_position = 2*bit_position + 1\n",
    "\n",
    "            Lv = u_hat * llrs[:, depth, left_bit_position*half_index:(left_bit_position+1)*half_index] + llrs[:,depth, (left_bit_position+1)*half_index:(left_bit_position+2)*half_index]\n",
    "            llrs[:, depth-1, right_bit_position*half_index:(right_bit_position+1)*half_index] = Lv\n",
    "            llrs, partial_llrs, decoded_bits = self.partial_decode(llrs, partial_llrs, depth-1, right_bit_position, leaf_position, prior, decoded_bits)\n",
    "\n",
    "            return llrs, partial_llrs, decoded_bits\n",
    "\n",
    "    def updatePartialSums(self, leaf_position, decoded_bits, partial_llrs):\n",
    "\n",
    "        u = decoded_bits.clone()\n",
    "        u[:, leaf_position+1:] = 0\n",
    "\n",
    "        for d in range(0, self.n):\n",
    "            partial_llrs[:, d] = u\n",
    "            num_bits = 2**d\n",
    "            for i in np.arange(0, self.N, 2*num_bits):\n",
    "                # [u v] encoded to [u xor(u,v)]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        partial_llrs[:, self.n] = u\n",
    "        return partial_llrs\n",
    "\n",
    "    def sc_decode_new(self, corrupted_codewords, snr, use_gt = None, channel = 'awgn'):\n",
    "\n",
    "        assert channel in ['awgn', 'bsc']\n",
    "\n",
    "        if channel == 'awgn':\n",
    "            noise_sigma = snr_db2sigma(snr)\n",
    "            llrs = (2/noise_sigma**2)*corrupted_codewords\n",
    "        elif channel == 'bsc':\n",
    "            # snr refers to transition prob\n",
    "            p = (torch.ones(1)*(snr + 1e-9)).to(corrupted_codewords.device)\n",
    "            llrs = (torch.clip(torch.log((1 - p) / p), -10000, 10000) * (corrupted_codewords + 1) - torch.clip(torch.log(p / (1-p)), -10000, 10000) * (corrupted_codewords - 1))/2\n",
    "\n",
    "        # step-wise implementation using updateLLR and updatePartialSums\n",
    "\n",
    "        priors = torch.zeros(self.N)\n",
    "        priors[self.frozen_positions] = self.infty\n",
    "\n",
    "        u_hat = torch.zeros(corrupted_codewords.shape[0], self.N, device=corrupted_codewords.device)\n",
    "        llr_array, partial_llrs = self.define_partial_arrays(llrs)\n",
    "        for ii in range(self.N):\n",
    "            #start = time.time()\n",
    "            llr_array , decoded_bits = self.updateLLR(ii, llr_array.clone(), partial_llrs, priors)\n",
    "            #print('SC update : {}'.format(time.time() - start), corrupted_codewords.shape[0])\n",
    "            if use_gt is None:\n",
    "                u_hat[:, ii] = torch.sign(llr_array[:, 0, ii])\n",
    "            else:\n",
    "                u_hat[:, ii] = use_gt[:, ii]\n",
    "            #start = time.time()\n",
    "            partial_llrs = self.updatePartialSums(ii, u_hat, partial_llrs)\n",
    "            #print('SC partial: {}s, {}', time.time() - start, 'frozen' if ii in self.frozen_positions else 'info')\n",
    "        decoded_bits = u_hat[:, self.info_positions]\n",
    "        return llr_array[:, 0, :].clone(), decoded_bits\n",
    "\n",
    "    def get_CRC(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # inout message should be int\n",
    "\n",
    "        padded_bits = torch.cat([message, torch.zeros(self.CRC_len).int().to(message.device)])\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + self.CRC_len + 1] = padded_bits[cur_shift: cur_shift + self.CRC_len + 1] ^ self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        return padded_bits[self.K_minus_CRC:]\n",
    "\n",
    "    def CRC_check(self, message):\n",
    "\n",
    "        # need to optimize.\n",
    "        # input message should be int\n",
    "\n",
    "        padded_bits = message\n",
    "        while len(padded_bits[0:self.K_minus_CRC].nonzero()):\n",
    "            cur_shift = (padded_bits != 0).int().argmax(0)\n",
    "            padded_bits[cur_shift: cur_shift + polar.CRC_len + 1] ^= self.CRC_polynomials[self.CRC_len].to(message.device)\n",
    "\n",
    "        if padded_bits[self.K_minus_CRC:].sum()>0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def encode_with_crc(self, message, CRC_len):\n",
    "        self.CRC_len = CRC_len\n",
    "        self.K_minus_CRC = self.K - CRC_len\n",
    "\n",
    "        if CRC_len == 0:\n",
    "            return self.encode_plotkin(message)\n",
    "        else:\n",
    "            crcs = 1-2*torch.vstack([self.get_CRC((0.5+0.5*message[jj]).int()) for jj in range(message.shape[0])])\n",
    "            encoded = self.encode_plotkin(torch.cat([message, crcs], 1))\n",
    "\n",
    "            return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6d51",
   "metadata": {},
   "source": [
    "# Part 3: DeepPolar Class and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPolar(PolarCode):\n",
    "    def __init__(self, device, N, K, ell = 2, infty = 1000., depth_map : defaultdict = None):\n",
    "\n",
    "        # rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        # Frozen = np.argsort(rmweight)[:-K]\n",
    "        # Frozen.sort()\n",
    "\n",
    "        #self.args = args\n",
    "        Fr = get_frozen(N, K, rate_profile)\n",
    "        super().__init__(n = int(np.log2(N)), K = K, Fr=Fr,  infty = infty)\n",
    "        self.N = N\n",
    "\n",
    "        if depth_map is not None:\n",
    "            # depth map is a dict, product of values should be equal to N\n",
    "            assert np.prod(list(depth_map.values())) == N\n",
    "            # assert that keys od depth map start from one and go continuosly till some point \n",
    "            assert min(list(depth_map.keys())) == 1\n",
    "            assert max(list(depth_map.keys())) <= int(np.log2(N))\n",
    "            self.ell = None\n",
    "            self.n_ell = len(depth_map.keys())\n",
    "            assert max(list(depth_map.keys())) == self.n_ell\n",
    "\n",
    "            self.depth_map = depth_map\n",
    "        else:\n",
    "            self.ell = ell\n",
    "            self.n_ell = int(np.log(N)/np.log(self.ell))\n",
    "\n",
    "            self.depth_map = defaultdict(int)\n",
    "            for d in range(1, self.n_ell+1):\n",
    "                self.depth_map[d] = self.ell\n",
    "            assert np.prod(list(self.depth_map.values())) == N\n",
    "\n",
    "        self.device = device\n",
    "        self.fnet_dict = None\n",
    "        self.gnet_dict = None\n",
    "\n",
    "        self.infty = infty\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot(actions):\n",
    "        inds = (0.5 + 0.5*actions).long()\n",
    "        if len(actions.shape) == 2:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], -1)\n",
    "        elif len(actions.shape) == 3:\n",
    "            return torch.eye(2, device = inds.device)[inds].reshape(actions.shape[0], actions.shape[1], -1)\n",
    "\n",
    "    def define_kernel_nns(self, ell, unfrozen = None, fnet = 'KO', gnet = 'KO', shared = False):\n",
    "\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "        #dec_hidden_size = dec_hidden_size\n",
    "        #enc_hidden_size = enc_hidden_size\n",
    "\n",
    "        depth = 1\n",
    "        assert len(unfrozen) > 0, \"No unfrozen bits!\"\n",
    "\n",
    "        self.fnet_dict[depth] = {}\n",
    "\n",
    "        if fnet == 'KO_parallel' or fnet == 'KO_last_parallel':\n",
    "            bit_position = 0\n",
    "                   \n",
    "            self.fnet_dict[depth][bit_position] = {}\n",
    "            # input_size = self.N if depth == self.n_ell else self.N // int(np.prod([self.depth_map[d] for d in range(depth+1, self.n_ell+1)]))\n",
    "            input_size = ell             \n",
    "            # For curriculum, only for lowest depth.\n",
    "            output_size = ell#len(unfrozen)\n",
    "            self.fnet_dict[depth][bit_position] = f_Full(input_size, dec_hidden_size, output_size, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    " \n",
    "        elif 'KO' in fnet:\n",
    "            if shared:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for current_position in range(ell):\n",
    "                    self.fnet_dict[depth][current_position] = f_Full(ell + current_position, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                for current_position in unfrozen:\n",
    "                    if not self.fnet_dict[depth].get(bit_position):\n",
    "                        self.fnet_dict[depth][bit_position] = {}\n",
    "                    input_size = ell + (int(onehot)+1)*current_position\n",
    "                    self.fnet_dict[depth][bit_position][current_position] = f_Full(input_size, dec_hidden_size, 1, activation = dec_activation, dropout_p = dropout_p, depth = f_depth, use_norm = use_norm).to(self.device)\n",
    "                \n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict[depth] = {}\n",
    "            if shared:\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "            else:\n",
    "                bit_position = 0\n",
    "                if gnet == 'KO':\n",
    "                    self.gnet_dict[depth][bit_position] = g_Full(ell, enc_hidden_size, ell-1, depth = g_depth, skip_depth = g_skip_depth, skip_layer = g_skip_layer, ell = ell, activation = enc_activation, use_skip = use_skip).to(self.device)\n",
    "\n",
    "    def define_and_load_nns(self, ell, kernel_load_path=None, fnet='KO', gnet='KO', shared=True, dataparallel=False):\n",
    "        # Initialize decoder and encoder dictionaries\n",
    "        if 'KO' in fnet:\n",
    "            self.fnet_dict = {}\n",
    "        else:\n",
    "            self.fnet_dict = None\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if 'KO' in gnet:\n",
    "            self.gnet_dict = {}\n",
    "        else:\n",
    "            self.gnet_dict = None\n",
    "\n",
    "        # Loop through each depth level\n",
    "        for depth in range(self.n_ell, 0, -1):\n",
    "            if depth in polar_depths:\n",
    "                continue\n",
    "\n",
    "            ell = self.depth_map[depth]\n",
    "            proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "            # Handle parallel decoder case\n",
    "            if fnet == 'KO_last_parallel' and depth == 1:\n",
    "                self.fnet_dict[depth] = {}\n",
    "                for bit_position in range(self.N // proj_size):\n",
    "                    proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                    get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                    num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                    subproj_len = len(proj) // ell\n",
    "                    subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                    num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                    unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                    input_size = ell             \n",
    "                    output_size = ell\n",
    "\n",
    "                    # Use attention-enhanced decoder for parallel case\n",
    "                    self.fnet_dict[depth][bit_position] = f_Full(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=dec_hidden_size,\n",
    "                        output_size=output_size,\n",
    "                        activation=dec_activation,\n",
    "                        dropout_p=dropout_p,\n",
    "                        depth=f_depth,\n",
    "                        use_norm=use_norm\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    # Load pretrained weights if available\n",
    "                    if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                        try:\n",
    "                            ckpt = torch.load(os.path.join(kernel_load_path + '_parallel', f'{ell}_{len(unfrozen)}.pt'))\n",
    "                            self.fnet_dict[depth][bit_position].load_state_dict(ckpt[0][1][0].state_dict())\n",
    "                        except FileNotFoundError:\n",
    "                            print(f\"Parallel File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                            pass\n",
    "\n",
    "                    if dataparallel:\n",
    "                        self.fnet_dict[depth][bit_position] = nn.DataParallel(self.fnet_dict[depth][bit_position])\n",
    "\n",
    "            # Handle sequential decoder case\n",
    "            elif 'KO' in fnet:\n",
    "                self.fnet_dict[depth] = {}\n",
    "\n",
    "                if shared:\n",
    "                    # Shared decoder network for all positions\n",
    "                    for current_position in range(ell):\n",
    "                        self.fnet_dict[depth][current_position] = f_Full(\n",
    "                            input_size=ell + current_position,\n",
    "                            hidden_size=dec_hidden_size,\n",
    "                            output_size=1,\n",
    "                            activation=dec_activation,\n",
    "                            dropout_p=dropout_p,\n",
    "                            depth=f_depth,\n",
    "                            use_norm=use_norm\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        if dataparallel:\n",
    "                            self.fnet_dict[depth][current_position] = nn.DataParallel(self.fnet_dict[depth][current_position])\n",
    "\n",
    "                else:\n",
    "                    # Individual decoder networks for each position\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                        num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                        subproj_len = len(proj) // ell\n",
    "                        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                        unfrozen = [i for i, x in enumerate(num_info_in_subproj) if x >= 1]\n",
    "\n",
    "                        # Load pretrained weights if available\n",
    "                        ckpt_exists = False\n",
    "                        if len(unfrozen) > 0 and kernel_load_path is not None:\n",
    "                            try:\n",
    "                                ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                ckpt_exists = True\n",
    "                            except FileNotFoundError:\n",
    "                                print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                pass\n",
    "\n",
    "                        # Create decoders for unfrozen positions\n",
    "                        for current_position in unfrozen:\n",
    "                            if not self.fnet_dict[depth].get(bit_position):\n",
    "                                self.fnet_dict[depth][bit_position] = {}\n",
    "\n",
    "                            input_size = ell + (int(onehot)+1)*current_position\n",
    "                            output_size = 1\n",
    "\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = f_Full(\n",
    "                                input_size=input_size,\n",
    "                                hidden_size=dec_hidden_size,\n",
    "                                output_size=output_size,\n",
    "                                activation=dec_activation,\n",
    "                                dropout_p=dropout_p,\n",
    "                                depth=f_depth,\n",
    "                                use_norm=use_norm\n",
    "                            ).to(self.device)\n",
    "\n",
    "                            if ckpt_exists:\n",
    "                                try:\n",
    "                                    f_ckpt = ckpt[0][1][0][current_position].state_dict()\n",
    "                                    self.fnet_dict[depth][bit_position][current_position].load_state_dict(f_ckpt)\n",
    "                                except:\n",
    "                                    print(f\"Warning: Could not load weights for position {current_position}\")\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.fnet_dict[depth][bit_position][current_position] = nn.DataParallel(\n",
    "                                    self.fnet_dict[depth][bit_position][current_position]\n",
    "                                )\n",
    "\n",
    "            # Handle encoder network\n",
    "            if 'KO' in gnet:\n",
    "                self.gnet_dict[depth] = {}\n",
    "                if shared:\n",
    "                    if gnet == 'KO':\n",
    "                        if not dataparallel:\n",
    "                            self.gnet_dict[depth] = g_Full(\n",
    "                                ell, enc_hidden_size, ell-1,\n",
    "                                depth=g_depth,\n",
    "                                skip_depth=g_skip_depth,\n",
    "                                skip_layer=g_skip_layer,\n",
    "                                ell=ell,\n",
    "                                use_skip=use_skip\n",
    "                            ).to(self.device)\n",
    "                        else:\n",
    "                            self.gnet_dict[depth] = nn.DataParallel(\n",
    "                                g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    use_skip=use_skip\n",
    "                                )\n",
    "                            ).to(self.device)\n",
    "                else:\n",
    "                    for bit_position in range(self.N // proj_size):\n",
    "                        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                        num_info_in_proj = sum([int(x in self.info_positions) for x in proj])\n",
    "\n",
    "                        if num_info_in_proj > 0:\n",
    "                            if gnet == 'KO':\n",
    "                                self.gnet_dict[depth][bit_position] = g_Full(\n",
    "                                    ell, enc_hidden_size, ell-1,\n",
    "                                    depth=g_depth,\n",
    "                                    skip_depth=g_skip_depth,\n",
    "                                    skip_layer=g_skip_layer,\n",
    "                                    ell=ell,\n",
    "                                    activation=enc_activation,\n",
    "                                    use_skip=use_skip\n",
    "                                ).to(self.device)\n",
    "\n",
    "                            # Load pretrained weights if available\n",
    "                            if kernel_load_path is not None:\n",
    "                                try:\n",
    "                                    ckpt = torch.load(os.path.join(kernel_load_path, f'{ell}_{len(unfrozen)}.pt'))\n",
    "                                    self.gnet_dict[depth][bit_position].load_state_dict(ckpt[1][1][0].state_dict())\n",
    "                                except FileNotFoundError:\n",
    "                                    print(f\"File not found for ell = {ell}, num_unfrozen = {len(unfrozen)}\")\n",
    "                                    pass\n",
    "\n",
    "                            if dataparallel:\n",
    "                                self.gnet_dict[depth][bit_position] = nn.DataParallel(self.gnet_dict[depth][bit_position])\n",
    "\n",
    "        if kernel_load_path is not None:\n",
    "            print(\"Loaded kernel from \", kernel_load_path)\n",
    "\n",
    "    def load_nns(self, fnet_dict, gnet_dict = None, shared = False):\n",
    "        self.fnet_dict = fnet_dict\n",
    "        self.gnet_dict = gnet_dict\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if self.fnet_dict is not None:\n",
    "                for bit_position in self.fnet_dict[depth].keys():\n",
    "                    if not isinstance(self.fnet_dict[depth][bit_position], dict):#shared or decoder_type == 'KO_parallel' or decoder_type == 'KO_RNN':\n",
    "                        self.fnet_dict[depth][bit_position].to(self.device)\n",
    "                    else:\n",
    "                        for current_position in self.fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "            if gnet_dict is not None:\n",
    "                if shared:\n",
    "                    self.gnet_dict[depth].to(self.device)\n",
    "                else:\n",
    "                    for bit_position in self.gnet_dict[depth].keys():\n",
    "                        self.gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def load_partial_nns(self, fnet_dict, gnet_dict = None):\n",
    "\n",
    "        for depth in fnet_dict.keys():\n",
    "            if fnet_dict is not None:\n",
    "                for bit_position in fnet_dict[depth].keys():\n",
    "                    if isinstance(fnet_dict[depth][bit_position], dict):\n",
    "                        for current_position in fnet_dict[depth][bit_position].keys():\n",
    "                            self.fnet_dict[depth][bit_position][current_position] = fnet_dict[depth][bit_position][current_position].to(self.device)\n",
    "                    else:\n",
    "                        self.fnet_dict[depth][bit_position] = fnet_dict[depth][bit_position].to(self.device)\n",
    "\n",
    "            if gnet_dict is not None:\n",
    "                for bit_position in gnet_dict[depth].keys():\n",
    "                    self.gnet_dict[depth][bit_position] = gnet_dict[depth][bit_position].to(self.device)\n",
    "        print(\"NN weights loaded!\")\n",
    "\n",
    "    def kernel_encode(self, ell, gnet, msg_bits, info_positions, binary = False):\n",
    "        input_shape = msg_bits.shape[-1]\n",
    "        assert input_shape <= ell\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, info_positions] = msg_bits\n",
    "        output =torch.cat([gnet(u.unsqueeze(1)).squeeze(1), u[:, -1:]], 1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(output)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def deeppolar_encode(self, msg_bits, binary = False):\n",
    "        u = torch.ones(msg_bits.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        u[:, self.info_positions] = msg_bits\n",
    "        for d in range(1, self.n_ell+1):\n",
    "            # num_bits = self.ell**(d-1)\n",
    "            num_bits = np.prod([self.depth_map[dd] for dd in range(1, d)]) if d > 1 else 1\n",
    "            # proj_size = self.ell**(d)\n",
    "            proj_size = np.prod([self.depth_map[dd] for dd in range(1, d+1)])\n",
    "            ell = self.depth_map[d]\n",
    "            for bit_position, i in enumerate(np.arange(0, self.N, ell*num_bits)):\n",
    "\n",
    "                # [u v] encoded to [(u xor v),v)]\n",
    "                proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "                get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "                num_info_in_proj = get_num_info_proj(proj)\n",
    "\n",
    "                subproj_len = len(proj) // ell\n",
    "                subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "                num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "                num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "                \n",
    "                if num_info_in_proj > 0:\n",
    "                    info_bits_present = True          \n",
    "                else:\n",
    "                    info_bits_present = False         \n",
    "                if d in polar_depths:\n",
    "                    info_bits_present = False\n",
    "\n",
    "                enc_chunks = []\n",
    "                ell = self.depth_map[d]\n",
    "                for j in range(ell):\n",
    "                    chunk = u[:, i + j*num_bits:i + (j+1)*num_bits].unsqueeze(2).clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[d](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        output = torch.cat([self.gnet_dict[d][bit_position](concatenated_chunks), u[:, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(msg_bits.shape[0], -1, 1).squeeze(2)\n",
    "\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                u = torch.cat((u[:, :i], output, u[:, i + ell*num_bits:]), dim=1)\n",
    "\n",
    "        power_constrained_u = self.power_constraint(u)\n",
    "        if binary:\n",
    "            stequantize = STEQuantize.apply\n",
    "            power_constrained_u = stequantize(power_constrained_u)\n",
    "        return power_constrained_u\n",
    "\n",
    "    def power_constraint(self, codewords):\n",
    "        return F.normalize(codewords, p=2, dim=1)*np.sqrt(self.N)\n",
    "\n",
    "    def encode_chunks_plotkin(self, enc_chunks, ell = None):\n",
    "\n",
    "        # message shape is (batch, k)\n",
    "        # BPSK convention : 0 -> +1, 1 -> -1\n",
    "        # Therefore, xor(a, b) = a*b\n",
    "\n",
    "        # to change for other kernels\n",
    "\n",
    "        if ell is None:\n",
    "            ell = self.ell\n",
    "        assert len(enc_chunks) == ell\n",
    "        chunk_size = enc_chunks[0].shape[1]\n",
    "        batch_size = enc_chunks[0].shape[0]\n",
    "\n",
    "        u = torch.cat(enc_chunks, 1).squeeze(2)\n",
    "        n = int(np.log2(ell))\n",
    "\n",
    "        for d in range(0, n):\n",
    "            num_bits = 2**d * chunk_size\n",
    "            for i in np.arange(0, chunk_size*ell, 2*num_bits):\n",
    "                # [u v] encoded to [(u,v) xor v]\n",
    "                u = torch.cat((u[:, :i], u[:, i:i+num_bits].clone() * u[:, i+num_bits: i+2*num_bits], u[:, i+num_bits:]), dim=1)\n",
    "        return u\n",
    "            \n",
    "    def deeppolar_parallel_decode(self, noisy_code):\n",
    "        # Successive cancellation decoder for polar codes\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "        decoded_llrs  = self.KO_parallel_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "\n",
    "    def deeppolar_parallel_decode_depth(self, llrs, depth, bit_position, decoded_llrs):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        dec_chunks = torch.cat([llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "        Lu = self.fnet_dict[depth][bit_position](dec_chunks)\n",
    "\n",
    "        if depth == 1:\n",
    "            u = torch.tanh(Lu/2)\n",
    "            decoded_llrs[:, left_bit_position + unfrozen] = Lu.squeeze(1)\n",
    "        else:\n",
    "            for index, current_position in enumerate(unfrozen):\n",
    "                bit_position_offset = left_bit_position + current_position                \n",
    "                decoded_llrs = self.deeppolar_parallel_decode_depth(Lu[:, :, index:index+1], depth-1, bit_position_offset, decoded_llrs)\n",
    "\n",
    "        return decoded_llrs\n",
    "            \n",
    "    def deeppolar_decode(self, noisy_code):\n",
    "        assert noisy_code.shape[1] == self.N\n",
    "\n",
    "        depth = self.n_ell\n",
    "\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        \n",
    "        # don't want to go into useless frozen subtrees.\n",
    "        partial_sums = torch.ones(noisy_code.shape[0], self.n_ell+1, self.N, device=noisy_code.device)\n",
    "\n",
    "        # function is recursively called (DFS)\n",
    "        # arguments: Beliefs at the input of node (LLRs at top node), depth of children, bit_position (zero at top node)\n",
    "\n",
    "        decoded_llrs, partial_sums = self.deeppolar_decode_depth(noisy_code.unsqueeze(2), depth, 0, decoded_llrs, partial_sums)\n",
    "        decoded_llrs = decoded_llrs[:, self.info_positions]\n",
    "\n",
    "        return decoded_llrs, torch.sign(decoded_llrs)\n",
    "    \n",
    "    def deeppolar_decode_depth(self, llrs, depth, bit_position, decoded_llrs, partial_sums):\n",
    "        # Function to call recursively, for SC decoder\n",
    "\n",
    "        # half_index = self.ell ** (depth - 1)\n",
    "        half_index = np.prod([self.depth_map[d] for d in range(1, depth)]) if depth > 1 else 1\n",
    "        ell = self.depth_map[depth]\n",
    "        left_bit_position = self.depth_map[depth] *  bit_position \n",
    "\n",
    "        # Check if >1 information bits are present in the current projection. If not, don't use NNs - use polar encoding and minsum SC decoding.\n",
    "        # proj_size = self.ell**(depth)\n",
    "        # size of the projection of tht subtree\n",
    "        proj_size = np.prod([self.depth_map[d] for d in range(1, depth+1)])\n",
    "\n",
    "        # This chunk - finds infrozen positions in this kernel.\n",
    "        proj = np.arange(bit_position*proj_size, (bit_position+1)*proj_size)\n",
    "        get_num_info_proj = lambda proj : sum([int(x in self.info_positions) for x in proj])\n",
    "        get_info_proj = lambda proj : [x for x in proj if x in self.info_positions]\n",
    "\n",
    "        num_info_in_proj = get_num_info_proj(proj)\n",
    "        info_in_proj = get_info_proj(proj)\n",
    "\n",
    "        subproj_len = len(proj) // ell\n",
    "        subproj = [proj[i:i+subproj_len] for i in range(0, len(proj), subproj_len)]\n",
    "        num_info_in_subproj = [get_num_info_proj(x) for x in subproj]\n",
    "        num_nonzero_subproj = sum([int(x != 0) for x in num_info_in_subproj])\n",
    "        unfrozen = np.array([i for i, x in enumerate(num_info_in_subproj) if x >= 1])\n",
    "\n",
    "        if num_nonzero_subproj > 0:\n",
    "            info_bits_present = True      \n",
    "        else:\n",
    "            info_bits_present = False \n",
    "\n",
    "        if depth in polar_depths:\n",
    "            info_bits_present = False\n",
    "                \n",
    "        # This will be input to decoder\n",
    "        dec_chunks = [llrs[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "        # n = 2 tree case\n",
    "        if depth == 1:\n",
    "            if decoder_type == 'KO_last_parallel':\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                Lu = self.fnet_dict[depth][bit_position](concatenated_chunks)[:, 0, unfrozen]\n",
    "                u_hat = torch.tanh(Lu/2)\n",
    "                decoded_llrs[:, left_bit_position + unfrozen] = Lu\n",
    "                partial_sums[:, depth-1, left_bit_position + unfrozen] = u_hat\n",
    "\n",
    "            else:\n",
    "                for current_position in range(ell):\n",
    "                    bit_position_offset = left_bit_position + current_position\n",
    "                    if current_position > 0:\n",
    "                        # I am adding previously decoded bits . (either onehot or normal)\n",
    "                        if onehot:\n",
    "                            prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                        else:\n",
    "                            prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                        dec_chunks.append(prev_decoded)\n",
    "\n",
    "                    if bit_position_offset in self.frozen_positions: # frozen \n",
    "                        # don't update decoded llrs. It already has ones*prior.\n",
    "                        # actually don't need this. can skip.\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = torch.ones_like(partial_sums[:, depth-1, bit_position_offset])\n",
    "                    else: # information bit\n",
    "                        # This is the decoding.\n",
    "                        concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                        if self.shared:\n",
    "                            Lu = self.fnet_dict[depth][current_position](concatenated_chunks)\n",
    "                        else:\n",
    "                            Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "\n",
    "                        u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                        decoded_llrs[:, bit_position_offset] = Lu.squeeze(2).squeeze(1)\n",
    "                        partial_sums[:, depth-1, bit_position_offset] = u_hat.squeeze(1)\n",
    "\n",
    "            # Encoding back the decoded bits - for higher layers.\n",
    "            # # Compute decoded codeword\n",
    "            i = left_bit_position * half_index\n",
    "            # num_bits = self.ell**(depth-1)\n",
    "            num_bits = 1\n",
    "\n",
    "            enc_chunks = []\n",
    "            for j in range(ell):\n",
    "                chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                enc_chunks.append(chunk)\n",
    "            if info_bits_present:\n",
    "                concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                if 'KO' in encoder_type:\n",
    "                    if self.shared:\n",
    "                        output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    else:\n",
    "                        # bit position of the previous depth.\n",
    "                        output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                    output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            else:\n",
    "                output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "            partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "            \n",
    "            return decoded_llrs, partial_sums\n",
    "\n",
    "        # General case\n",
    "        else:\n",
    "            for current_position in range(ell):\n",
    "                bit_position_offset = left_bit_position + current_position\n",
    "\n",
    "                if current_position > 0:\n",
    "                    if onehot:\n",
    "                        prev_decoded = get_onehot(partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).sign()).detach().clone()\n",
    "                    else:\n",
    "                        prev_decoded = partial_sums[:, depth-1, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                    dec_chunks.append(prev_decoded)\n",
    "                concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "\n",
    "                if current_position in unfrozen:\n",
    "                    # General decoding ....\n",
    "                    # add the decoded bit here\n",
    "                    if self.shared:\n",
    "                        Lu = self.fnet_dict[depth][current_position](concatenated_chunks).squeeze(2)\n",
    "                    else:\n",
    "                        # if current_position == 0:\n",
    "                        #     Lu = self.fnet_dict[depth][bit_position][current_position](llrs)\n",
    "                        # else:\n",
    "                        Lu = self.fnet_dict[depth][bit_position][current_position](concatenated_chunks)\n",
    "                    decoded_llrs, partial_sums = self.deeppolar_decode_depth(Lu, depth-1, bit_position_offset, decoded_llrs, partial_sums)\n",
    "                else:\n",
    "                    Lu = self.infty*torch.ones_like(llrs)\n",
    "\n",
    "\n",
    "            # Compute decoded codeword\n",
    "            if depth < self.n_ell :\n",
    "                i = left_bit_position * half_index\n",
    "                # num_bits = self.ell**(depth-1)\n",
    "                num_bits = np.prod([self.depth_map[d] for d in range(1, depth)])\n",
    "                enc_chunks = []\n",
    "                for j in range(ell):\n",
    "                    chunk = torch.sign(partial_sums[:, depth-1, i + j*num_bits:i + (j+1)*num_bits]).unsqueeze(2).detach().clone()\n",
    "                    enc_chunks.append(chunk)\n",
    "                if info_bits_present:\n",
    "                    concatenated_chunks = torch.cat(enc_chunks, 2)\n",
    "                    if 'KO' in encoder_type:\n",
    "                        if self.shared:\n",
    "                            output = torch.cat([self.gnet_dict[depth](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        else:\n",
    "                            # bit position of the previous depth.\n",
    "                            output = torch.cat([self.gnet_dict[depth][bit_position](concatenated_chunks), partial_sums[:, depth-1, i + (ell-1)*num_bits:i + (ell)*num_bits].unsqueeze(2)], dim=2)\n",
    "                        output = output.permute(0,2,1).reshape(llrs.shape[0], -1, 1).squeeze(2)\n",
    "                    else:\n",
    "                        output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                else:\n",
    "                    output = self.encode_chunks_plotkin(enc_chunks, ell)\n",
    "                partial_sums[:, depth, i : i + num_bits*ell] = output.clone()\n",
    "\n",
    "                return decoded_llrs, partial_sums\n",
    "            else: # encoding not required for last level - we have already decoded all bits.\n",
    "                return decoded_llrs, partial_sums\n",
    "\n",
    "\n",
    "    def kernel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = [noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)]\n",
    "\n",
    "        for current_position in range(ell):\n",
    "            if current_position > 0:\n",
    "                if onehot:\n",
    "                    prev_decoded = get_onehot(u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone().sign()).detach().clone()\n",
    "                else:\n",
    "                    prev_decoded = u[:, (current_position -1)*half_index:(current_position)*half_index].unsqueeze(2).clone()\n",
    "                dec_chunks.append(prev_decoded)\n",
    "            if current_position in info_positions:\n",
    "                if current_position in info_positions:\n",
    "                    concatenated_chunks = torch.cat(dec_chunks, 2)\n",
    "                    Lu = fnet_dict[current_position](concatenated_chunks)\n",
    "                    decoded_llrs[:, current_position] = Lu.squeeze(2).squeeze(1)\n",
    "                    u_hat = torch.tanh(Lu/2).squeeze(2)\n",
    "                    u[:, current_position] = u_hat.squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n",
    "\n",
    "    def kernel_parallel_decode(self, ell, fnet_dict, noisy_code, info_positions = None):\n",
    "        input_shape = noisy_code.shape[-1]\n",
    "        noisy_code = noisy_code.unsqueeze(2)\n",
    "        assert input_shape == ell\n",
    "        u = torch.ones(noisy_code.shape[0], self.N, dtype=torch.float).to(self.device)\n",
    "        decoded_llrs = self.infty*torch.ones(noisy_code.shape[0], self.N, device = noisy_code.device)\n",
    "        half_index = 1\n",
    "        dec_chunks = torch.cat([noisy_code[:, (j)*half_index:(j+1)*half_index].clone() for j in range(ell)], 2)\n",
    "\n",
    "        decoded_llrs = fnet_dict(dec_chunks).squeeze(1)\n",
    "        u = torch.tanh(decoded_llrs/2).squeeze(1)\n",
    "        return decoded_llrs[:, info_positions], u[:, info_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d749",
   "metadata": {},
   "source": [
    "# Part 4: Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279f4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(polar, optimizer, scheduler, batch_size, train_snr, train_iters, criterion, device, info_positions, binary = False, noise_type = 'awgn'):\n",
    "\n",
    "    if N == polar.ell:\n",
    "        assert len(info_positions) == K\n",
    "        kernel = True \n",
    "    else:\n",
    "        kernel = False\n",
    "\n",
    "    for iter in range(train_iters):\n",
    "#         if batch_size > small_batch_size:\n",
    "#             small_batch_size = small_batch_size \n",
    "#         else:\n",
    "#             small_batch_size = batch_size\n",
    "\n",
    "        num_batches = batch_size // small_batch_size\n",
    "        for ii in range(num_batches):\n",
    "            msg_bits = 1 - 2*(torch.rand(small_batch_size, K) > 0.5).float().to(device)\n",
    "            if encoder_type == 'polar':\n",
    "                codes = polar.encode_plotkin(msg_bits)\n",
    "            elif 'KO' in encoder_type:\n",
    "                if kernel:\n",
    "                    codes = polar.kernel_encode(kernel_size, polar.gnet_dict[1][0], msg_bits, info_positions, binary = binary)\n",
    "                else:\n",
    "                    codes = polar.deeppolar_encode(msg_bits, binary = binary)\n",
    "\n",
    "            noisy_codes = polar.channel(codes, train_snr, noise_type)\n",
    "\n",
    "            if 'KO' in decoder_type:\n",
    "                if kernel:\n",
    "                    if decoder_type == 'KO_parallel':\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_parallel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                    else:\n",
    "                        decoded_llrs, decoded_bits = polar.kernel_decode(kernel_size, polar.fnet_dict[1][0], noisy_codes, info_positions)\n",
    "                else:\n",
    "                    decoded_llrs, decoded_bits = polar.deeppolar_decode(noisy_codes)\n",
    "            elif decoder_type == 'SC':\n",
    "                decoded_llrs, decoded_bits = polar.sc_decode_new(noisy_codes, train_snr)\n",
    "\n",
    "#             if 'BCE' in loss_type or loss_type == 'focal':\n",
    "#                 loss = criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "#             else:\n",
    "#                 loss = criterion(torch.tanh(0.5*decoded_llrs), msg_bits.to(polar.device))\n",
    "            \n",
    "#             if regularizer == 'std':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.std(codes, dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.std(codes[:, ::2], dim=1).mean() + .5*torch.std(codes[:, 1::2], dim=1).mean())\n",
    "#             elif regularizer == 'max_deviation':\n",
    "#                 if K == 1:\n",
    "#                     loss += regularizer_weight * torch.amax(torch.abs(codes - codes.mean(dim=1, keepdim=True)), dim=1).mean()\n",
    "#                 elif K == 2:\n",
    "#                     loss += regularizer_weight * (0.5*torch.amax(torch.abs(codes[:, ::2] - codes[:, ::2].mean(dim=1, keepdim=True)), dim=1).mean() + .5*torch.amax(torch.abs(codes[:, 1::2] - codes[:, 1::2].mean(dim=1, keepdim=True)), dim=1).mean())\n",
    "#             elif regularizer == 'polar':\n",
    "#                 loss += regularizer_weight * F.mse_loss(codes, polar.encode_plotkin(msg_bits))\n",
    "            loss = soft_bler_loss(decoded_llrs, 0.5 * msg_bits.to(polar.device)+0.5)+criterion(decoded_llrs, 0.5 * msg_bits.to(polar.device) + 0.5)\n",
    "            loss = loss/num_batches\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_ber = errors_ber(decoded_bits.sign(), msg_bits.to(polar.device)).item()\n",
    "    \n",
    "    return loss.item(), train_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79570aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_full_test(polar, KO, snr_range, device, info_positions, binary=False, num_errors=100, noise_type = 'awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "\n",
    "    print(f\"TESTING until {num_errors} block errors\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)  # Assuming SNR is given in dB and noise variance is derived from it\n",
    "\n",
    "        try:\n",
    "            while min(total_block_errors_SC, total_block_errors_KO) <= num_errors:\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:  # if SC is also used for KO\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results for logging\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Real-time logging for progress, updating in-place\n",
    "                print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]/batches_processed:.6f}, SC_BLER: {blers_SC_test[snr_ind]/batches_processed:.6f}, KO_BER: {bers_KO_test[snr_ind]/batches_processed:.6f}, KO_BLER: {blers_KO_test[snr_ind]/batches_processed:.6f}, Batches: {batches_processed}\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # print(\"\\nInterrupted by user. Finalizing current SNR...\")\n",
    "            pass\n",
    "\n",
    "        # Normalize cumulative metrics by the number of processed batches for accuracy\n",
    "        bers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        bers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_KO_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        blers_SC_test[snr_ind] /= (batches_processed + 0.00000001)\n",
    "        print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, SC_BER: {bers_SC_test[snr_ind]:.6f}, SC_BLER: {blers_SC_test[snr_ind]:.6f}, KO_BER: {bers_KO_test[snr_ind]:.6f}, KO_BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen(N, K, rate_profile, target_K = None):\n",
    "    n = int(np.log2(N))\n",
    "    if rate_profile == 'polar':\n",
    "        # computed for SNR = 0\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "\n",
    "            # for RM :(\n",
    "            # rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 3, 5, 8, 4, 2, 1, 0])\n",
    "\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "        elif n<9:\n",
    "            rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "        else:\n",
    "            rs = np.array([1023, 1022, 1021, 1019, 1015, 1007, 1020,  991, 1018, 1017, 1014,\n",
    "       1006,  895, 1013, 1011,  959, 1005,  990, 1003,  989,  767, 1016,\n",
    "        999, 1012,  987,  958,  983,  957, 1010, 1004,  955, 1009,  894,\n",
    "        975,  893, 1002,  951, 1001,  988,  511,  766,  998,  891,  943,\n",
    "        986,  997,  985,  887,  956,  765,  995,  927,  982,  981,  879,\n",
    "        954,  974,  763,  953,  979,  510, 1008,  759,  863,  950,  892,\n",
    "       1000,  973,  949,  509,  890,  971,  996,  942,  751,  984,  889,\n",
    "        507,  947,  831,  886,  967,  941,  764,  926,  980,  994,  939,\n",
    "        885,  993,  735,  878,  925,  503,  762,  883,  978,  935,  703,\n",
    "        495,  952,  877,  761,  972,  923,  977,  948,  758,  862,  875,\n",
    "        919,  970,  757,  861,  508,  969,  750,  946,  479,  888,  639,\n",
    "        871,  911,  830,  940,  859,  755,  966,  945,  749,  506,  884,\n",
    "        938,  965,  829,  734,  924,  855,  505,  747,  963,  937,  882,\n",
    "        934,  827,  733,  447,  992,  847,  876,  501,  921,  702,  494,\n",
    "        881,  760,  743,  933,  502,  918,  874,  922,  823,  731,  499,\n",
    "        860,  756,  931,  701,  873,  493,  727,  917,  870,  976,  815,\n",
    "        910,  383,  968,  478,  858,  754,  699,  491,  869,  944,  748,\n",
    "        638,  915,  477,  719,  909,  964,  255,  799,  504,  857,  854,\n",
    "        753,  828,  746,  695,  487,  907,  637,  867,  853,  475,  936,\n",
    "        962,  446,  732,  826,  745,  846,  500,  825,  903,  687,  932,\n",
    "        635,  471,  445,  742,  880,  498,  730,  851,  822,  382,  920,\n",
    "        845,  741,  443,  700,  729,  631,  492,  872,  961,  726,  821,\n",
    "        930,  497,  381,  843,  463,  916,  739,  671,  623,  490,  929,\n",
    "        439,  814,  819,  868,  752,  914,  698,  725,  839,  856,  476,\n",
    "        813,  718,  908,  486,  723,  866,  489,  607,  431,  697,  379,\n",
    "        811,  798,  913,  575,  717,  254,  694,  636,  474,  807,  715,\n",
    "        906,  797,  693,  865,  960,  852,  744,  634,  473,  795,  905,\n",
    "        485,  415,  483,  470,  444,  375,  850,  740,  686,  902,  824,\n",
    "        691,  253,  711,  633,  844,  685,  630,  901,  367,  791,  928,\n",
    "        728,  820,  849,  783,  670,  899,  738,  842,  683,  247,  469,\n",
    "        441,  442,  462,  251,  737,  438,  467,  351,  629,  841,  724,\n",
    "        679,  669,  496,  461,  818,  380,  437,  627,  622,  459,  378,\n",
    "        239,  488,  667,  838,  430,  484,  812,  621,  319,  817,  435,\n",
    "        377,  696,  722,  912,  606,  810,  864,  716,  837,  721,  714,\n",
    "        809,  796,  455,  472,  619,  835,  692,  663,  223,  414,  904,\n",
    "        427,  806,  482,  632,  713,  690,  848,  605,  373,  252,  794,\n",
    "        429,  710,  684,  615,  805,  900,  655,  468,  366,  603,  413,\n",
    "        574,  481,  371,  250,  793,  466,  423,  374,  689,  628,  440,\n",
    "        365,  709,  789,  803,  411,  573,  682,  249,  460,  790,  668,\n",
    "        599,  350,  707,  246,  681,  465,  571,  626,  436,  407,  782,\n",
    "        191,  127,  363,  620,  666,  458,  245,  349,  677,  434,  678,\n",
    "        591,  787,  399,  457,  359,  238,  625,  840,  567,  736,  665,\n",
    "        428,  376,  781,  898,  618,  675,  318,  454,  662,  243,  897,\n",
    "        347,  836,  816,  720,  433,  604,  617,  779,  808,  661,  834,\n",
    "        712,  804,  833,  559,  237,  453,  426,  222,  317,  775,  372,\n",
    "        343,  412,  235,  543,  614,  451,  425,  422,  613,  370,  221,\n",
    "        315,  480,  335,  659,  654,  364,  190,  369,  248,  653,  688,\n",
    "        231,  410,  602,  611,  802,  792,  421,  651,  601,  598,  708,\n",
    "        311,  219,  572,  597,  788,  570,  409,  590,  362,  801,  680,\n",
    "        464,  406,  419,  348,  647,  786,  215,  589,  706,  361,  676,\n",
    "        566,  189,  595,  244,  569,  303,  405,  358,  456,  346,  398,\n",
    "        565,  242,  126,  705,  780,  587,  624,  664,  236,  187,  357,\n",
    "        432,  785,  558,  674,  207,  403,  397,  452,  345,  563,  778,\n",
    "        241,  316,  342,  616,  660,  557,  125,  234,  183,  287,  355,\n",
    "        583,  673,  395,  424,  314,  220,  777,  341,  612,  658,  123,\n",
    "        175,  774,  555,  233,  334,  542,  450,  313,  391,  230,  652,\n",
    "        368,  218,  339,  600,  119,  333,  657,  610,  773,  541,  310,\n",
    "        420,  159,  229,  650,  551,  596,  609,  408,  217,  449,  188,\n",
    "        309,  214,  331,  111,  539,  360,  771,  649,  302,  418,  594,\n",
    "        896,  227,  404,  646,  186,  588,  832,  568,  213,  417,  301,\n",
    "        307,  356,  402,  800,  564,  327,   95,  206,  240,  535,  593,\n",
    "        645,  586,  344,  396,  185,  401,  211,  354,  299,  585,  286,\n",
    "        562,  643,  182,  205,  124,  232,  285,  295,  181,  556,  582,\n",
    "        527,  394,  340,   63,  203,  561,  353,  448,  122,  283,  393,\n",
    "        581,  554,  174,  390,  704,  312,  338,  228,  179,  784,  199,\n",
    "        553,  121,  173,  389,  540,  579,  332,  118,  672,  550,  337,\n",
    "        158,  279,  271,  416,  216,  308,  387,  538,  549,  226,  330,\n",
    "        776,  171,  212,  117,  110,  329,  656,  157,  772,  306,  326,\n",
    "        225,  167,  115,  537,  534,  184,  109,  300,  547,  305,  210,\n",
    "        155,  533,  325,  352,  608,  400,  298,  204,   94,  648,  284,\n",
    "        209,  151,  180,  107,  770,  297,  392,  323,  592,  202,  644,\n",
    "         93,  294,  178,  103,  143,  282,   62,  336,  201,  120,  172,\n",
    "        198,  769,  584,   91,  388,  293,  177,  526,  278,  281,  642,\n",
    "        525,  531,   61,  170,  116,  197,   87,  156,  277,  114,  560,\n",
    "        169,   59,  291,  580,  275,  523,  641,  270,  195,  552,  519,\n",
    "        166,  224,  578,  108,  269,   79,  154,  113,  548,  577,  536,\n",
    "        328,   55,  106,  165,  153,  150,  386,  208,  324,  546,  385,\n",
    "        267,   47,   92,  163,  296,  304,  105,  102,  149,  263,  532,\n",
    "        322,  292,  545,   90,  200,   31,  321,  530,  142,  176,  147,\n",
    "        101,  141,  196,  524,  529,  290,   89,  280,   60,   86,   99,\n",
    "        139,  168,   58,  522,  276,   85,  194,  289,   78,  135,  112,\n",
    "        521,   57,   83,   54,  518,  274,  268,  768,  164,   77,  152,\n",
    "        193,   53,  162,  104,  517,  273,  266,   75,   46,  148,   51,\n",
    "        640,  100,   45,  576,  161,  265,  262,   71,  146,   30,  140,\n",
    "         88,  515,   98,   43,   29,  261,  145,  138,   84,  259,   39,\n",
    "         97,   27,   56,   82,  137,   76,  384,  134,   23,   52,  133,\n",
    "        320,   15,   73,   50,   81,  131,   44,   70,  544,  192,  528,\n",
    "        288,  520,  160,  272,   74,   49,  516,   42,   69,   28,  144,\n",
    "         41,   67,   96,  514,   38,  264,  260,  136,   22,   25,   37,\n",
    "         80,  513,   26,  258,   35,  132,   21,  257,   72,   14,   48,\n",
    "         13,   19,  130,   68,   40,   11,  512,   66,  129,    7,   36,\n",
    "         24,   34,  256,   20,   65,   33,   12,  128,   18,   10,   17,\n",
    "          6,    9,   64,    5,    3,   32,   16,    8,    4,    2,    1,\n",
    "          0])\n",
    "        rs = rs[rs<N]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'RM':\n",
    "        rmweight = np.array([countSetBits(i) for i in range(N)])\n",
    "        Fr = np.argsort(rmweight)[:-K]\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'sorted_last':\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:K].copy()\n",
    "        first_inds.sort()\n",
    "        rs[:K] = first_inds[::-1]\n",
    "\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    elif rate_profile == 'rev_polar':\n",
    "\n",
    "        if n == 5:\n",
    "            rs = np.array([31, 30, 29, 27, 23, 15, 28, 26, 25, 22, 21, 14, 19, 13, 11, 24,  7, 20, 18, 12, 17, 10,  9,  6,  5,  3, 16,  8,  4,  2,  1,  0])\n",
    "\n",
    "        elif n == 4:\n",
    "            rs = np.array([15, 14, 13, 11, 7, 12, 10, 9, 6, 5, 3, 8, 4, 2, 1, 0])\n",
    "        elif n == 3:\n",
    "            rs = np.array([7, 6, 5, 3, 4, 2, 1, 0])\n",
    "        elif n == 2:\n",
    "            rs = np.array([3, 2, 1, 0])\n",
    "\n",
    "        rs = np.array([256 ,255 ,252 ,254 ,248 ,224 ,240 ,192 ,128 ,253 ,244 ,251 ,250 ,239 ,238 ,247 ,246 ,223 ,222 ,232 ,216 ,236 ,220 ,188 ,208 ,184 ,191 ,190 ,176 ,127 ,126 ,124 ,120 ,249 ,245 ,243 ,242 ,160 ,231 ,230 ,237 ,235 ,234 ,112 ,228 ,221 ,219 ,218 ,212 ,215 ,214 ,189 ,187 ,96 ,186 ,207 ,206 ,183 ,182 ,204 ,180 ,200 ,64 ,175 ,174 ,172 ,125 ,123 ,122 ,119 ,159 ,118 ,158 ,168 ,241 ,116 ,111 ,233 ,156 ,110 ,229 ,227 ,217 ,108 ,213 ,152 ,226 ,95 ,211 ,94 ,205 ,185 ,104 ,210 ,203 ,181 ,92 ,144 ,202 ,179 ,199 ,173 ,178 ,63 ,198 ,121 ,171 ,88 ,62 ,117 ,170 ,196 ,157 ,167 ,60 ,115 ,155 ,109 ,166 ,80 ,114 ,154 ,107 ,56 ,225 ,151 ,164 ,106 ,93 ,150 ,209 ,103 ,91 ,143 ,201 ,102 ,48 ,148 ,177 ,90 ,142 ,197 ,87 ,100 ,61 ,169 ,195 ,140 ,86 ,59 ,32 ,165 ,194 ,113 ,79 ,58 ,153 ,84 ,136 ,55 ,163 ,78 ,105 ,149 ,162 ,54 ,76 ,101 ,47 ,147 ,89 ,52 ,141 ,99 ,46 ,146 ,72 ,85 ,139 ,98 ,31 ,44 ,193 ,138 ,57 ,83 ,30 ,135 ,77 ,40 ,82 ,134 ,161 ,28 ,53 ,75 ,132 ,24 ,51 ,74 ,45 ,145 ,71 ,50 ,16 ,97 ,70 ,43 ,137 ,68 ,42 ,29 ,39 ,81 ,27 ,133 ,38 ,26 ,36 ,131 ,23 ,73 ,22 ,130 ,49 ,15 ,20 ,69 ,14 ,12 ,67 ,41 ,8 ,66 ,37 ,25 ,35 ,34 ,21 ,129 ,19 ,13 ,18 ,11 ,10 ,7 ,65 ,6 ,4 ,33 ,17 ,9 ,5 ,3 ,2 ,1 ]) - 1\n",
    "\n",
    "        rs = rs[rs<N]\n",
    "        first_inds = rs[:target_K].copy()\n",
    "        rs[:target_K] = first_inds[::-1]\n",
    "        Fr = rs[K:].copy()\n",
    "        Fr.sort()\n",
    "\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(codebook):\n",
    "    \"\"\"Calculate pairwise distances between codewords\"\"\"\n",
    "    dists = []\n",
    "    for row1, row2 in combinations(codebook, 2):\n",
    "        distance = (row1-row2).pow(2).sum()\n",
    "        dists.append(np.sqrt(distance.item()))\n",
    "    return dists, np.min(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54073b6",
   "metadata": {},
   "source": [
    "# Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2a9c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(bers_enc, label='BER')\n",
    "    plt.plot(moving_average(bers_enc, n=10), label='BER moving avg')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training BER ENC')\n",
    "    plt.savefig(os.path.join(results_save_path, 'training_ber_enc.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Similar plots for losses_enc, bers_dec, losses_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(polar, iter, results_save_path, best=False):\n",
    "    torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map], \n",
    "               os.path.join(results_save_path, f'Models/fnet_gnet_{iter}.pt'))\n",
    "    if iter > 1:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_final.pt'))\n",
    "    if best:\n",
    "        torch.save([polar.fnet_dict, polar.gnet_dict, polar.depth_map],\n",
    "                  os.path.join(results_save_path, 'Models/fnet_gnet_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4986216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen positions : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 120 121 122 124 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 176 177 178 179 180 181 182 184 185 186\n",
      " 188 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 208 209\n",
      " 210 211 212 213 214 216 217 218 220 224 225 226 227 228 229 230 232 233\n",
      " 234 236 240]\n",
      "Loaded kernel from  Polar_Results/curriculum/final_kernels/16_normal_polar_eh64_dh128_selu_new\n"
     ]
    }
   ],
   "source": [
    "if anomaly:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "#ID = str(np.random.randint(100000, 999999)) if id is None else id\n",
    "#ID = 207515\n",
    "\n",
    "\n",
    "###############\n",
    "### Polar code\n",
    "##############\n",
    "\n",
    "### Encoder\n",
    "\n",
    "if last_ell is not None:\n",
    "    depth_map = defaultdict(int)\n",
    "    n = int(np.log2(N // last_ell) // np.log2(kernel_size))\n",
    "    for d in range(1, n+1):\n",
    "        depth_map[d] = kernel_size\n",
    "    depth_map[n+1] = last_ell\n",
    "    assert np.prod(list(depth_map.values())) == N\n",
    "    polar = DeepPolar(device, N, K, infty = infty, depth_map = depth_map)\n",
    "else:\n",
    "    polar = DeepPolar(device, N, K, kernel_size, infty)\n",
    "\n",
    "info_inds = polar.info_positions\n",
    "frozen_inds = polar.frozen_positions\n",
    "\n",
    "print(\"Frozen positions : {}\".format(frozen_inds))\n",
    "\n",
    "##############\n",
    "### Neural networks\n",
    "##############\n",
    "ell = kernel_size\n",
    "if N == ell: # Kernel pre-training\n",
    "    polar.define_kernel_nns(ell = kernel_size, unfrozen = polar.info_positions, fnet = decoder_type, gnet = encoder_type, shared = shared)\n",
    "elif N > ell: # Initialize full network with pretrained kernels\n",
    "    polar.define_and_load_nns(ell = kernel_size, kernel_load_path=kernel_load_path, fnet = decoder_type, gnet = encoder_type, shared = shared, dataparallel=dataparallel)\n",
    "\n",
    "if binary:\n",
    "    load_path = os.path.join(results_save_path, 'Models/fnet_gnet_final.pt')\n",
    "    assert os.path.exists(load_path), \"Model does not exist!!\"\n",
    "    results_save_path = os.path.join(results_save_path, 'Binary')\n",
    "    os.makedirs(results_save_path, exist_ok=True)\n",
    "    os.makedirs(results_save_path +'/Models', exist_ok=True)\n",
    "\n",
    "if load_path is not None:\n",
    "    if test:\n",
    "        if test_load_path is None:\n",
    "            print(\"WARNING : have you used load_path instead of test_load_path?\")\n",
    "    else:\n",
    "        checkpoint1 = torch.load(load_path , map_location=lambda storage, loc: storage)\n",
    "        fnet_dict = checkpoint1[0]\n",
    "        gnet_dict = checkpoint1[1]\n",
    "\n",
    "        polar.load_partial_nns(fnet_dict, gnet_dict)\n",
    "        print(\"Loaded nets from {}\".format(load_path))\n",
    "\n",
    "if 'KO' in decoder_type:\n",
    "    dec_params = []\n",
    "    for i in polar.fnet_dict.keys():\n",
    "        for j in polar.fnet_dict[i].keys():\n",
    "            if isinstance(polar.fnet_dict[i][j], dict):\n",
    "                for k in polar.fnet_dict[i][j].keys():\n",
    "                    dec_params += list(polar.fnet_dict[i][j][k].parameters())\n",
    "            else:\n",
    "                dec_params += list(polar.fnet_dict[i][j].parameters())\n",
    "elif decoder_type == 'RNN':\n",
    "    dec_params = polar.fnet_dict.parameters()\n",
    "else:\n",
    "    dec_train_iters = 0\n",
    "\n",
    "if 'KO' in encoder_type:\n",
    "    enc_params = []\n",
    "    if shared:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            enc_params += list(polar.gnet_dict[i].parameters())\n",
    "    else:\n",
    "        for i in polar.gnet_dict.keys():\n",
    "            for j in polar.gnet_dict[i].keys():\n",
    "                enc_params += list(polar.gnet_dict[i][j].parameters())\n",
    "elif encoder_type == 'scaled':\n",
    "    enc_params = [polar.a]\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)\n",
    "else:\n",
    "    enc_train_iters = 0\n",
    "\n",
    "if dec_train_iters > 0:\n",
    "    if optim_name == 'Adam':\n",
    "        dec_optimizer = optim.Adam(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'SGD':\n",
    "        dec_optimizer = optim.SGD(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    elif optim_name == 'RMS':\n",
    "        dec_optimizer = optim.RMSprop(dec_params, lr = dec_lr, weight_decay = weight_decay)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        dec_scheduler = optim.lr_scheduler.OneCycleLR(dec_optimizer, max_lr = dec_lr, total_steps=dec_train_iters*full_iters)  \n",
    "    if scheduler == 'cosine':\n",
    "        dec_scheduler = WarmUpCosineAnnealingLR(optimizer=dec_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=40,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        dec_scheduler = None\n",
    "\n",
    "if enc_train_iters > 0:\n",
    "    enc_optimizer = optim.Adam(enc_params, lr = enc_lr)#, momentum=0.9, nesterov=True) #, amsgrad=True)\n",
    "    if scheduler == 'reduce':\n",
    "        enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min', patience = scheduler_patience)  \n",
    "    elif scheduler == '1cycle':\n",
    "        enc_scheduler = optim.lr_scheduler.OneCycleLR(enc_optimizer, max_lr = enc_lr, total_steps=enc_train_iters*full_iters) \n",
    "    if scheduler == 'cosine':\n",
    "        enc_scheduler = WarmUpCosineAnnealingLR(optimizer=enc_optimizer,\n",
    "                                            T_max=full_iters,\n",
    "                                            T_warmup=40,\n",
    "                                            eta_min=1e-6)\n",
    "    else:\n",
    "        enc_scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'BCE' in loss_type:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif loss_type == 'L1':\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_type == 'huber':\n",
    "    criterion = nn.HuberLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "info_positions = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fec064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen = polar.info_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2abc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905d1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to save for: 100\n",
      "[1/200] At -2.0 dB, Train Loss: 8.271987915039062 Train BER 0.4760972857475281,                  \n",
      " [1/200] At 0.0 dB, Train Loss: 8.077513694763184 Train BER 0.4727729856967926\n",
      "Time for one full iteration is 7.3317 minutes\n",
      "encoder learning rate: 2.50e-05, decoder learning rate: 2.50e-05\n",
      "[2/200] At -2.0 dB, Train Loss: 6.328723430633545 Train BER 0.4661891758441925,                  \n",
      " [2/200] At 0.0 dB, Train Loss: 6.264033794403076 Train BER 0.4599080979824066\n",
      "Time for one full iteration is 7.3869 minutes\n",
      "encoder learning rate: 5.00e-05, decoder learning rate: 5.00e-05\n",
      "[3/200] At -2.0 dB, Train Loss: 5.915865898132324 Train BER 0.44615134596824646,                  \n",
      " [3/200] At 0.0 dB, Train Loss: 5.8589630126953125 Train BER 0.43352431058883667\n",
      "Time for one full iteration is 7.1671 minutes\n",
      "encoder learning rate: 7.50e-05, decoder learning rate: 7.50e-05\n",
      "[4/200] At -2.0 dB, Train Loss: 4.843169212341309 Train BER 0.31717297434806824,                  \n",
      " [4/200] At 0.0 dB, Train Loss: 4.032005786895752 Train BER 0.25360000133514404\n",
      "Time for one full iteration is 7.2045 minutes\n",
      "encoder learning rate: 1.00e-04, decoder learning rate: 1.00e-04\n",
      "[5/200] At -2.0 dB, Train Loss: 2.05336856842041 Train BER 0.10633513331413269,                  \n",
      " [5/200] At 0.0 dB, Train Loss: 0.8785850405693054 Train BER 0.038145944476127625\n",
      "Time for one full iteration is 7.1738 minutes\n",
      "encoder learning rate: 1.25e-04, decoder learning rate: 1.25e-04\n",
      "[6/200] At -2.0 dB, Train Loss: 1.0810601711273193 Train BER 0.05329729616641998,                  \n",
      " [6/200] At 0.0 dB, Train Loss: 0.33487004041671753 Train BER 0.017697297036647797\n",
      "Time for one full iteration is 7.1915 minutes\n",
      "encoder learning rate: 1.50e-04, decoder learning rate: 1.50e-04\n",
      "[7/200] At -2.0 dB, Train Loss: 0.7218202948570251 Train BER 0.03475675731897354,                  \n",
      " [7/200] At 0.0 dB, Train Loss: 0.24286161363124847 Train BER 0.013021621853113174\n",
      "Time for one full iteration is 7.2719 minutes\n",
      "encoder learning rate: 1.75e-04, decoder learning rate: 1.75e-04\n",
      "[8/200] At -2.0 dB, Train Loss: 0.4253736734390259 Train BER 0.018010810017585754,                  \n",
      " [8/200] At 0.0 dB, Train Loss: 0.055587589740753174 Train BER 0.00130270270165056\n",
      "Time for one full iteration is 7.4845 minutes\n",
      "encoder learning rate: 2.00e-04, decoder learning rate: 2.00e-04\n",
      "[9/200] At -2.0 dB, Train Loss: 0.29091066122055054 Train BER 0.011972973123192787,                  \n",
      " [9/200] At 0.0 dB, Train Loss: 0.023980600759387016 Train BER 0.0005729729891754687\n",
      "Time for one full iteration is 7.5541 minutes\n",
      "encoder learning rate: 2.25e-04, decoder learning rate: 2.25e-04\n",
      "[10/200] At -2.0 dB, Train Loss: 0.2347816675901413 Train BER 0.009800000116229057,                  \n",
      " [10/200] At 0.0 dB, Train Loss: 0.013429570011794567 Train BER 0.00035675676190294325\n",
      "Time for one full iteration is 7.5408 minutes\n",
      "encoder learning rate: 2.50e-04, decoder learning rate: 2.50e-04\n",
      "[11/200] At -2.0 dB, Train Loss: 0.1883862167596817 Train BER 0.007799999788403511,                  \n",
      " [11/200] At 0.0 dB, Train Loss: 0.009514198638498783 Train BER 0.00018918918794952333\n",
      "Time for one full iteration is 7.4527 minutes\n",
      "encoder learning rate: 2.75e-04, decoder learning rate: 2.75e-04\n",
      "[12/200] At -2.0 dB, Train Loss: 0.1526280641555786 Train BER 0.006129729561507702,                  \n",
      " [12/200] At 0.0 dB, Train Loss: 0.008632881566882133 Train BER 0.0002162162127206102\n",
      "Time for one full iteration is 7.5675 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[13/200] At -2.0 dB, Train Loss: 0.14526425302028656 Train BER 0.005697297398000956,                  \n",
      " [13/200] At 0.0 dB, Train Loss: 0.007163402158766985 Train BER 0.00021081081649754196\n",
      "Time for one full iteration is 7.0495 minutes\n",
      "encoder learning rate: 3.25e-04, decoder learning rate: 3.25e-04\n",
      "[14/200] At -2.0 dB, Train Loss: 0.12412899732589722 Train BER 0.005108108278363943,                  \n",
      " [14/200] At 0.0 dB, Train Loss: 0.004634067881852388 Train BER 0.0001351351384073496\n",
      "Time for one full iteration is 7.0184 minutes\n",
      "encoder learning rate: 3.50e-04, decoder learning rate: 3.50e-04\n",
      "[15/200] At -2.0 dB, Train Loss: 0.1169266328215599 Train BER 0.004913513548672199,                  \n",
      " [15/200] At 0.0 dB, Train Loss: 0.0039751832373440266 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 7.0491 minutes\n",
      "encoder learning rate: 3.75e-04, decoder learning rate: 3.75e-04\n",
      "[16/200] At -2.0 dB, Train Loss: 0.10103552788496017 Train BER 0.004167567472904921,                  \n",
      " [16/200] At 0.0 dB, Train Loss: 0.0018950810190290213 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 7.0749 minutes\n",
      "encoder learning rate: 4.00e-04, decoder learning rate: 4.00e-04\n",
      "[17/200] At -2.0 dB, Train Loss: 0.1026269868016243 Train BER 0.004172972869127989,                  \n",
      " [17/200] At 0.0 dB, Train Loss: 0.0021388630848377943 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 6.9879 minutes\n",
      "encoder learning rate: 4.25e-04, decoder learning rate: 4.25e-04\n",
      "[18/200] At -2.0 dB, Train Loss: 0.10676702111959457 Train BER 0.004632432479411364,                  \n",
      " [18/200] At 0.0 dB, Train Loss: 0.001378814340569079 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.0248 minutes\n",
      "encoder learning rate: 4.50e-04, decoder learning rate: 4.50e-04\n",
      "[19/200] At -2.0 dB, Train Loss: 0.1027972474694252 Train BER 0.004567567724734545,                  \n",
      " [19/200] At 0.0 dB, Train Loss: 0.0016057109460234642 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 7.0208 minutes\n",
      "encoder learning rate: 4.75e-04, decoder learning rate: 4.75e-04\n",
      "[20/200] At -2.0 dB, Train Loss: 0.08610068261623383 Train BER 0.0040054055862128735,                  \n",
      " [20/200] At 0.0 dB, Train Loss: 0.001786820008419454 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 7.1129 minutes\n",
      "encoder learning rate: 5.00e-04, decoder learning rate: 5.00e-04\n",
      "[21/200] At -2.0 dB, Train Loss: 0.06841260194778442 Train BER 0.0029081080574542284,                  \n",
      " [21/200] At 0.0 dB, Train Loss: 0.0016502104699611664 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 7.2095 minutes\n",
      "encoder learning rate: 5.25e-04, decoder learning rate: 5.25e-04\n",
      "[22/200] At -2.0 dB, Train Loss: 0.07841627299785614 Train BER 0.0034486486110836267,                  \n",
      " [22/200] At 0.0 dB, Train Loss: 0.0009785543661564589 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 7.1479 minutes\n",
      "encoder learning rate: 5.50e-04, decoder learning rate: 5.50e-04\n",
      "[23/200] At -2.0 dB, Train Loss: 0.06377377361059189 Train BER 0.0027405405417084694,                  \n",
      " [23/200] At 0.0 dB, Train Loss: 0.0010638622334226966 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 7.1044 minutes\n",
      "encoder learning rate: 5.75e-04, decoder learning rate: 5.75e-04\n",
      "[24/200] At -2.0 dB, Train Loss: 0.07510221004486084 Train BER 0.0035675675608217716,                  \n",
      " [24/200] At 0.0 dB, Train Loss: 0.0013899835757911205 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 7.3048 minutes\n",
      "encoder learning rate: 6.00e-04, decoder learning rate: 6.00e-04\n",
      "[25/200] At -2.0 dB, Train Loss: 0.07229571044445038 Train BER 0.003167567541822791,                  \n",
      " [25/200] At 0.0 dB, Train Loss: 0.0013480829074978828 Train BER 4.86486496811267e-05\n",
      "Time for one full iteration is 7.3534 minutes\n",
      "encoder learning rate: 6.25e-04, decoder learning rate: 6.25e-04\n",
      "[26/200] At -2.0 dB, Train Loss: 0.062094952911138535 Train BER 0.002729729749262333,                  \n",
      " [26/200] At 0.0 dB, Train Loss: 0.0009260136284865439 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.5752 minutes\n",
      "encoder learning rate: 6.50e-04, decoder learning rate: 6.50e-04\n",
      "[27/200] At -2.0 dB, Train Loss: 0.04875343292951584 Train BER 0.0018972973339259624,                  \n",
      " [27/200] At 0.0 dB, Train Loss: 0.0009918734431266785 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 7.5424 minutes\n",
      "encoder learning rate: 6.75e-04, decoder learning rate: 6.75e-04\n",
      "[28/200] At -2.0 dB, Train Loss: 0.06756674498319626 Train BER 0.0030756755731999874,                  \n",
      " [28/200] At 0.0 dB, Train Loss: 0.0007871516281738877 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.7025 minutes\n",
      "encoder learning rate: 7.00e-04, decoder learning rate: 7.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/200] At -2.0 dB, Train Loss: 0.05867006257176399 Train BER 0.0026918919757008553,                  \n",
      " [29/200] At 0.0 dB, Train Loss: 0.0009549919050186872 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 7.7483 minutes\n",
      "encoder learning rate: 7.25e-04, decoder learning rate: 7.25e-04\n",
      "[30/200] At -2.0 dB, Train Loss: 0.06106964871287346 Train BER 0.0026216215919703245,                  \n",
      " [30/200] At 0.0 dB, Train Loss: 0.001507632783614099 Train BER 4.324324254412204e-05\n",
      "Time for one full iteration is 7.8332 minutes\n",
      "encoder learning rate: 7.50e-04, decoder learning rate: 7.50e-04\n",
      "[31/200] At -2.0 dB, Train Loss: 0.06504836678504944 Train BER 0.002854054095223546,                  \n",
      " [31/200] At 0.0 dB, Train Loss: 0.0008291112026199698 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.7131 minutes\n",
      "encoder learning rate: 7.75e-04, decoder learning rate: 7.75e-04\n",
      "[32/200] At -2.0 dB, Train Loss: 0.05837571993470192 Train BER 0.0027405405417084694,                  \n",
      " [32/200] At 0.0 dB, Train Loss: 0.0008089520852081478 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.8035 minutes\n",
      "encoder learning rate: 8.00e-04, decoder learning rate: 8.00e-04\n",
      "[33/200] At -2.0 dB, Train Loss: 0.05860394984483719 Train BER 0.002491891849786043,                  \n",
      " [33/200] At 0.0 dB, Train Loss: 0.0005212164833210409 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.8977 minutes\n",
      "encoder learning rate: 8.25e-04, decoder learning rate: 8.25e-04\n",
      "[34/200] At -2.0 dB, Train Loss: 0.05840248614549637 Train BER 0.0026270269881933928,                  \n",
      " [34/200] At 0.0 dB, Train Loss: 0.0015267153503373265 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 7.7990 minutes\n",
      "encoder learning rate: 8.50e-04, decoder learning rate: 8.50e-04\n",
      "[35/200] At -2.0 dB, Train Loss: 0.051595453172922134 Train BER 0.002075675642117858,                  \n",
      " [35/200] At 0.0 dB, Train Loss: 0.0006676936754956841 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.6791 minutes\n",
      "encoder learning rate: 8.75e-04, decoder learning rate: 8.75e-04\n",
      "[36/200] At -2.0 dB, Train Loss: 0.06544717401266098 Train BER 0.003016216214746237,                  \n",
      " [36/200] At 0.0 dB, Train Loss: 0.0034618831705302 Train BER 0.00012972972763236612\n",
      "Time for one full iteration is 7.6421 minutes\n",
      "encoder learning rate: 9.00e-04, decoder learning rate: 9.00e-04\n",
      "[37/200] At -2.0 dB, Train Loss: 0.10706159472465515 Train BER 0.004713513422757387,                  \n",
      " [37/200] At 0.0 dB, Train Loss: 0.021882232278585434 Train BER 0.0007135135238058865\n",
      "Time for one full iteration is 7.7660 minutes\n",
      "encoder learning rate: 9.25e-04, decoder learning rate: 9.25e-04\n",
      "[38/200] At -2.0 dB, Train Loss: 0.1534505933523178 Train BER 0.0064324322156608105,                  \n",
      " [38/200] At 0.0 dB, Train Loss: 0.13386648893356323 Train BER 0.004059459548443556\n",
      "Time for one full iteration is 7.5648 minutes\n",
      "encoder learning rate: 9.50e-04, decoder learning rate: 9.50e-04\n",
      "[39/200] At -2.0 dB, Train Loss: 0.22500576078891754 Train BER 0.009432432241737843,                  \n",
      " [39/200] At 0.0 dB, Train Loss: 0.19845204055309296 Train BER 0.007929729297757149\n",
      "Time for one full iteration is 7.7138 minutes\n",
      "encoder learning rate: 9.75e-04, decoder learning rate: 9.75e-04\n",
      "[40/200] At -2.0 dB, Train Loss: 0.46096110343933105 Train BER 0.02090270258486271,                  \n",
      " [40/200] At 0.0 dB, Train Loss: 0.42996764183044434 Train BER 0.016854053363204002\n",
      "Time for one full iteration is 7.5989 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[41/200] At -2.0 dB, Train Loss: 0.43690744042396545 Train BER 0.019789189100265503,                  \n",
      " [41/200] At 0.0 dB, Train Loss: 0.2779938280582428 Train BER 0.011313513852655888\n",
      "Time for one full iteration is 7.5989 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[42/200] At -2.0 dB, Train Loss: 0.44092613458633423 Train BER 0.02007026970386505,                  \n",
      " [42/200] At 0.0 dB, Train Loss: 0.22531072795391083 Train BER 0.00878378376364708\n",
      "Time for one full iteration is 7.7533 minutes\n",
      "encoder learning rate: 1.00e-03, decoder learning rate: 1.00e-03\n",
      "[43/200] At -2.0 dB, Train Loss: 0.3998813331127167 Train BER 0.01784324273467064,                  \n",
      " [43/200] At 0.0 dB, Train Loss: 0.27521955966949463 Train BER 0.01057297270745039\n",
      "Time for one full iteration is 7.7458 minutes\n",
      "encoder learning rate: 9.99e-04, decoder learning rate: 9.99e-04\n",
      "[44/200] At -2.0 dB, Train Loss: 0.32163554430007935 Train BER 0.014708108268678188,                  \n",
      " [44/200] At 0.0 dB, Train Loss: 0.10164389759302139 Train BER 0.003632432548329234\n",
      "Time for one full iteration is 7.4714 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[45/200] At -2.0 dB, Train Loss: 0.2752819061279297 Train BER 0.012789188884198666,                  \n",
      " [45/200] At 0.0 dB, Train Loss: 0.21020632982254028 Train BER 0.00783243216574192\n",
      "Time for one full iteration is 7.4914 minutes\n",
      "encoder learning rate: 9.98e-04, decoder learning rate: 9.98e-04\n",
      "[46/200] At -2.0 dB, Train Loss: 0.2338017076253891 Train BER 0.010702703148126602,                  \n",
      " [46/200] At 0.0 dB, Train Loss: 0.036299556493759155 Train BER 0.0011459459783509374\n",
      "Time for one full iteration is 7.4512 minutes\n",
      "encoder learning rate: 9.97e-04, decoder learning rate: 9.97e-04\n",
      "[47/200] At -2.0 dB, Train Loss: 0.2326943278312683 Train BER 0.010491891764104366,                  \n",
      " [47/200] At 0.0 dB, Train Loss: 0.11708099395036697 Train BER 0.003956756554543972\n",
      "Time for one full iteration is 7.4920 minutes\n",
      "encoder learning rate: 9.95e-04, decoder learning rate: 9.95e-04\n",
      "[48/200] At -2.0 dB, Train Loss: 0.1632225215435028 Train BER 0.0074756755493581295,                  \n",
      " [48/200] At 0.0 dB, Train Loss: 0.005568517837673426 Train BER 5.945946031715721e-05\n",
      "Time for one full iteration is 7.4602 minutes\n",
      "encoder learning rate: 9.94e-04, decoder learning rate: 9.94e-04\n",
      "[49/200] At -2.0 dB, Train Loss: 0.10991315543651581 Train BER 0.005091892089694738,                  \n",
      " [49/200] At 0.0 dB, Train Loss: 0.008534496650099754 Train BER 0.0002918918908108026\n",
      "Time for one full iteration is 7.4468 minutes\n",
      "encoder learning rate: 9.92e-04, decoder learning rate: 9.92e-04\n",
      "[50/200] At -2.0 dB, Train Loss: 0.07334339618682861 Train BER 0.003286486491560936,                  \n",
      " [50/200] At 0.0 dB, Train Loss: 0.001919133705087006 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 7.4752 minutes\n",
      "encoder learning rate: 9.90e-04, decoder learning rate: 9.90e-04\n",
      "[51/200] At -2.0 dB, Train Loss: 0.06573240458965302 Train BER 0.002875675680115819,                  \n",
      " [51/200] At 0.0 dB, Train Loss: 0.003031935542821884 Train BER 9.189188858726993e-05\n",
      "Time for one full iteration is 7.4809 minutes\n",
      "encoder learning rate: 9.88e-04, decoder learning rate: 9.88e-04\n",
      "[52/200] At -2.0 dB, Train Loss: 0.04840334504842758 Train BER 0.002124324208125472,                  \n",
      " [52/200] At 0.0 dB, Train Loss: 0.0021751965396106243 Train BER 6.486486381618306e-05\n",
      "Time for one full iteration is 7.4673 minutes\n",
      "encoder learning rate: 9.86e-04, decoder learning rate: 9.86e-04\n",
      "[53/200] At -2.0 dB, Train Loss: 0.0643763393163681 Train BER 0.0027891891077160835,                  \n",
      " [53/200] At 0.0 dB, Train Loss: 0.002367067150771618 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 7.4878 minutes\n",
      "encoder learning rate: 9.84e-04, decoder learning rate: 9.84e-04\n",
      "[54/200] At -2.0 dB, Train Loss: 0.05788348615169525 Train BER 0.002589189214631915,                  \n",
      " [54/200] At 0.0 dB, Train Loss: 0.0012314975028857589 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.3068 minutes\n",
      "encoder learning rate: 9.81e-04, decoder learning rate: 9.81e-04\n",
      "[55/200] At -2.0 dB, Train Loss: 0.06171629577875137 Train BER 0.0026432431768625975,                  \n",
      " [55/200] At 0.0 dB, Train Loss: 0.0011401585070416331 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.3906 minutes\n",
      "encoder learning rate: 9.78e-04, decoder learning rate: 9.78e-04\n",
      "[56/200] At -2.0 dB, Train Loss: 0.05583348497748375 Train BER 0.0022054053843021393,                  \n",
      " [56/200] At 0.0 dB, Train Loss: 0.0025733469519764185 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 7.5409 minutes\n",
      "encoder learning rate: 9.76e-04, decoder learning rate: 9.76e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/200] At -2.0 dB, Train Loss: 0.05555059388279915 Train BER 0.0025567568372935057,                  \n",
      " [57/200] At 0.0 dB, Train Loss: 0.0006999755860306323 Train BER 0.0\n",
      "Time for one full iteration is 7.4775 minutes\n",
      "encoder learning rate: 9.72e-04, decoder learning rate: 9.72e-04\n",
      "[58/200] At -2.0 dB, Train Loss: 0.04878365993499756 Train BER 0.0020864864345639944,                  \n",
      " [58/200] At 0.0 dB, Train Loss: 0.00046773714711889625 Train BER 0.0\n",
      "Time for one full iteration is 7.3602 minutes\n",
      "encoder learning rate: 9.69e-04, decoder learning rate: 9.69e-04\n",
      "[59/200] At -2.0 dB, Train Loss: 0.04730863496661186 Train BER 0.0020000000949949026,                  \n",
      " [59/200] At 0.0 dB, Train Loss: 0.0006978505407460034 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.3496 minutes\n",
      "encoder learning rate: 9.66e-04, decoder learning rate: 9.66e-04\n",
      "[60/200] At -2.0 dB, Train Loss: 0.04295637831091881 Train BER 0.002118918811902404,                  \n",
      " [60/200] At 0.0 dB, Train Loss: 0.0007643166463822126 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.4034 minutes\n",
      "encoder learning rate: 9.62e-04, decoder learning rate: 9.62e-04\n",
      "[61/200] At -2.0 dB, Train Loss: 0.036106713116168976 Train BER 0.0016486486420035362,                  \n",
      " [61/200] At 0.0 dB, Train Loss: 0.0006039469153620303 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 7.4812 minutes\n",
      "encoder learning rate: 9.58e-04, decoder learning rate: 9.58e-04\n",
      "[62/200] At -2.0 dB, Train Loss: 0.04058270528912544 Train BER 0.0019405405037105083,                  \n",
      " [62/200] At 0.0 dB, Train Loss: 0.0005349649582058191 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.2711 minutes\n",
      "encoder learning rate: 9.54e-04, decoder learning rate: 9.54e-04\n",
      "[63/200] At -2.0 dB, Train Loss: 0.03917573392391205 Train BER 0.0018432432552799582,                  \n",
      " [63/200] At 0.0 dB, Train Loss: 0.0002687428204808384 Train BER 0.0\n",
      "Time for one full iteration is 7.2946 minutes\n",
      "encoder learning rate: 9.50e-04, decoder learning rate: 9.50e-04\n",
      "[64/200] At -2.0 dB, Train Loss: 0.05155796557664871 Train BER 0.0022378377616405487,                  \n",
      " [64/200] At 0.0 dB, Train Loss: 0.0014091897755861282 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 7.2589 minutes\n",
      "encoder learning rate: 9.46e-04, decoder learning rate: 9.46e-04\n",
      "[65/200] At -2.0 dB, Train Loss: 0.046464528888463974 Train BER 0.0019297297112643719,                  \n",
      " [65/200] At 0.0 dB, Train Loss: 0.0005464448477141559 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3089 minutes\n",
      "encoder learning rate: 9.41e-04, decoder learning rate: 9.41e-04\n",
      "[66/200] At -2.0 dB, Train Loss: 0.03798159584403038 Train BER 0.0017945945728570223,                  \n",
      " [66/200] At 0.0 dB, Train Loss: 0.0007093746680766344 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 7.6338 minutes\n",
      "encoder learning rate: 9.36e-04, decoder learning rate: 9.36e-04\n",
      "[67/200] At -2.0 dB, Train Loss: 0.03891266882419586 Train BER 0.0016810811357572675,                  \n",
      " [67/200] At 0.0 dB, Train Loss: 0.0004687317705247551 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.5259 minutes\n",
      "encoder learning rate: 9.31e-04, decoder learning rate: 9.31e-04\n",
      "[68/200] At -2.0 dB, Train Loss: 0.048468463122844696 Train BER 0.002248648554086685,                  \n",
      " [68/200] At 0.0 dB, Train Loss: 0.00029533979250118136 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2637 minutes\n",
      "encoder learning rate: 9.26e-04, decoder learning rate: 9.26e-04\n",
      "[69/200] At -2.0 dB, Train Loss: 0.043015316128730774 Train BER 0.0018702702363952994,                  \n",
      " [69/200] At 0.0 dB, Train Loss: 0.000219652836676687 Train BER 0.0\n",
      "Time for one full iteration is 7.4148 minutes\n",
      "encoder learning rate: 9.21e-04, decoder learning rate: 9.21e-04\n",
      "[70/200] At -2.0 dB, Train Loss: 0.03703226521611214 Train BER 0.0018324324628338218,                  \n",
      " [70/200] At 0.0 dB, Train Loss: 0.000264893751591444 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4979 minutes\n",
      "encoder learning rate: 9.16e-04, decoder learning rate: 9.16e-04\n",
      "[71/200] At -2.0 dB, Train Loss: 0.06328025460243225 Train BER 0.0027621621266007423,                  \n",
      " [71/200] At 0.0 dB, Train Loss: 0.0019376685377210379 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.2497 minutes\n",
      "encoder learning rate: 9.10e-04, decoder learning rate: 9.10e-04\n",
      "[72/200] At -2.0 dB, Train Loss: 0.04848253354430199 Train BER 0.0022162161767482758,                  \n",
      " [72/200] At 0.0 dB, Train Loss: 0.0012110621901229024 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.2408 minutes\n",
      "encoder learning rate: 9.05e-04, decoder learning rate: 9.05e-04\n",
      "[73/200] At -2.0 dB, Train Loss: 0.05265933275222778 Train BER 0.002464864868670702,                  \n",
      " [73/200] At 0.0 dB, Train Loss: 0.0009351434418931603 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3170 minutes\n",
      "encoder learning rate: 8.99e-04, decoder learning rate: 8.99e-04\n",
      "[74/200] At -2.0 dB, Train Loss: 0.04695245623588562 Train BER 0.0019621620886027813,                  \n",
      " [74/200] At 0.0 dB, Train Loss: 0.000882726744748652 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.2456 minutes\n",
      "encoder learning rate: 8.93e-04, decoder learning rate: 8.93e-04\n",
      "[75/200] At -2.0 dB, Train Loss: 0.0471770316362381 Train BER 0.002075675642117858,                  \n",
      " [75/200] At 0.0 dB, Train Loss: 0.000344862841302529 Train BER 0.0\n",
      "Time for one full iteration is 7.3664 minutes\n",
      "encoder learning rate: 8.87e-04, decoder learning rate: 8.87e-04\n",
      "[76/200] At -2.0 dB, Train Loss: 0.03185601159930229 Train BER 0.0012432432267814875,                  \n",
      " [76/200] At 0.0 dB, Train Loss: 0.0004119627410545945 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3545 minutes\n",
      "encoder learning rate: 8.80e-04, decoder learning rate: 8.80e-04\n",
      "[77/200] At -2.0 dB, Train Loss: 0.03158874437212944 Train BER 0.0013135134940966964,                  \n",
      " [77/200] At 0.0 dB, Train Loss: 0.0007136182393878698 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 7.3677 minutes\n",
      "encoder learning rate: 8.74e-04, decoder learning rate: 8.74e-04\n",
      "[78/200] At -2.0 dB, Train Loss: 0.03882478550076485 Train BER 0.0017729729879647493,                  \n",
      " [78/200] At 0.0 dB, Train Loss: 0.00032019338686950505 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3914 minutes\n",
      "encoder learning rate: 8.67e-04, decoder learning rate: 8.67e-04\n",
      "[79/200] At -2.0 dB, Train Loss: 0.0456540584564209 Train BER 0.0019459458999335766,                  \n",
      " [79/200] At 0.0 dB, Train Loss: 0.0007729261997155845 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 7.4208 minutes\n",
      "encoder learning rate: 8.61e-04, decoder learning rate: 8.61e-04\n",
      "[80/200] At -2.0 dB, Train Loss: 0.03448576480150223 Train BER 0.0013675675727427006,                  \n",
      " [80/200] At 0.0 dB, Train Loss: 0.00016151949239429086 Train BER 0.0\n",
      "Time for one full iteration is 7.4662 minutes\n",
      "encoder learning rate: 8.54e-04, decoder learning rate: 8.54e-04\n",
      "[81/200] At -2.0 dB, Train Loss: 0.03481893613934517 Train BER 0.0016378378495573997,                  \n",
      " [81/200] At 0.0 dB, Train Loss: 0.00020441770902834833 Train BER 0.0\n",
      "Time for one full iteration is 7.4000 minutes\n",
      "encoder learning rate: 8.47e-04, decoder learning rate: 8.47e-04\n",
      "[82/200] At -2.0 dB, Train Loss: 0.04329719394445419 Train BER 0.0018918919377028942,                  \n",
      " [82/200] At 0.0 dB, Train Loss: 0.00037098320899531245 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2632 minutes\n",
      "encoder learning rate: 8.40e-04, decoder learning rate: 8.40e-04\n",
      "[83/200] At -2.0 dB, Train Loss: 0.03304402157664299 Train BER 0.0012054054532200098,                  \n",
      " [83/200] At 0.0 dB, Train Loss: 0.00017251090321224183 Train BER 0.0\n",
      "Time for one full iteration is 7.2790 minutes\n",
      "encoder learning rate: 8.32e-04, decoder learning rate: 8.32e-04\n",
      "[84/200] At -2.0 dB, Train Loss: 0.04124811291694641 Train BER 0.002102702623233199,                  \n",
      " [84/200] At 0.0 dB, Train Loss: 0.00019807199714705348 Train BER 0.0\n",
      "Time for one full iteration is 7.4016 minutes\n",
      "encoder learning rate: 8.25e-04, decoder learning rate: 8.25e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/200] At -2.0 dB, Train Loss: 0.03315471485257149 Train BER 0.0015297296922653913,                  \n",
      " [85/200] At 0.0 dB, Train Loss: 0.00031047483207657933 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3346 minutes\n",
      "encoder learning rate: 8.17e-04, decoder learning rate: 8.17e-04\n",
      "[86/200] At -2.0 dB, Train Loss: 0.028109295293688774 Train BER 0.0012432432267814875,                  \n",
      " [86/200] At 0.0 dB, Train Loss: 0.0003158923063892871 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4289 minutes\n",
      "encoder learning rate: 8.10e-04, decoder learning rate: 8.10e-04\n",
      "[87/200] At -2.0 dB, Train Loss: 0.03757896274328232 Train BER 0.0017513514030724764,                  \n",
      " [87/200] At 0.0 dB, Train Loss: 0.0002975079114548862 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4200 minutes\n",
      "encoder learning rate: 8.02e-04, decoder learning rate: 8.02e-04\n",
      "[88/200] At -2.0 dB, Train Loss: 0.03785979002714157 Train BER 0.0017459458904340863,                  \n",
      " [88/200] At 0.0 dB, Train Loss: 0.00028489931719377637 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3449 minutes\n",
      "encoder learning rate: 7.94e-04, decoder learning rate: 7.94e-04\n",
      "[89/200] At -2.0 dB, Train Loss: 0.035015493631362915 Train BER 0.0015135135035961866,                  \n",
      " [89/200] At 0.0 dB, Train Loss: 0.00015287271526176482 Train BER 0.0\n",
      "Time for one full iteration is 7.3666 minutes\n",
      "encoder learning rate: 7.86e-04, decoder learning rate: 7.86e-04\n",
      "[90/200] At -2.0 dB, Train Loss: 0.04721454158425331 Train BER 0.0020216216798871756,                  \n",
      " [90/200] At 0.0 dB, Train Loss: 0.0012508168583735824 Train BER 7.567567809019238e-05\n",
      "Time for one full iteration is 7.3984 minutes\n",
      "encoder learning rate: 7.78e-04, decoder learning rate: 7.78e-04\n",
      "[91/200] At -2.0 dB, Train Loss: 0.03772129490971565 Train BER 0.001713513513095677,                  \n",
      " [91/200] At 0.0 dB, Train Loss: 0.0002519021218176931 Train BER 0.0\n",
      "Time for one full iteration is 7.4466 minutes\n",
      "encoder learning rate: 7.70e-04, decoder learning rate: 7.70e-04\n",
      "[92/200] At -2.0 dB, Train Loss: 0.03592199832201004 Train BER 0.0015999999595806003,                  \n",
      " [92/200] At 0.0 dB, Train Loss: 0.00037476173019967973 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.2942 minutes\n",
      "encoder learning rate: 7.61e-04, decoder learning rate: 7.61e-04\n",
      "[93/200] At -2.0 dB, Train Loss: 0.03534373268485069 Train BER 0.0016162162646651268,                  \n",
      " [93/200] At 0.0 dB, Train Loss: 0.00020252894319128245 Train BER 0.0\n",
      "Time for one full iteration is 7.2454 minutes\n",
      "encoder learning rate: 7.53e-04, decoder learning rate: 7.53e-04\n",
      "[94/200] At -2.0 dB, Train Loss: 0.0351174920797348 Train BER 0.001621621660888195,                  \n",
      " [94/200] At 0.0 dB, Train Loss: 0.00015442691801581532 Train BER 0.0\n",
      "Time for one full iteration is 7.2798 minutes\n",
      "encoder learning rate: 7.45e-04, decoder learning rate: 7.45e-04\n",
      "[95/200] At -2.0 dB, Train Loss: 0.03395926207304001 Train BER 0.0014648648211732507,                  \n",
      " [95/200] At 0.0 dB, Train Loss: 0.0001365877833450213 Train BER 0.0\n",
      "Time for one full iteration is 7.4148 minutes\n",
      "encoder learning rate: 7.36e-04, decoder learning rate: 7.36e-04\n",
      "[96/200] At -2.0 dB, Train Loss: 0.032786253839731216 Train BER 0.0013243242865428329,                  \n",
      " [96/200] At 0.0 dB, Train Loss: 0.00016556275659240782 Train BER 0.0\n",
      "Time for one full iteration is 7.3339 minutes\n",
      "encoder learning rate: 7.27e-04, decoder learning rate: 7.27e-04\n",
      "[97/200] At -2.0 dB, Train Loss: 0.03439601510763168 Train BER 0.0016594594344496727,                  \n",
      " [97/200] At 0.0 dB, Train Loss: 0.00012980375322513282 Train BER 0.0\n",
      "Time for one full iteration is 7.4069 minutes\n",
      "encoder learning rate: 7.18e-04, decoder learning rate: 7.18e-04\n",
      "[98/200] At -2.0 dB, Train Loss: 0.039183832705020905 Train BER 0.0017297297017648816,                  \n",
      " [98/200] At 0.0 dB, Train Loss: 0.0013628611341118813 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 7.4868 minutes\n",
      "encoder learning rate: 7.10e-04, decoder learning rate: 7.10e-04\n",
      "[99/200] At -2.0 dB, Train Loss: 0.03843114525079727 Train BER 0.0019621620886027813,                  \n",
      " [99/200] At 0.0 dB, Train Loss: 0.00018068939971271902 Train BER 0.0\n",
      "Time for one full iteration is 7.3805 minutes\n",
      "encoder learning rate: 7.01e-04, decoder learning rate: 7.01e-04\n",
      "[100/200] At -2.0 dB, Train Loss: 0.03537445515394211 Train BER 0.0017243243055418134,                  \n",
      " [100/200] At 0.0 dB, Train Loss: 0.00016744184540584683 Train BER 0.0\n",
      "Time for one full iteration is 7.3476 minutes\n",
      "encoder learning rate: 6.92e-04, decoder learning rate: 6.92e-04\n",
      "[101/200] At -2.0 dB, Train Loss: 0.034074898809194565 Train BER 0.0014162162551656365,                  \n",
      " [101/200] At 0.0 dB, Train Loss: 0.00022831419482827187 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2761 minutes\n",
      "encoder learning rate: 6.83e-04, decoder learning rate: 6.83e-04\n",
      "[102/200] At -2.0 dB, Train Loss: 0.029821356758475304 Train BER 0.00130270270165056,                  \n",
      " [102/200] At 0.0 dB, Train Loss: 0.0002925411390606314 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.4544 minutes\n",
      "encoder learning rate: 6.73e-04, decoder learning rate: 6.73e-04\n",
      "[103/200] At -2.0 dB, Train Loss: 0.03261586278676987 Train BER 0.0015675675822421908,                  \n",
      " [103/200] At 0.0 dB, Train Loss: 0.0001427925017196685 Train BER 0.0\n",
      "Time for one full iteration is 7.3599 minutes\n",
      "encoder learning rate: 6.64e-04, decoder learning rate: 6.64e-04\n",
      "[104/200] At -2.0 dB, Train Loss: 0.030159959569573402 Train BER 0.0014648648211732507,                  \n",
      " [104/200] At 0.0 dB, Train Loss: 0.0002538065309636295 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3701 minutes\n",
      "encoder learning rate: 6.55e-04, decoder learning rate: 6.55e-04\n",
      "[105/200] At -2.0 dB, Train Loss: 0.03821684047579765 Train BER 0.0017027027206495404,                  \n",
      " [105/200] At 0.0 dB, Train Loss: 0.00013027362001594156 Train BER 0.0\n",
      "Time for one full iteration is 7.3056 minutes\n",
      "encoder learning rate: 6.45e-04, decoder learning rate: 6.45e-04\n",
      "[106/200] At -2.0 dB, Train Loss: 0.03420114144682884 Train BER 0.0015297296922653913,                  \n",
      " [106/200] At 0.0 dB, Train Loss: 0.0009111692779697478 Train BER 5.405405318015255e-05\n",
      "Time for one full iteration is 7.4131 minutes\n",
      "encoder learning rate: 6.36e-04, decoder learning rate: 6.36e-04\n",
      "[107/200] At -2.0 dB, Train Loss: 0.033748555928468704 Train BER 0.0015837837709113955,                  \n",
      " [107/200] At 0.0 dB, Train Loss: 0.0003634012828115374 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3170 minutes\n",
      "encoder learning rate: 6.27e-04, decoder learning rate: 6.27e-04\n",
      "[108/200] At -2.0 dB, Train Loss: 0.029365209862589836 Train BER 0.0012864865129813552,                  \n",
      " [108/200] At 0.0 dB, Train Loss: 0.0002339704951737076 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3850 minutes\n",
      "encoder learning rate: 6.17e-04, decoder learning rate: 6.17e-04\n",
      "[109/200] At -2.0 dB, Train Loss: 0.03830999508500099 Train BER 0.0016864865319803357,                  \n",
      " [109/200] At 0.0 dB, Train Loss: 0.00015937357966322452 Train BER 0.0\n",
      "Time for one full iteration is 7.4093 minutes\n",
      "encoder learning rate: 6.08e-04, decoder learning rate: 6.08e-04\n",
      "[110/200] At -2.0 dB, Train Loss: 0.03394143283367157 Train BER 0.0017243243055418134,                  \n",
      " [110/200] At 0.0 dB, Train Loss: 7.047390681691468e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2331 minutes\n",
      "encoder learning rate: 5.98e-04, decoder learning rate: 5.98e-04\n",
      "[111/200] At -2.0 dB, Train Loss: 0.030381031334400177 Train BER 0.0012594594154506922,                  \n",
      " [111/200] At 0.0 dB, Train Loss: 8.644865738460794e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5602 minutes\n",
      "encoder learning rate: 5.88e-04, decoder learning rate: 5.88e-04\n",
      "[112/200] At -2.0 dB, Train Loss: 0.03354315459728241 Train BER 0.0017027027206495404,                  \n",
      " [112/200] At 0.0 dB, Train Loss: 0.00010614205530146137 Train BER 0.0\n",
      "Time for one full iteration is 7.3396 minutes\n",
      "encoder learning rate: 5.79e-04, decoder learning rate: 5.79e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/200] At -2.0 dB, Train Loss: 0.02893536351621151 Train BER 0.0014216216513887048,                  \n",
      " [113/200] At 0.0 dB, Train Loss: 0.00010949364514090121 Train BER 0.0\n",
      "Time for one full iteration is 7.1994 minutes\n",
      "encoder learning rate: 5.69e-04, decoder learning rate: 5.69e-04\n",
      "[114/200] At -2.0 dB, Train Loss: 0.03337288275361061 Train BER 0.0015405404847115278,                  \n",
      " [114/200] At 0.0 dB, Train Loss: 8.927466114982963e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.4451 minutes\n",
      "encoder learning rate: 5.59e-04, decoder learning rate: 5.59e-04\n",
      "[115/200] At -2.0 dB, Train Loss: 0.04247177019715309 Train BER 0.0020432432647794485,                  \n",
      " [115/200] At 0.0 dB, Train Loss: 9.242470696335658e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2921 minutes\n",
      "encoder learning rate: 5.49e-04, decoder learning rate: 5.49e-04\n",
      "[116/200] At -2.0 dB, Train Loss: 0.033391837030649185 Train BER 0.0015405404847115278,                  \n",
      " [116/200] At 0.0 dB, Train Loss: 9.445075556868687e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1896 minutes\n",
      "encoder learning rate: 5.40e-04, decoder learning rate: 5.40e-04\n",
      "[117/200] At -2.0 dB, Train Loss: 0.02680191770195961 Train BER 0.0012054054532200098,                  \n",
      " [117/200] At 0.0 dB, Train Loss: 0.00010205584112554789 Train BER 0.0\n",
      "Time for one full iteration is 7.3752 minutes\n",
      "encoder learning rate: 5.30e-04, decoder learning rate: 5.30e-04\n",
      "[118/200] At -2.0 dB, Train Loss: 0.026530202478170395 Train BER 0.0012486486230045557,                  \n",
      " [118/200] At 0.0 dB, Train Loss: 0.00014519439719151706 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.1072 minutes\n",
      "encoder learning rate: 5.20e-04, decoder learning rate: 5.20e-04\n",
      "[119/200] At -2.0 dB, Train Loss: 0.038219429552555084 Train BER 0.0016972973244264722,                  \n",
      " [119/200] At 0.0 dB, Train Loss: 0.00022617183276452124 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.0762 minutes\n",
      "encoder learning rate: 5.10e-04, decoder learning rate: 5.10e-04\n",
      "[120/200] At -2.0 dB, Train Loss: 0.02609149180352688 Train BER 0.0011513513745740056,                  \n",
      " [120/200] At 0.0 dB, Train Loss: 0.00023888549185357988 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.0732 minutes\n",
      "encoder learning rate: 5.01e-04, decoder learning rate: 5.01e-04\n",
      "[121/200] At -2.0 dB, Train Loss: 0.03460100293159485 Train BER 0.0015999999595806003,                  \n",
      " [121/200] At 0.0 dB, Train Loss: 0.0014225599588826299 Train BER 0.00011891892063431442\n",
      "Time for one full iteration is 7.1578 minutes\n",
      "encoder learning rate: 4.91e-04, decoder learning rate: 4.91e-04\n",
      "[122/200] At -2.0 dB, Train Loss: 0.028153859078884125 Train BER 0.0012648648116737604,                  \n",
      " [122/200] At 0.0 dB, Train Loss: 0.00014342271606437862 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8845 minutes\n",
      "encoder learning rate: 4.81e-04, decoder learning rate: 4.81e-04\n",
      "[123/200] At -2.0 dB, Train Loss: 0.02806105464696884 Train BER 0.0012594594154506922,                  \n",
      " [123/200] At 0.0 dB, Train Loss: 0.00011767672549467534 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8699 minutes\n",
      "encoder learning rate: 4.71e-04, decoder learning rate: 4.71e-04\n",
      "[124/200] At -2.0 dB, Train Loss: 0.03346935659646988 Train BER 0.0016486486420035362,                  \n",
      " [124/200] At 0.0 dB, Train Loss: 0.00014429999282583594 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8765 minutes\n",
      "encoder learning rate: 4.61e-04, decoder learning rate: 4.61e-04\n",
      "[125/200] At -2.0 dB, Train Loss: 0.029321890324354172 Train BER 0.0015405404847115278,                  \n",
      " [125/200] At 0.0 dB, Train Loss: 0.00035138626117259264 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 7.0165 minutes\n",
      "encoder learning rate: 4.52e-04, decoder learning rate: 4.52e-04\n",
      "[126/200] At -2.0 dB, Train Loss: 0.03312301263213158 Train BER 0.0016162162646651268,                  \n",
      " [126/200] At 0.0 dB, Train Loss: 4.5663105993298814e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8512 minutes\n",
      "encoder learning rate: 4.42e-04, decoder learning rate: 4.42e-04\n",
      "[127/200] At -2.0 dB, Train Loss: 0.03331015259027481 Train BER 0.0015243242960423231,                  \n",
      " [127/200] At 0.0 dB, Train Loss: 0.00012621124915312976 Train BER 0.0\n",
      "Time for one full iteration is 6.8381 minutes\n",
      "encoder learning rate: 4.32e-04, decoder learning rate: 4.32e-04\n",
      "[128/200] At -2.0 dB, Train Loss: 0.03718266636133194 Train BER 0.0017297297017648816,                  \n",
      " [128/200] At 0.0 dB, Train Loss: 0.00013164564734324813 Train BER 0.0\n",
      "Time for one full iteration is 6.8359 minutes\n",
      "encoder learning rate: 4.22e-04, decoder learning rate: 4.22e-04\n",
      "[129/200] At -2.0 dB, Train Loss: 0.0237924512475729 Train BER 0.0011189188808202744,                  \n",
      " [129/200] At 0.0 dB, Train Loss: 0.0006578254979103804 Train BER 3.783783904509619e-05\n",
      "Time for one full iteration is 6.8092 minutes\n",
      "encoder learning rate: 4.13e-04, decoder learning rate: 4.13e-04\n",
      "[130/200] At -2.0 dB, Train Loss: 0.025110971182584763 Train BER 0.0011405405821278691,                  \n",
      " [130/200] At 0.0 dB, Train Loss: 0.00035373750142753124 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 6.8675 minutes\n",
      "encoder learning rate: 4.03e-04, decoder learning rate: 4.03e-04\n",
      "[131/200] At -2.0 dB, Train Loss: 0.02854299172759056 Train BER 0.0014864865224808455,                  \n",
      " [131/200] At 0.0 dB, Train Loss: 8.093490032479167e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8622 minutes\n",
      "encoder learning rate: 3.93e-04, decoder learning rate: 3.93e-04\n",
      "[132/200] At -2.0 dB, Train Loss: 0.026253677904605865 Train BER 0.0013459459878504276,                  \n",
      " [132/200] At 0.0 dB, Train Loss: 0.00016564832185395062 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8364 minutes\n",
      "encoder learning rate: 3.84e-04, decoder learning rate: 3.84e-04\n",
      "[133/200] At -2.0 dB, Train Loss: 0.02749481052160263 Train BER 0.0012594594154506922,                  \n",
      " [133/200] At 0.0 dB, Train Loss: 0.00019134093599859625 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8778 minutes\n",
      "encoder learning rate: 3.74e-04, decoder learning rate: 3.74e-04\n",
      "[134/200] At -2.0 dB, Train Loss: 0.029524732381105423 Train BER 0.0013891891576349735,                  \n",
      " [134/200] At 0.0 dB, Train Loss: 8.51640579639934e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8339 minutes\n",
      "encoder learning rate: 3.65e-04, decoder learning rate: 3.65e-04\n",
      "[135/200] At -2.0 dB, Train Loss: 0.024629753082990646 Train BER 0.0013189188903197646,                  \n",
      " [135/200] At 0.0 dB, Train Loss: 0.00011750201520044357 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8916 minutes\n",
      "encoder learning rate: 3.56e-04, decoder learning rate: 3.56e-04\n",
      "[136/200] At -2.0 dB, Train Loss: 0.03469207510352135 Train BER 0.0018054053653031588,                  \n",
      " [136/200] At 0.0 dB, Train Loss: 0.00027795936330221593 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.0730 minutes\n",
      "encoder learning rate: 3.46e-04, decoder learning rate: 3.46e-04\n",
      "[137/200] At -2.0 dB, Train Loss: 0.021998662501573563 Train BER 0.0010432432172819972,                  \n",
      " [137/200] At 0.0 dB, Train Loss: 0.00013322611630428582 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.0128 minutes\n",
      "encoder learning rate: 3.37e-04, decoder learning rate: 3.37e-04\n",
      "[138/200] At -2.0 dB, Train Loss: 0.02348296344280243 Train BER 0.0011081080883741379,                  \n",
      " [138/200] At 0.0 dB, Train Loss: 5.071665873401798e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.9511 minutes\n",
      "encoder learning rate: 3.28e-04, decoder learning rate: 3.28e-04\n",
      "[139/200] At -2.0 dB, Train Loss: 0.025282317772507668 Train BER 0.0011189188808202744,                  \n",
      " [139/200] At 0.0 dB, Train Loss: 0.00012747003347612917 Train BER 0.0\n",
      "Time for one full iteration is 6.9104 minutes\n",
      "encoder learning rate: 3.18e-04, decoder learning rate: 3.18e-04\n",
      "[140/200] At -2.0 dB, Train Loss: 0.022339772433042526 Train BER 0.0010216216323897243,                  \n",
      " [140/200] At 0.0 dB, Train Loss: 0.00012433964002411813 Train BER 0.0\n",
      "Time for one full iteration is 6.8193 minutes\n",
      "encoder learning rate: 3.09e-04, decoder learning rate: 3.09e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141/200] At -2.0 dB, Train Loss: 0.01830018311738968 Train BER 0.0008432432659901679,                  \n",
      " [141/200] At 0.0 dB, Train Loss: 0.0001013093424262479 Train BER 0.0\n",
      "Time for one full iteration is 6.8565 minutes\n",
      "encoder learning rate: 3.00e-04, decoder learning rate: 3.00e-04\n",
      "[142/200] At -2.0 dB, Train Loss: 0.028700372204184532 Train BER 0.0013675675727427006,                  \n",
      " [142/200] At 0.0 dB, Train Loss: 0.0001957991044037044 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.9238 minutes\n",
      "encoder learning rate: 2.91e-04, decoder learning rate: 2.91e-04\n",
      "[143/200] At -2.0 dB, Train Loss: 0.03661938011646271 Train BER 0.0016378378495573997,                  \n",
      " [143/200] At 0.0 dB, Train Loss: 0.00014954738435335457 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 6.8387 minutes\n",
      "encoder learning rate: 2.83e-04, decoder learning rate: 2.83e-04\n",
      "[144/200] At -2.0 dB, Train Loss: 0.025068743154406548 Train BER 0.0011081080883741379,                  \n",
      " [144/200] At 0.0 dB, Train Loss: 3.895830377587117e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8051 minutes\n",
      "encoder learning rate: 2.74e-04, decoder learning rate: 2.74e-04\n",
      "[145/200] At -2.0 dB, Train Loss: 0.02713705413043499 Train BER 0.0013081080978736281,                  \n",
      " [145/200] At 0.0 dB, Train Loss: 0.00035136271617375314 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 6.7820 minutes\n",
      "encoder learning rate: 2.65e-04, decoder learning rate: 2.65e-04\n",
      "[146/200] At -2.0 dB, Train Loss: 0.028960485011339188 Train BER 0.0015297296922653913,                  \n",
      " [146/200] At 0.0 dB, Train Loss: 6.835667591076344e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.9921 minutes\n",
      "encoder learning rate: 2.56e-04, decoder learning rate: 2.56e-04\n",
      "[147/200] At -2.0 dB, Train Loss: 0.021378811448812485 Train BER 0.0009837837424129248,                  \n",
      " [147/200] At 0.0 dB, Train Loss: 0.0002003614790737629 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.1356 minutes\n",
      "encoder learning rate: 2.48e-04, decoder learning rate: 2.48e-04\n",
      "[148/200] At -2.0 dB, Train Loss: 0.03875814005732536 Train BER 0.0019459458999335766,                  \n",
      " [148/200] At 0.0 dB, Train Loss: 0.00012320603127591312 Train BER 0.0\n",
      "Time for one full iteration is 6.9388 minutes\n",
      "encoder learning rate: 2.40e-04, decoder learning rate: 2.40e-04\n",
      "[149/200] At -2.0 dB, Train Loss: 0.022533439099788666 Train BER 0.001059459405951202,                  \n",
      " [149/200] At 0.0 dB, Train Loss: 9.363365825265646e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.1084 minutes\n",
      "encoder learning rate: 2.31e-04, decoder learning rate: 2.31e-04\n",
      "[150/200] At -2.0 dB, Train Loss: 0.030799346044659615 Train BER 0.0014162162551656365,                  \n",
      " [150/200] At 0.0 dB, Train Loss: 0.00028468266827985644 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.0883 minutes\n",
      "encoder learning rate: 2.23e-04, decoder learning rate: 2.23e-04\n",
      "[151/200] At -2.0 dB, Train Loss: 0.03192290663719177 Train BER 0.0015297296922653913,                  \n",
      " [151/200] At 0.0 dB, Train Loss: 8.078677637968212e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.9133 minutes\n",
      "encoder learning rate: 2.15e-04, decoder learning rate: 2.15e-04\n",
      "[152/200] At -2.0 dB, Train Loss: 0.024583250284194946 Train BER 0.001064864918589592,                  \n",
      " [152/200] At 0.0 dB, Train Loss: 8.191465894924477e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.1305 minutes\n",
      "encoder learning rate: 2.07e-04, decoder learning rate: 2.07e-04\n",
      "[153/200] At -2.0 dB, Train Loss: 0.024927381426095963 Train BER 0.0011081080883741379,                  \n",
      " [153/200] At 0.0 dB, Train Loss: 0.0004241444985382259 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 7.1279 minutes\n",
      "encoder learning rate: 1.99e-04, decoder learning rate: 1.99e-04\n",
      "[154/200] At -2.0 dB, Train Loss: 0.027777526527643204 Train BER 0.00139999995008111,                  \n",
      " [154/200] At 0.0 dB, Train Loss: 0.0001429750263923779 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8075 minutes\n",
      "encoder learning rate: 1.91e-04, decoder learning rate: 1.91e-04\n",
      "[155/200] At -2.0 dB, Train Loss: 0.021528849378228188 Train BER 0.000994594651274383,                  \n",
      " [155/200] At 0.0 dB, Train Loss: 0.00024232122814282775 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8439 minutes\n",
      "encoder learning rate: 1.84e-04, decoder learning rate: 1.84e-04\n",
      "[156/200] At -2.0 dB, Train Loss: 0.02383020520210266 Train BER 0.001037837821058929,                  \n",
      " [156/200] At 0.0 dB, Train Loss: 3.9956605178304017e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3055 minutes\n",
      "encoder learning rate: 1.76e-04, decoder learning rate: 1.76e-04\n",
      "[157/200] At -2.0 dB, Train Loss: 0.03249477967619896 Train BER 0.0015081081073731184,                  \n",
      " [157/200] At 0.0 dB, Train Loss: 7.013171125436202e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8540 minutes\n",
      "encoder learning rate: 1.69e-04, decoder learning rate: 1.69e-04\n",
      "[158/200] At -2.0 dB, Train Loss: 0.02993805520236492 Train BER 0.0014162162551656365,                  \n",
      " [158/200] At 0.0 dB, Train Loss: 0.00013395377027336508 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2709 minutes\n",
      "encoder learning rate: 1.61e-04, decoder learning rate: 1.61e-04\n",
      "[159/200] At -2.0 dB, Train Loss: 0.029730824753642082 Train BER 0.0013459459878504276,                  \n",
      " [159/200] At 0.0 dB, Train Loss: 6.110049434937537e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8353 minutes\n",
      "encoder learning rate: 1.54e-04, decoder learning rate: 1.54e-04\n",
      "[160/200] At -2.0 dB, Train Loss: 0.02380075491964817 Train BER 0.0010432432172819972,                  \n",
      " [160/200] At 0.0 dB, Train Loss: 0.0006977737648412585 Train BER 2.162162127206102e-05\n",
      "Time for one full iteration is 6.9639 minutes\n",
      "encoder learning rate: 1.47e-04, decoder learning rate: 1.47e-04\n",
      "[161/200] At -2.0 dB, Train Loss: 0.022923065349459648 Train BER 0.0010702703148126602,                  \n",
      " [161/200] At 0.0 dB, Train Loss: 5.952631545369513e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.8600 minutes\n",
      "encoder learning rate: 1.40e-04, decoder learning rate: 1.40e-04\n",
      "[162/200] At -2.0 dB, Train Loss: 0.019915012642741203 Train BER 0.0010918918997049332,                  \n",
      " [162/200] At 0.0 dB, Train Loss: 0.000655664480291307 Train BER 2.7027026590076275e-05\n",
      "Time for one full iteration is 6.7794 minutes\n",
      "encoder learning rate: 1.34e-04, decoder learning rate: 1.34e-04\n",
      "[163/200] At -2.0 dB, Train Loss: 0.020997105166316032 Train BER 0.0008864864939823747,                  \n",
      " [163/200] At 0.0 dB, Train Loss: 0.00012401628191582859 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8538 minutes\n",
      "encoder learning rate: 1.27e-04, decoder learning rate: 1.27e-04\n",
      "[164/200] At -2.0 dB, Train Loss: 0.02327675186097622 Train BER 0.001162162167020142,                  \n",
      " [164/200] At 0.0 dB, Train Loss: 0.0001415318693034351 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.2546 minutes\n",
      "encoder learning rate: 1.21e-04, decoder learning rate: 1.21e-04\n",
      "[165/200] At -2.0 dB, Train Loss: 0.024693481624126434 Train BER 0.001135135185904801,                  \n",
      " [165/200] At 0.0 dB, Train Loss: 0.00010459496115799993 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.0887 minutes\n",
      "encoder learning rate: 1.14e-04, decoder learning rate: 1.14e-04\n",
      "[166/200] At -2.0 dB, Train Loss: 0.02215757593512535 Train BER 0.0010000000474974513,                  \n",
      " [166/200] At 0.0 dB, Train Loss: 0.00013245519949123263 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8136 minutes\n",
      "encoder learning rate: 1.08e-04, decoder learning rate: 1.08e-04\n",
      "[167/200] At -2.0 dB, Train Loss: 0.024790549650788307 Train BER 0.0011405405821278691,                  \n",
      " [167/200] At 0.0 dB, Train Loss: 4.470916974241845e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.2342 minutes\n",
      "encoder learning rate: 1.02e-04, decoder learning rate: 1.02e-04\n",
      "[168/200] At -2.0 dB, Train Loss: 0.02264215797185898 Train BER 0.0010270270286127925,                  \n",
      " [168/200] At 0.0 dB, Train Loss: 0.00011740684567485005 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3724 minutes\n",
      "encoder learning rate: 9.64e-05, decoder learning rate: 9.64e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169/200] At -2.0 dB, Train Loss: 0.02482166327536106 Train BER 0.0009783783461898565,                  \n",
      " [169/200] At 0.0 dB, Train Loss: 0.00011313849245198071 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8906 minutes\n",
      "encoder learning rate: 9.07e-05, decoder learning rate: 9.07e-05\n",
      "[170/200] At -2.0 dB, Train Loss: 0.02709127590060234 Train BER 0.0010810811072587967,                  \n",
      " [170/200] At 0.0 dB, Train Loss: 0.00010041270434157923 Train BER 0.0\n",
      "Time for one full iteration is 6.7705 minutes\n",
      "encoder learning rate: 8.52e-05, decoder learning rate: 8.52e-05\n",
      "[171/200] At -2.0 dB, Train Loss: 0.025058072060346603 Train BER 0.0012162162456661463,                  \n",
      " [171/200] At 0.0 dB, Train Loss: 5.014695489080623e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7641 minutes\n",
      "encoder learning rate: 7.98e-05, decoder learning rate: 7.98e-05\n",
      "[172/200] At -2.0 dB, Train Loss: 0.023958640173077583 Train BER 0.0010702703148126602,                  \n",
      " [172/200] At 0.0 dB, Train Loss: 8.073364006122574e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7642 minutes\n",
      "encoder learning rate: 7.46e-05, decoder learning rate: 7.46e-05\n",
      "[173/200] At -2.0 dB, Train Loss: 0.02615281008183956 Train BER 0.0013081080978736281,                  \n",
      " [173/200] At 0.0 dB, Train Loss: 0.00014674186240881681 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.9055 minutes\n",
      "encoder learning rate: 6.96e-05, decoder learning rate: 6.96e-05\n",
      "[174/200] At -2.0 dB, Train Loss: 0.01897239498794079 Train BER 0.0008864864939823747,                  \n",
      " [174/200] At 0.0 dB, Train Loss: 0.00011928692401852459 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.5420 minutes\n",
      "encoder learning rate: 6.47e-05, decoder learning rate: 6.47e-05\n",
      "[175/200] At -2.0 dB, Train Loss: 0.025500420480966568 Train BER 0.0011135134845972061,                  \n",
      " [175/200] At 0.0 dB, Train Loss: 5.813555253553204e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3132 minutes\n",
      "encoder learning rate: 6.00e-05, decoder learning rate: 6.00e-05\n",
      "[176/200] At -2.0 dB, Train Loss: 0.026864992454648018 Train BER 0.0012864865129813552,                  \n",
      " [176/200] At 0.0 dB, Train Loss: 0.0002194681146647781 Train BER 1.6216215954045765e-05\n",
      "Time for one full iteration is 6.7529 minutes\n",
      "encoder learning rate: 5.54e-05, decoder learning rate: 5.54e-05\n",
      "[177/200] At -2.0 dB, Train Loss: 0.02040533907711506 Train BER 0.0008918918902054429,                  \n",
      " [177/200] At 0.0 dB, Train Loss: 3.113092316198163e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7640 minutes\n",
      "encoder learning rate: 5.11e-05, decoder learning rate: 5.11e-05\n",
      "[178/200] At -2.0 dB, Train Loss: 0.020983338356018066 Train BER 0.0008972972864285111,                  \n",
      " [178/200] At 0.0 dB, Train Loss: 7.14466514182277e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7458 minutes\n",
      "encoder learning rate: 4.69e-05, decoder learning rate: 4.69e-05\n",
      "[179/200] At -2.0 dB, Train Loss: 0.024251190945506096 Train BER 0.0011783783556893468,                  \n",
      " [179/200] At 0.0 dB, Train Loss: 4.588722003973089e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7575 minutes\n",
      "encoder learning rate: 4.29e-05, decoder learning rate: 4.29e-05\n",
      "[180/200] At -2.0 dB, Train Loss: 0.033437103033065796 Train BER 0.0017945945728570223,                  \n",
      " [180/200] At 0.0 dB, Train Loss: 0.0001013248402159661 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.7518 minutes\n",
      "encoder learning rate: 3.90e-05, decoder learning rate: 3.90e-05\n",
      "[181/200] At -2.0 dB, Train Loss: 0.03216946870088577 Train BER 0.0015675675822421908,                  \n",
      " [181/200] At 0.0 dB, Train Loss: 0.0007452113786712289 Train BER 3.243243190809153e-05\n",
      "Time for one full iteration is 6.7445 minutes\n",
      "encoder learning rate: 3.54e-05, decoder learning rate: 3.54e-05\n",
      "[182/200] At -2.0 dB, Train Loss: 0.020204275846481323 Train BER 0.0010054054437205195,                  \n",
      " [182/200] At 0.0 dB, Train Loss: 8.517398964613676e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7747 minutes\n",
      "encoder learning rate: 3.19e-05, decoder learning rate: 3.19e-05\n",
      "[183/200] At -2.0 dB, Train Loss: 0.02594878152012825 Train BER 0.001162162167020142,                  \n",
      " [183/200] At 0.0 dB, Train Loss: 8.475991489831358e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7418 minutes\n",
      "encoder learning rate: 2.86e-05, decoder learning rate: 2.86e-05\n",
      "[184/200] At -2.0 dB, Train Loss: 0.027052750810980797 Train BER 0.0013945945538580418,                  \n",
      " [184/200] At 0.0 dB, Train Loss: 0.0001926964905578643 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.7314 minutes\n",
      "encoder learning rate: 2.54e-05, decoder learning rate: 2.54e-05\n",
      "[185/200] At -2.0 dB, Train Loss: 0.020429834723472595 Train BER 0.0008918918902054429,                  \n",
      " [185/200] At 0.0 dB, Train Loss: 4.7171841288218275e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7338 minutes\n",
      "encoder learning rate: 2.25e-05, decoder learning rate: 2.25e-05\n",
      "[186/200] At -2.0 dB, Train Loss: 0.028027081862092018 Train BER 0.0011945945443585515,                  \n",
      " [186/200] At 0.0 dB, Train Loss: 0.00017776101594790816 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 6.7561 minutes\n",
      "encoder learning rate: 1.98e-05, decoder learning rate: 1.98e-05\n",
      "[187/200] At -2.0 dB, Train Loss: 0.03379413112998009 Train BER 0.0015351350884884596,                  \n",
      " [187/200] At 0.0 dB, Train Loss: 0.00011702974006766453 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.7611 minutes\n",
      "encoder learning rate: 1.72e-05, decoder learning rate: 1.72e-05\n",
      "[188/200] At -2.0 dB, Train Loss: 0.02394847385585308 Train BER 0.0010756757110357285,                  \n",
      " [188/200] At 0.0 dB, Train Loss: 0.00018503826868254691 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.7739 minutes\n",
      "encoder learning rate: 1.48e-05, decoder learning rate: 1.48e-05\n",
      "[189/200] At -2.0 dB, Train Loss: 0.021383680403232574 Train BER 0.0011135134845972061,                  \n",
      " [189/200] At 0.0 dB, Train Loss: 2.0586285245371982e-05 Train BER 0.0\n",
      "Time for one full iteration is 6.7844 minutes\n",
      "encoder learning rate: 1.26e-05, decoder learning rate: 1.26e-05\n",
      "[190/200] At -2.0 dB, Train Loss: 0.026288405060768127 Train BER 0.0013135134940966964,                  \n",
      " [190/200] At 0.0 dB, Train Loss: 0.0001349514495814219 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 6.8845 minutes\n",
      "encoder learning rate: 1.06e-05, decoder learning rate: 1.06e-05\n",
      "[191/200] At -2.0 dB, Train Loss: 0.02281804569065571 Train BER 0.001135135185904801,                  \n",
      " [191/200] At 0.0 dB, Train Loss: 3.0152337785693817e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.0719 minutes\n",
      "encoder learning rate: 8.78e-06, decoder learning rate: 8.78e-06\n",
      "[192/200] At -2.0 dB, Train Loss: 0.02119896560907364 Train BER 0.0010810811072587967,                  \n",
      " [192/200] At 0.0 dB, Train Loss: 0.00022301750141195953 Train BER 1.081081063603051e-05\n",
      "Time for one full iteration is 7.3591 minutes\n",
      "encoder learning rate: 7.15e-06, decoder learning rate: 7.15e-06\n",
      "[193/200] At -2.0 dB, Train Loss: 0.03041166439652443 Train BER 0.0014324324438348413,                  \n",
      " [193/200] At 0.0 dB, Train Loss: 6.661655061179772e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.4962 minutes\n",
      "encoder learning rate: 5.71e-06, decoder learning rate: 5.71e-06\n",
      "[194/200] At -2.0 dB, Train Loss: 0.023157143965363503 Train BER 0.0011945945443585515,                  \n",
      " [194/200] At 0.0 dB, Train Loss: 2.4236051103798673e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3343 minutes\n",
      "encoder learning rate: 4.46e-06, decoder learning rate: 4.46e-06\n",
      "[195/200] At -2.0 dB, Train Loss: 0.022235233336687088 Train BER 0.000989189138635993,                  \n",
      " [195/200] At 0.0 dB, Train Loss: 1.920899740071036e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.5060 minutes\n",
      "encoder learning rate: 3.41e-06, decoder learning rate: 3.41e-06\n",
      "[196/200] At -2.0 dB, Train Loss: 0.023923560976982117 Train BER 0.0012000000569969416,                  \n",
      " [196/200] At 0.0 dB, Train Loss: 5.956088352832012e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3089 minutes\n",
      "encoder learning rate: 2.54e-06, decoder learning rate: 2.54e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/200] At -2.0 dB, Train Loss: 0.023097295314073563 Train BER 0.0011027026921510696,                  \n",
      " [197/200] At 0.0 dB, Train Loss: 6.418114935513586e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.4403 minutes\n",
      "encoder learning rate: 1.87e-06, decoder learning rate: 1.87e-06\n",
      "[198/200] At -2.0 dB, Train Loss: 0.025676660239696503 Train BER 0.0012162162456661463,                  \n",
      " [198/200] At 0.0 dB, Train Loss: 3.158494291710667e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3827 minutes\n",
      "encoder learning rate: 1.39e-06, decoder learning rate: 1.39e-06\n",
      "[199/200] At -2.0 dB, Train Loss: 0.023837080225348473 Train BER 0.0011513513745740056,                  \n",
      " [199/200] At 0.0 dB, Train Loss: 3.274253322160803e-05 Train BER 0.0\n",
      "Time for one full iteration is 7.3047 minutes\n",
      "encoder learning rate: 1.10e-06, decoder learning rate: 1.10e-06\n",
      "[200/200] At -2.0 dB, Train Loss: 0.024400288239121437 Train BER 0.0009675675537437201,                  \n",
      " [200/200] At 0.0 dB, Train Loss: 9.418659465154633e-05 Train BER 5.405405318015255e-06\n",
      "Time for one full iteration is 7.3360 minutes\n",
      "encoder learning rate: 1.00e-06, decoder learning rate: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    " if not test:\n",
    "    bers_enc = []\n",
    "    losses_enc = []\n",
    "    bers_dec = []\n",
    "    losses_dec = []\n",
    "    train_ber_dec = 0.\n",
    "    train_ber_enc = 0.\n",
    "    loss_dec = 0.\n",
    "    loss_enc = 0.\n",
    "   \n",
    "    \n",
    "\n",
    "    # Create CSV at the beginning of training\n",
    "    #save_path_id = random.randint(100000, 999999)\n",
    "    with open(os.path.join(results_save_path, f'training_results.csv'), 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Step', 'Loss', 'BER'])\n",
    "\n",
    "        # save args in a json file\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Need to save for:\", model_save_per)\n",
    "    if not batch_schedule:\n",
    "        batch_size = batch_size \n",
    "    else:\n",
    "        batch_size = min_batch_size \n",
    "        best_batch_ber = 10.\n",
    "        best_batch_iter = 0\n",
    "    try:\n",
    "        best_ber = 10.\n",
    "        for iter in range(1, full_iters + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not batch_schedule:\n",
    "                batch_size = batch_size \n",
    "            elif batch_size != max_batch_size:\n",
    "                if iter - best_batch_iter > batch_patience:\n",
    "                    batch_size = min(batch_size * 2, max_batch_size)\n",
    "                    print(f\"Increased batch size to {batch_size}\")\n",
    "                    best_batch_ber = train_ber_enc\n",
    "                    best_batch_iter = iter                        \n",
    "            if 'KO' in decoder_type or decoder_type == 'RNN':\n",
    "                # Train decoder\n",
    "                loss_dec, train_ber_dec = train(polar, dec_optimizer, \n",
    "                                      dec_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, dec_train_snr, dec_train_iters, \n",
    "                                      criterion, device, info_positions, \n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    dec_scheduler.step(loss_dec)                 \n",
    "                bers_dec.append(train_ber_dec)\n",
    "                losses_dec.append(loss_dec)\n",
    "            if 'KO' in encoder_type:\n",
    "                # Train encoder\n",
    "                loss_enc, train_ber_enc = train(polar, enc_optimizer,\n",
    "                                      enc_scheduler if scheduler in ['1cycle'] else None,\n",
    "                                      batch_size, enc_train_snr, enc_train_iters,\n",
    "                                      criterion, device, info_positions,\n",
    "                                      binary=binary, noise_type=noise_type)\n",
    "                # Update ReduceLROnPlateau scheduler if used\n",
    "                if scheduler == 'reduce':\n",
    "                    enc_scheduler.step(loss_enc)                 \n",
    "                bers_enc.append(train_ber_enc)\n",
    "                losses_enc.append(loss_enc)  \n",
    "            if scheduler == 'cosine':\n",
    "                dec_scheduler.step() \n",
    "                enc_scheduler.step()\n",
    "\n",
    "\n",
    "            if batch_schedule and train_ber_enc < best_batch_ber:\n",
    "                best_batch_ber = train_ber_enc\n",
    "                best_batch_iter = iter\n",
    "                print(f'Best BER {best_batch_ber} at {best_batch_iter}')\n",
    "\n",
    "            # Save to CSV\n",
    "            with open(os.path.join(results_save_path, f'training_results.csv'), 'a', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([iter, loss_enc, train_ber_enc, loss_dec, train_ber_dec])\n",
    "            \n",
    "            print(f\"[{iter}/{full_iters}] At {dec_train_snr} dB, Train Loss: {loss_dec} Train BER {train_ber_dec}, \\\n",
    "                  \\n [{iter}/{full_iters}] At {enc_train_snr} dB, Train Loss: {loss_enc} Train BER {train_ber_enc}\")\n",
    "            print(\"Time for one full iteration is {0:.4f} minutes\".format((time.time() - start_time)/60))\n",
    "            print(f'encoder learning rate: {enc_optimizer.param_groups[0][\"lr\"]:.2e}, decoder learning rate: {dec_optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if iter % model_save_per == 0 or iter == 1:\n",
    "                if train_ber_enc < best_ber:\n",
    "                    best_ber = train_ber_enc\n",
    "                    best = True \n",
    "                else:\n",
    "                    best = False\n",
    "                save_model(polar, iter, results_save_path, best = best)\n",
    "                plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        save_model(polar, iter, results_save_path)\n",
    "        plot_stuff(bers_enc, losses_enc, bers_dec, losses_dec, results_save_path)\n",
    "\n",
    "        print(\"Exited and saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053eafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepPolar_Results/attention_Polar_16(256,37)/Scheme_polar/KO_Encoder_KO_Decoder/epochs_200_batchsize_20000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e6b672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "NN weights loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "times = []\n",
    "results_load_path = results_save_path\n",
    "\n",
    "\n",
    "if model_iters is not None:\n",
    "    checkpoint1 = torch.load(results_save_path +'/Models/fnet_gnet_{}.pt'.format(model_iters), map_location=lambda storage, loc: storage)\n",
    "elif test_load_path is not None:\n",
    "    checkpoint1 = torch.load(test_load_path , map_location=lambda storage, loc: storage)\n",
    "else:\n",
    "    checkpoint1 = torch.load(results_load_path +'/Models/fnet_gnet_final.pt', map_location=lambda storage, loc: storage)\n",
    "\n",
    "fnet_dict = checkpoint1[0]\n",
    "gnet_dict = checkpoint1[1]\n",
    "\n",
    "polar.load_nns(fnet_dict, gnet_dict, shared = shared)\n",
    "\n",
    "if snr_points == 1 and test_snr_start == test_snr_end:\n",
    "    snr_range = [test_snr_start]\n",
    "else:\n",
    "    snrs_interval = (test_snr_end - test_snr_start)* 1.0 /  (snr_points-1)\n",
    "    snr_range = [snrs_interval* item + test_snr_start for item in range(snr_points)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For polar code testing.\n",
    "\n",
    "ell = 2\n",
    "Frozen = get_frozen(N, K, rate_profile)\n",
    "Frozen.sort()\n",
    "polar_l_2 = PolarCode(int(np.log2(N)), K, Fr=Frozen, infty = infty, hard_decision=hard_decision)\n",
    "\n",
    "\n",
    "if pairwise:\n",
    "    codebook_size = 1000\n",
    "    all_msg_bits = 2 * (torch.rand(codebook_size, K, device = device) < 0.5).float() - 1\n",
    "    deeppolar_codebook = polar.deeppolar_encode(all_msg_bits)\n",
    "    polar_codebook = polar_l_2.encode_plotkin(all_msg_bits)\n",
    "    gaussian_codebook = F.normalize(torch.randn(codebook_size, N), p=2, dim=1)*np.sqrt(N)\n",
    "\n",
    "    from scipy import stats\n",
    "    w_statistic_deeppolar, p_value_deeppolar = stats.shapiro(deeppolar_codebook.detach().cpu().numpy())\n",
    "    w_statistic_gaussian, p_value_gaussian = stats.shapiro(gaussian_codebook.detach().cpu().numpy())\n",
    "    w_statistic_polar, p_value_polar = stats.shapiro(polar_codebook.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Deeppolar Shapiro test W = {w_statistic_deeppolar}, p-value = {p_value_deeppolar}\")\n",
    "    print(f\"Gaussian Shapiro test W = {w_statistic_gaussian}, p-value = {p_value_gaussian}\")\n",
    "    print(f\"Polar Shapiro test W = {w_statistic_polar}, p-value = {p_value_polar}\")\n",
    "\n",
    "    dists_deeppolar, md_deeppolar = pairwise_distances(deeppolar_codebook)\n",
    "    dists_polar, md_polar = pairwise_distances(polar_codebook)\n",
    "    dists_gaussian, md_gaussian = pairwise_distances(gaussian_codebook)\n",
    "\n",
    "    # Function to calculate and plot PDF\n",
    "    def plot_pdf(data, label, bins=30, alpha=0.5):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.plot(bin_centers, counts, label=label, alpha=alpha)\n",
    "\n",
    "    # Plotting PDF for each list\n",
    "    plt.figure()\n",
    "    plot_pdf(dists_deeppolar, 'Neural', 300)\n",
    "    # plot_pdf(dists_polar, 'Polar', 300)\n",
    "    plot_pdf(dists_gaussian, 'Gaussian', 300)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title(f'Pairwise Distances - N = {N}, K = {K}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(results_save_path, f\"hists_N{N}_K{K}_{id}_2.pdf\"))\n",
    "    plt.show()\n",
    "    print(f'dists_deeppolar: {dists_deeppolar}')\n",
    "    print(f'dists_gaussian: {dists_gaussian}')\n",
    "if epos:\n",
    "    from collections import OrderedDict, Counter\n",
    "\n",
    "    def get_epos(k1, k2):\n",
    "        # return counter for bit ocations of first-errors\n",
    "        bb = torch.ne(k1.cpu().sign(), k2.cpu().sign())\n",
    "        # inds = torch.nonzero(bb)[:, 1].numpy()\n",
    "        idx = []\n",
    "        for ii in range(bb.shape[0]):\n",
    "            try:\n",
    "                iii = list(bb.cpu().float().numpy()[ii]).index(1)\n",
    "                idx.append(iii)\n",
    "            except:\n",
    "                pass\n",
    "        counter = Counter(idx)\n",
    "        ordered_counter = OrderedDict(sorted(counter.items()))\n",
    "        return ordered_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (k, msg_bits) in enumerate(Test_Data_Generator):\n",
    "            msg_bits = msg_bits.to(device)\n",
    "            polar_code = polar_l_2.encode_plotkin(msg_bits)\n",
    "            noisy_code = polar.channel(polar_code, dec_train_snr)\n",
    "            noise = noisy_code - polar_code\n",
    "            deeppolar_code = polar.deeppolar_encode(msg_bits)\n",
    "            noisy_deeppolar_code = deeppolar_code + noise\n",
    "            SC_llrs, decoded_SC_msg_bits = polar_l_2.sc_decode_new(noisy_code, dec_train_snr)\n",
    "            deeppolar_llrs, decoded_deeppolar_msg_bits = polar.deeppolar_decode(noisy_deeppolar_code)\n",
    "\n",
    "            if k == 0:\n",
    "                epos_deeppolar = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "            else:\n",
    "                epos_deeppolar1 = get_epos(msg_bits, decoded_deeppolar_msg_bits.sign())\n",
    "                epos_SC1 = get_epos(msg_bits, decoded_SC_msg_bits.sign())\n",
    "                epos_deeppolar = epos_deeppolar + epos_deeppolar1\n",
    "                epos_SC = epos_SC + epos_SC1\n",
    "\n",
    "        print(f\"epos_deeppolar: {epos_deeppolar}\")\n",
    "        print(f\"EPOS_SC: {epos_SC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ada1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeppolar_example_test(polar, KO, snr_range, device, info_positions, binary=False, num_examples=10**7, noise_type='awgn'):\n",
    "    bers_KO_test = [0. for _ in snr_range]\n",
    "    blers_KO_test = [0. for _ in snr_range]\n",
    "    bers_SC_test = [0. for _ in snr_range]\n",
    "    blers_SC_test = [0. for _ in snr_range]\n",
    "\n",
    "    kernel = N == KO.ell\n",
    "    num_batches = num_examples // test_batch_size\n",
    "\n",
    "    print(f\"TESTING for {num_examples} examples ({num_batches} batches)\")\n",
    "    for snr_ind, snr in enumerate(snr_range):\n",
    "        total_block_errors_SC = 0\n",
    "        total_block_errors_KO = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        sigma = snr_db2sigma(snr)\n",
    "\n",
    "        try:\n",
    "            for _ in range(num_batches):\n",
    "                msg_bits = 2 * (torch.rand(test_batch_size, K) < 0.5).float() - 1\n",
    "                msg_bits = msg_bits.to(device)\n",
    "                polar_code = polar.encode_plotkin(msg_bits)\n",
    "\n",
    "                if 'KO' in encoder_type:\n",
    "                    if kernel:\n",
    "                        KO_polar_code = KO.kernel_encode(kernel_size, KO.gnet_dict[1][0], msg_bits, info_positions, binary=binary)\n",
    "                    else:\n",
    "                        KO_polar_code = KO.deeppolar_encode(msg_bits, binary=binary)\n",
    "\n",
    "                noisy_code = polar.channel(polar_code, snr, noise_type)\n",
    "                noise = noisy_code - polar_code\n",
    "                noisy_KO_code = KO_polar_code + noise if 'KO' in encoder_type else noisy_code\n",
    "\n",
    "                SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(noisy_code, snr)\n",
    "                ber_SC = errors_ber(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                bler_SC = errors_bler(msg_bits, decoded_SC_msg_bits.sign()).item()\n",
    "                total_block_errors_SC += int(bler_SC*test_batch_size)\n",
    "\n",
    "                if 'KO' in decoder_type:\n",
    "                    if kernel:\n",
    "                        if decoder_type == 'KO_parallel':\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_parallel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                        else:\n",
    "                            KO_llrs, decoded_KO_msg_bits = KO.kernel_decode(kernel_size, KO.fnet_dict[1][0], noisy_KO_code, info_positions)\n",
    "                    else:\n",
    "                        KO_llrs, decoded_KO_msg_bits = KO.deeppolar_decode(noisy_KO_code)\n",
    "                else:\n",
    "                    KO_llrs, decoded_KO_msg_bits = KO.sc_decode_new(noisy_KO_code, snr)\n",
    "\n",
    "                ber_KO = errors_ber(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                bler_KO = errors_bler(msg_bits, decoded_KO_msg_bits.sign()).item()\n",
    "                total_block_errors_KO += int(bler_KO*test_batch_size)\n",
    "\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update accumulative results\n",
    "                bers_KO_test[snr_ind] += ber_KO\n",
    "                bers_SC_test[snr_ind] += ber_SC\n",
    "                blers_KO_test[snr_ind] += bler_KO\n",
    "                blers_SC_test[snr_ind] += bler_SC\n",
    "\n",
    "                # Progress logging\n",
    "                if batches_processed % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"SNR: {snr} dB, Sigma: {sigma:.5f}, Progress: {batches_processed}/{num_batches} batches\", end='\\r')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        # Normalize by actual number of batches processed\n",
    "        bers_KO_test[snr_ind] /= batches_processed\n",
    "        bers_SC_test[snr_ind] /= batches_processed\n",
    "        blers_KO_test[snr_ind] /= batches_processed\n",
    "        blers_SC_test[snr_ind] /= batches_processed\n",
    "\n",
    "        print(f\"\\nSNR: {snr} dB, Sigma: {sigma:.5f}\")\n",
    "        print(f\"SC   - BER: {bers_SC_test[snr_ind]:.6f}, BLER: {blers_SC_test[snr_ind]:.6f}\")\n",
    "        print(f\"Deep - BER: {bers_KO_test[snr_ind]:.6f}, BLER: {blers_KO_test[snr_ind]:.6f}\")\n",
    "\n",
    "    return bers_SC_test, blers_SC_test, bers_KO_test, blers_KO_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING\n",
      "TESTING for 1000000 examples (1000 batches)\n",
      "SNR: -5.0 dB, Sigma: 1.77828, Progress: 1000/1000 batches\n",
      "SNR: -5.0 dB, Sigma: 1.77828\n",
      "SC   - BER: 0.166683, BLER: 0.435815\n",
      "Deep - BER: 0.128320, BLER: 0.494898\n",
      "SNR: -4.0 dB, Sigma: 1.58489, Progress: 1000/1000 batches\n",
      "SNR: -4.0 dB, Sigma: 1.58489\n",
      "SC   - BER: 0.072163, BLER: 0.195701\n",
      "Deep - BER: 0.046815, BLER: 0.221169\n",
      "SNR: -3.0 dB, Sigma: 1.41254, Progress: 1000/1000 batches\n",
      "SNR: -3.0 dB, Sigma: 1.41254\n",
      "SC   - BER: 0.020080, BLER: 0.055892\n",
      "Deep - BER: 0.010132, BLER: 0.061511\n",
      "SNR: -2.0 dB, Sigma: 1.25893, Progress: 1000/1000 batches\n",
      "SNR: -2.0 dB, Sigma: 1.25893\n",
      "SC   - BER: 0.003009, BLER: 0.008466\n",
      "Deep - BER: 0.001168, BLER: 0.010139\n",
      "SNR: -1.0 dB, Sigma: 1.12202, Progress: 1000/1000 batches\n",
      "SNR: -1.0 dB, Sigma: 1.12202\n",
      "SC   - BER: 0.000193, BLER: 0.000537\n",
      "Deep - BER: 0.000074, BLER: 0.001071\n",
      "Test SNRs : [-5.0, -4.0, -3.0, -2.0, -1.0]\n",
      "\n",
      "Test Sigmas : [1.7782794100389228, 1.5848931924611136, 1.4125375446227544, 1.2589254117941673, 1.1220184543019633]\n",
      "\n",
      "BERs of DeepPolar: [0.1283198648095131, 0.046814648672938344, 0.010131918928120286, 0.0011678648655761208, 7.427027007179276e-05]\n",
      "BERs of SC decoding: [0.1666827570050955, 0.07216294594109059, 0.020079864889383316, 0.0030092702662514056, 0.00019340540536722984]\n",
      "BLERs of DeepPolar: [0.4948979999999995, 0.221169, 0.061511000000000045, 0.010138999999999943, 0.0010709999999999975]\n",
      "BLERs of SC decoding: [0.4358150000000007, 0.19570099999999993, 0.0558919999999999, 0.008465999999999953, 0.0005370000000000004]\n",
      "time = 359.26917806466423 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING\")\n",
    "\n",
    "start = time.time()\n",
    "bers_SC_test, blers_SC_test, bers_deeppolar_test, blers_deeppolar_test = deeppolar_example_test(polar_l_2, polar, snr_range, device, info_positions, binary = binary, num_examples=10**6, noise_type = noise_type)\n",
    "print(\"Test SNRs : {}\\n\".format(snr_range))\n",
    "print(f\"Test Sigmas : {[snr_db2sigma(s) for s in snr_range]}\\n\")\n",
    "print(\"BERs of DeepPolar: {0}\".format(bers_deeppolar_test))\n",
    "print(\"BERs of SC decoding: {0}\".format(bers_SC_test))\n",
    "print(\"BLERs of DeepPolar: {0}\".format(blers_deeppolar_test))\n",
    "print(\"BLERs of SC decoding: {0}\".format(blers_SC_test))\n",
    "print(f\"time = {(time.time() - start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f42683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## BER\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "ok = 0\n",
    "plt.semilogy(snr_range, bers_deeppolar_test, label=\"DeepPolar\", marker='*', linewidth=1.5)\n",
    "\n",
    "plt.semilogy(snr_range, bers_SC_test, label=\"SC decoder\", marker='^', linewidth=1.5)\n",
    "\n",
    "## BLER\n",
    "plt.semilogy(snr_range, blers_deeppolar_test, label=\"DeepPolar (BLER)\", marker='*', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.semilogy(snr_range, blers_SC_test, label=\"SC decoder (BLER)\", marker='^', linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"SNR (dB)\", fontsize=16)\n",
    "plt.ylabel(\"Error Rate\", fontsize=16)\n",
    "if enc_train_iters > 0:\n",
    "    plt.title(\"PolarC({2}, {3}): DeepPolar trained at Dec_SNR = {0} dB, Enc_SNR = {1}dB\".format(dec_train_snr, enc_train_snr, K,N))\n",
    "else:\n",
    "    plt.title(\"Polar({1}, {2}): DeepPolar trained at Dec_SNR = {0} dB\".format(dec_train_snr, K,N))\n",
    "plt.legend(prop={'size': 15})\n",
    "if test_load_path is not None:\n",
    "    os.makedirs('Polar_Results/figures', exist_ok=True)\n",
    "    fig_save_path = 'Polar_Results/figures/new_plot_DeepPolar.pdf'\n",
    "else:\n",
    "    fig_save_path = results_load_path + f\"/Step_{model_iters if model_iters is not None else 'final'}{'_binary' if binary else ''}.pdf\"\n",
    "if not no_fig:\n",
    "    plt.savefig(fig_save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff45b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
